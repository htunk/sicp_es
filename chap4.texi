@chapter Metalinguistic Abstraction

@quotation
@dots{} It's in words that the magic is---Abracadabra, Open Sesame, and the
rest---but the magic words in one story aren't magical in the next.  The real
magic is to understand which words work, and when, and for what; the trick is
to learn the trick.

@dots{} And those words are made from the letters of our alphabet: a
couple-dozen squiggles we can draw with the pen.  This is the key!  And the
treasure, too, if we can only get our hands on it!  It's as if---as if the key
to the treasure @emph{is} the treasure!

---John Barth, @cite{Chimera}
@end quotation

In our study of program design, we have seen that expert programmers control
the complexity of their designs with the same general techniques used by
designers of all complex systems.  They combine primitive elements to form
compound objects, they abstract compound objects to form higher-level building
blocks, and they preserve modularity by adopting appropriate large-scale views
of system structure.  In illustrating these techniques, we have used Lisp as a
language for describing processes and for constructing computational data
objects and processes to model complex phenomena in the real world.  However,
as we confront increasingly complex problems, we will find that Lisp, or indeed
any fixed programming language, is not sufficient for our needs.  We must
constantly turn to new languages in order to express our ideas more
effectively.  Establishing new languages is a powerful strategy for controlling
complexity in engineering design; we can often enhance our ability to deal with
a complex problem by adopting a new language that enables us to describe (and
hence to think about) the problem in a different way, using primitives, means
of combination, and means of abstraction that are particularly well suited to
the problem at hand.@footnote{The same idea is pervasive throughout all of
engineering.  For example, electrical engineers use many different languages
for describing circuits.  Two of these are the language of electrical
@newterm{networks} and the language of electrical @newterm{systems}.  The
network language emphasizes the physical modeling of devices in terms of
discrete electrical elements.  The primitive objects of the network language
are primitive electrical components such as resistors, capacitors, inductors,
and transistors, which are characterized in terms of physical variables called
voltage and current.  When describing circuits in the network language, the
engineer is concerned with the physical characteristics of a design.  In
contrast, the primitive objects of the system language are signal-processing
modules such as filters and amplifiers.  Only the functional behavior of the
modules is relevant, and signals are manipulated without concern for their
physical realization as voltages and currents.  The system language is erected
on the network language, in the sense that the elements of signal-processing
systems are constructed from electrical networks.  Here, however, the concerns
are with the large-scale organization of electrical devices to solve a given
application problem; the physical feasibility of the parts is assumed.  This
layered collection of languages is another example of the stratified design
technique illustrated by the picture language of section @ref{2-2-4}.}

Programming is endowed with a multitude of languages.  There are physical
languages, such as the machine languages for particular computers.  These
languages are concerned with the representation of data and control in terms of
individual bits of storage and primitive machine instructions.  The
machine-language programmer is concerned with using the given hardware to erect
systems and utilities for the efficient implementation of resource-limited
computations.  High-level languages, erected on a machine-language substrate,
hide concerns about the representation of data as collections of bits and the
representation of programs as sequences of primitive instructions.  These
languages have means of combination and abstraction, such as procedure
definition, that are appropriate to the larger-scale organization of systems.

@newterm{Metalinguistic abstraction}---establishing new languages---plays an
important role in all branches of engineering design.  It is particularly
important to computer programming, because in programming not only can we
formulate new languages but we can also implement these languages by
constructing evaluators.  An @newterm{evaluator} (or @newterm{interpreter}) for
a programming language is a procedure that, when applied to an expression of
the language, performs the actions required to evaluate that expression.

It is no exaggeration to regard this as the most fundamental idea in
programming:

@quotation
The evaluator, which determines the meaning of expressions in a programming
language, is just another program.
@end quotation

To appreciate this point is to change our images of ourselves as programmers.
We come to see ourselves as designers of languages, rather than only users of
languages designed by others.

In fact, we can regard almost any program as the evaluator for some language.
For instance, the polynomial manipulation system of section @ref{2-5-3}
embodies the rules of polynomial arithmetic and implements them in terms of
operations on list-structured data.  If we augment this system with procedures
to read and print polynomial expressions, we have the core of a special-purpose
language for dealing with problems in symbolic mathematics.  The digital-logic
simulator of section @ref{3-3-4} and the constraint propagator of section
@ref{3-3-5} are legitimate languages in their own right, each with its own
primitives, means of combination, and means of abstraction.  Seen from this
perspective, the technology for coping with large-scale computer systems merges
with the technology for building new computer languages, and computer science
itself becomes no more (and no less) than the discipline of constructing
appropriate descriptive languages.

We now embark on a tour of the technology by which languages are established in
terms of other languages.  In this chapter we shall use Lisp as a base,
implementing evaluators as Lisp procedures.  Lisp is particularly well suited
to this task, because of its ability to represent and manipulate symbolic
expressions.  We will take the first step in understanding how languages are
implemented by building an evaluator for Lisp itself.  The language implemented
by our evaluator will be a subset of the Scheme dialect of Lisp that we use in
this book.  Although the evaluator described in this chapter is written for a
particular dialect of Lisp, it contains the essential structure of an evaluator
for any expression-oriented language designed for writing programs for a
sequential machine.  (In fact, most language processors contain, deep within
them, a little ``Lisp'' evaluator.)  The evaluator has been simplified for the
purposes of illustration and discussion, and some features have been left out
that would be important to include in a production-quality Lisp system.
Nevertheless, this simple evaluator is adequate to execute most of the programs
in this book.@footnote{The most important features that our evaluator leaves
out are mechanisms for handling errors and supporting debugging.  For a more
extensive discussion of evaluators, see Friedman, Wand, and Haynes 1992, which
gives an exposition of programming languages that proceeds via a sequence of
evaluators written in Scheme.}

An important advantage of making the evaluator accessible as a Lisp program is
that we can implement alternative evaluation rules by describing these as
modifications to the evaluator program.  One place where we can use this power
to good effect is to gain extra control over the ways in which computational
models embody the notion of time, which was so central to the discussion in
@ref{Chapter 3}.  There, we mitigated some of the complexities of state and
assignment by using streams to decouple the representation of time in the world
from time in the computer.  Our stream programs, however, were sometimes
cumbersome, because they were constrained by the applicative-order evaluation
of Scheme.  In section @ref{4-2}, we'll change the underlying language to
provide for a more elegant approach, by modifying the evaluator to provide for
@newterm{normal-order evaluation}.

Section @ref{4-3} implements a more ambitious linguistic change, whereby
expressions have many values, rather than just a single value.  In this
language of @newterm{nondeterministic computing}, it is natural to express
processes that generate all possible values for expressions and then search for
those values that satisfy certain constraints.  In terms of models of
computation and time, this is like having time branch into a set of ``possible
futures'' and then searching for appropriate time lines.  With our
nondeterministic evaluator, keeping track of multiple values and performing
searches are handled automatically by the underlying mechanism of the language.

In section @ref{4-4} we implement a @newterm{logic-programming} language in
which knowledge is expressed in terms of relations, rather than in terms of
computations with inputs and outputs.  Even though this makes the language
drastically different from Lisp, or indeed from any conventional language, we
will see that the logic-programming evaluator shares the essential structure of
the Lisp evaluator.

@menu
* 4-1::              The Metacircular Evaluator
* 4-2::              Variations on a Scheme -- Lazy Evaluation
* 4-3::              Variations on a Scheme -- Nondeterministic Computing
* 4-4::              Logic Programming
@end menu

@node	4-1, 4-2, Chapter 4, Chapter 4
@section The Metacircular Evaluator

Our evaluator for Lisp will be implemented as a Lisp program.  It may seem
circular to think about evaluating Lisp programs using an evaluator that is
itself implemented in Lisp.  However, evaluation is a process, so it is
appropriate to describe the evaluation process using Lisp, which, after all, is
our tool for describing processes.@footnote{Even so, there will remain
important aspects of the evaluation process that are not elucidated by our
evaluator.  The most important of these are the detailed mechanisms by which
procedures call other procedures and return values to their callers.  We will
address these issues in @ref{Chapter 5}, where we take a closer look at the
evaluation process by implementing the evaluator as a simple register machine.}
An evaluator that is written in the same language that it evaluates is said to
be @newterm{metacircular}.

The metacircular evaluator is essentially a Scheme formulation of the
environment model of evaluation described in section @ref{3-2}.  Recall that
the model has two basic parts:

@enumerate 1

@item
To evaluate a combination (a compound expression other than a special form),
evaluate the subexpressions and then apply the value of the operator
subexpression to the values of the operand subexpressions.

@item
To apply a compound procedure to a set of arguments, evaluate the body of the
procedure in a new environment.  To construct this environment, extend the
environment part of the procedure object by a frame in which the formal
parameters of the procedure are bound to the arguments to which the procedure
is applied.

@end enumerate

These two rules describe the essence of the evaluation process, a basic cycle
in which expressions to be evaluated in environments are reduced to procedures
to be applied to arguments, which in turn are reduced to new expressions to be
evaluated in new environments, and so on, until we get down to symbols, whose
values are looked up in the environment, and to primitive procedures, which are
applied directly (see @ref{Figure 4-1}).@footnote{If we grant ourselves the
ability to apply primitives, then what remains for us to implement in the
evaluator?  The job of the evaluator is not to specify the primitives of the
language, but rather to provide the connective tissue---the means of
combination and the means of abstraction---that binds a collection of
primitives to form a language.  Specifically:

@itemize @bullet

@item
The evaluator enables us to deal with nested expressions.  For example,
although simply applying primitives would suffice for evaluating the expression
@code{(+ 1 6)}, it is not adequate for handling @code{(+ 1 (* 2 3))}.  As far
as the primitive procedure @code{+} is concerned, its arguments must be
numbers, and it would choke if we passed it the expression @code{(* 2 3)} as an
argument.  One important role of the evaluator is to choreograph procedure
composition so that @code{(* 2 3)} is reduced to 6 before being passed as an
argument to @code{+}.

@item
The evaluator allows us to use variables.  For example, the primitive procedure
for addition has no way to deal with expressions such as @code{(+ x 1)}.  We
need an evaluator to keep track of variables and obtain their values before
invoking the primitive procedures.

@item
The evaluator allows us to define compound procedures.  This involves keeping
track of procedure definitions, knowing how to use these definitions in
evaluating expressions, and providing a mechanism that enables procedures to
accept arguments.

@item
The evaluator provides the special forms, which must be evaluated differently
from procedure calls.

@end itemize
} This evaluation cycle will be embodied by the interplay between the two
critical procedures in the evaluator, @code{eval} and @code{apply}, which are
described in section @ref{4-1-1} (see @ref{Figure 4-1}).

The implementation of the evaluator will depend upon procedures that define the
@newterm{syntax} of the expressions to be evaluated.  We will use data
abstraction to make the evaluator independent of the representation of the
language.  For example, rather than committing to a choice that an assignment
is to be represented by a list beginning with the symbol @code{set!} we use an
abstract predicate @code{assignment?} to test for an assignment, and we use
abstract selectors @code{assignment-variable} and @code{assignment-value} to
access the parts of an assignment.  Implementation of expressions will be
described in detail in section @ref{4-1-2}.  There are also operations,
described in section @ref{4-1-3}, that specify the representation of procedures
and environments.  For example, @code{make-procedure} constructs compound
procedures, @code{lookup-variable-value} accesses the values of variables, and
@code{apply-primitive-procedure} applies a primitive procedure to a given list
of arguments.

@menu
* 4-1-1::            The Core of the Evaluator
* 4-1-2::            Representing Expressions
* 4-1-3::            Evaluator Data Structures
* 4-1-4::            Running the Evaluator as a Program
* 4-1-5::            Data as Programs
* 4-1-6::            Internal Definitions
* 4-1-7::            Separating Syntactic Analysis from Execution
@end menu

@node	4-1-1, 4-1-2, 4-1, 4-1
@subsection The Core of the Evaluator

@quotation
@strong{@anchor{Figure 4-1}Figure 4.1:} The @code{eval}-@code{apply} cycle
exposes the essence of a computer language.

@example
                           .,ad88888888baa,
                  _    ,d8P"""        ""9888ba.      _
                 /  .a8"          ,ad88888888888a   |\
               /   aP'          ,88888888888888888a   \
              /  ,8"           ,88888888888888888888,  \
             |  ,8'            (888888888888888888888, |
            /  ,8'             `8888888888888888888888  \
            |  8)               `888888888888888888888, |
Procedure,  |  8                  "88888 Apply 8888888) | Expression
Arguments   |  8     Eval          `888888888888888888) | Environment
            |  8)                    "8888888888888888  |
            \  (b                     "88888888888888'  /
             | `8,                     8888888888888)  |
             \  "8a                   ,888888888888)  /
              \   V8,                 d88888888888"  /
              _\| `8b,             ,d8888888888P' _/
                     `V8a,       ,ad8888888888P'
                        ""88888888888888888P"
                             """"""""""""

                               [graphic by Normand Veillux, modified]
@end example
@end quotation

The evaluation process can be described as the interplay between two
procedures: @code{eval} and @code{apply}.

@subsubheading Eval

@code{Eval} takes as arguments an expression and an environment.  It classifies
the expression and directs its evaluation.  @code{Eval} is structured as a case
analysis of the syntactic type of the expression to be evaluated.  In order to
keep the procedure general, we express the determination of the type of an
expression abstractly, making no commitment to any particular representation
for the various types of expressions.  Each type of expression has a predicate
that tests for it and an abstract means for selecting its parts.  This
@newterm{abstract syntax} makes it easy to see how we can change the syntax of
the language by using the same evaluator, but with a different collection of
syntax procedures.

@noindent
@b{Primitive expressions}

@itemize @bullet

@item
For self-evaluating expressions, such as numbers, @code{eval} returns the
expression itself.

@item
@code{Eval} must look up variables in the environment to find their values.

@end itemize

@noindent
@b{Special forms}

@itemize @bullet

@item
For quoted expressions, @code{eval} returns the expression that was quoted.

@item
An assignment to (or a definition of) a variable must recursively call
@code{eval} to compute the new value to be associated with the variable.  The
environment must be modified to change (or create) the binding of the variable.

@item
An @code{if} expression requires special processing of its parts, so as to
evaluate the consequent if the predicate is true, and otherwise to evaluate the
alternative.

@item
A @code{lambda} expression must be transformed into an applicable procedure by
packaging together the parameters and body specified by the @code{lambda}
expression with the environment of the evaluation.

@item
A @code{begin} expression requires evaluating its sequence of expressions in
the order in which they appear.

@item
A case analysis (@code{cond}) is transformed into a nest of @code{if}
expressions and then evaluated.

@end itemize

@noindent
@b{Combinations}

@itemize @bullet

@item
For a procedure application, @code{eval} must recursively evaluate the operator
part and the operands of the combination.  The resulting procedure and
arguments are passed to @code{apply}, which handles the actual procedure
application.

@end itemize

Here is the definition of @code{eval}:

@lisp
(define (eval exp env)
  (cond ((self-evaluating? exp) exp)
        ((variable? exp) (lookup-variable-value exp env))
        ((quoted? exp) (text-of-quotation exp))
        ((assignment? exp) (eval-assignment exp env))
        ((definition? exp) (eval-definition exp env))
        ((if? exp) (eval-if exp env))
        ((lambda? exp)
         (make-procedure (lambda-parameters exp)
                         (lambda-body exp)
                         env))
        ((begin? exp)
         (eval-sequence (begin-actions exp) env))
        ((cond? exp) (eval (cond->if exp) env))
        ((application? exp)
         (apply (eval (operator exp) env)
                (list-of-values (operands exp) env)))
        (else
         (error "Unknown expression type -- EVAL" exp))))
@end lisp

For clarity, @code{eval} has been implemented as a case analysis using
@code{cond}.  The disadvantage of this is that our procedure handles only a few
distinguishable types of expressions, and no new ones can be defined without
editing the definition of @code{eval}.  In most Lisp implementations,
dispatching on the type of an expression is done in a data-directed style.
This allows a user to add new types of expressions that @code{eval} can
distinguish, without modifying the definition of @code{eval} itself.  (See
@ref{Exercise 4-3}.)

@subsubheading Apply

@code{Apply} takes two arguments, a procedure and a list of arguments to which
the procedure should be applied.  @code{Apply} classifies procedures into two
kinds: It calls @code{apply-primitive-procedure} to apply primitives; it
applies compound procedures by sequentially evaluating the expressions that
make up the body of the procedure.  The environment for the evaluation of the
body of a compound procedure is constructed by extending the base environment
carried by the procedure to include a frame that binds the parameters of the
procedure to the arguments to which the procedure is to be applied.  Here is
the definition of @code{apply}:

@lisp
(define (apply procedure arguments)
  (cond ((primitive-procedure? procedure)
         (apply-primitive-procedure procedure arguments))
        ((compound-procedure? procedure)
         (eval-sequence
           (procedure-body procedure)
           (extend-environment
             (procedure-parameters procedure)
             arguments
             (procedure-environment procedure))))
        (else
         (error
          "Unknown procedure type -- APPLY" procedure))))
@end lisp

@subsubheading Procedure arguments

When @code{eval} processes a procedure application, it uses
@code{list-of-values} to produce the list of arguments to which the procedure
is to be applied. @code{List-of-values} takes as an argument the operands of
the combination.  It evaluates each operand and returns a list of the
corresponding values:@footnote{We could have simplified the @code{application?}
clause in @code{eval} by using @code{map} (and stipulating that @code{operands}
returns a list) rather than writing an explicit @code{list-of-values}
procedure.  We chose not to use @code{map} here to emphasize the fact that the
evaluator can be implemented without any use of higher-order procedures (and
thus could be written in a language that doesn't have higher-order procedures),
even though the language that it supports will include higher-order
procedures.}

@lisp
(define (list-of-values exps env)
  (if (no-operands? exps)
      '()
      (cons (eval (first-operand exps) env)
            (list-of-values (rest-operands exps) env))))
@end lisp

@subsubheading Conditionals

@code{Eval-if} evaluates the predicate part of an @code{if} expression in the
given environment.  If the result is true, @code{eval-if} evaluates the
consequent, otherwise it evaluates the alternative:

@lisp
(define (eval-if exp env)
  (if (true? (eval (if-predicate exp) env))
      (eval (if-consequent exp) env)
      (eval (if-alternative exp) env)))
@end lisp

The use of @code{true?} in @code{eval-if} highlights the issue of the
connection between an implemented language and an implementation language.  The
@code{if-predicate} is evaluated in the language being implemented and thus
yields a value in that language.  The interpreter predicate @code{true?}
translates that value into a value that can be tested by the @code{if} in the
implementation language: The metacircular representation of truth might not be
the same as that of the underlying Scheme.@footnote{In this case, the language
being implemented and the implementation language are the same.  Contemplation
of the meaning of @code{true?} here yields expansion of consciousness without
the abuse of substance.}

@subsubheading Sequences

@code{Eval-sequence} is used by @code{apply} to evaluate the sequence of
expressions in a procedure body and by @code{eval} to evaluate the sequence of
expressions in a @code{begin} expression.  It takes as arguments a sequence of
expressions and an environment, and evaluates the expressions in the order in
which they occur.  The value returned is the value of the final expression.

@lisp
(define (eval-sequence exps env)
  (cond ((last-exp? exps) (eval (first-exp exps) env))
        (else (eval (first-exp exps) env)
              (eval-sequence (rest-exps exps) env))))
@end lisp

@subsubheading Assignments and definitions

The following procedure handles assignments to variables.  It calls @code{eval}
to find the value to be assigned and transmits the variable and the resulting
value to @code{set-variable-value!} to be installed in the designated
environment.

@lisp
(define (eval-assignment exp env)
  (set-variable-value! (assignment-variable exp)
                       (eval (assignment-value exp) env)
                       env)
  'ok)
@end lisp

@noindent
Definitions of variables are handled in a similar manner.@footnote{This
implementation of @code{define} ignores a subtle issue in the handling of
internal definitions, although it works correctly in most cases.  We will see
what the problem is and how to solve it in section @ref{4-1-6}.}

@lisp
(define (eval-definition exp env)
  (define-variable! (definition-variable exp)
                    (eval (definition-value exp) env)
                    env)
  'ok)
@end lisp

We have chosen here to return the symbol @code{ok} as the value of an
assignment or a definition.@footnote{As we said when we introduced
@code{define} and @code{set!}, these values are implementation-dependent in
Scheme---that is, the implementor can choose what value to return.}

@quotation
@strong{@anchor{Exercise 4-1}Exercise 4.1:} Notice that we cannot tell whether
the metacircular evaluator evaluates operands from left to right or from right
to left.  Its evaluation order is inherited from the underlying Lisp: If the
arguments to @code{cons} in @code{list-of-values} are evaluated from left to
right, then @code{list-of-values} will evaluate operands from left to right;
and if the arguments to @code{cons} are evaluated from right to left, then
@code{list-of-values} will evaluate operands from right to left.

Write a version of @code{list-of-values} that evaluates operands from left to
right regardless of the order of evaluation in the underlying Lisp.  Also write
a version of @code{list-of-values} that evaluates operands from right to left.
@end quotation

@node	4-1-2, 4-1-3, 4-1-1, 4-1
@subsection Representing Expressions

The evaluator is reminiscent of the symbolic differentiation program discussed
in section @ref{2-3-2}.  Both programs operate on symbolic expressions.  In
both programs, the result of operating on a compound expression is determined
by operating recursively on the pieces of the expression and combining the
results in a way that depends on the type of the expression.  In both programs
we used data abstraction to decouple the general rules of operation from the
details of how expressions are represented.  In the differentiation program
this meant that the same differentiation procedure could deal with algebraic
expressions in prefix form, in infix form, or in some other form.  For the
evaluator, this means that the syntax of the language being evaluated is
determined solely by the procedures that classify and extract pieces of
expressions.

Here is the specification of the syntax of our language:

@itemize

@item
The only self-evaluating items are numbers and strings:

@lisp
(define (self-evaluating? exp)
  (cond ((number? exp) true)
        ((string? exp) true)
        (else false)))
@end lisp

@item
Variables are represented by symbols:

@lisp
(define (variable? exp) (symbol? exp))
@end lisp

@item
Quotations have the form @code{(quote <@var{text-of-quotation}>)}:@footnote{As
mentioned in section @ref{2-3-1}, the evaluator sees a quoted expression as a
list beginning with @code{quote}, even if the expression is typed with the
quotation mark.  For example, the expression @code{'a} would be seen by the
evaluator as @code{(quote a)}.  See @ref{Exercise 2-55}.}

@lisp
(define (quoted? exp)
  (tagged-list? exp 'quote))

(define (text-of-quotation exp) (cadr exp))
@end lisp

@code{Quoted?} is defined in terms of the procedure @code{tagged-list?}, which
identifies lists beginning with a designated symbol:

@lisp
(define (tagged-list? exp tag)
  (if (pair? exp)
      (eq? (car exp) tag)
      false))
@end lisp

@item
Assignments have the form @code{(set! <@var{var}>
<@var{value}>)}:

@lisp
(define (assignment? exp)
  (tagged-list? exp 'set!))

(define (assignment-variable exp) (cadr exp))

(define (assignment-value exp) (caddr exp))
@end lisp

@item
Definitions have the form

@lisp
(define <@var{var}> <@var{value}>)
@end lisp

@noindent
or the form

@lisp
(define (<@var{var}> <@var{parameter_1}> @dots{} <@var{parameter_@i{n}}>)
  <@var{body}>)
@end lisp

The latter form (standard procedure definition) is syntactic sugar for

@lisp
(define <@var{var}>
  (lambda (<@var{parameter_1}> @dots{} <@var{parameter_@i{n}}>)
    <@var{body}>))
@end lisp

The corresponding syntax procedures are the following:

@lisp
(define (definition? exp)
  (tagged-list? exp 'define))

(define (definition-variable exp)
  (if (symbol? (cadr exp))
      (cadr exp)
      (caadr exp)))

(define (definition-value exp)
  (if (symbol? (cadr exp))
      (caddr exp)
      (make-lambda (cdadr exp)   @r{; formal parameters}
                   (cddr exp)))) @r{; body}
@end lisp

@item
@code{Lambda} expressions are lists that begin with the symbol @code{lambda}:

@lisp
(define (lambda? exp) (tagged-list? exp 'lambda))

(define (lambda-parameters exp) (cadr exp))

(define (lambda-body exp) (cddr exp))
@end lisp

We also provide a constructor for @code{lambda} expressions, which is used by
@code{definition-value}, above:

@lisp
(define (make-lambda parameters body)
  (cons 'lambda (cons parameters body)))
@end lisp

@item
Conditionals begin with @code{if} and have a predicate, a consequent, and an
(optional) alternative.  If the expression has no alternative part, we provide
@code{false} as the alternative.@footnote{The value of an @code{if} expression
when the predicate is false and there is no alternative is unspecified in
Scheme; we have chosen here to make it false.  We will support the use of the
variables @code{true} and @code{false} in expressions to be evaluated by
binding them in the global environment.  See section @ref{4-1-4}.}

@lisp
(define (if? exp) (tagged-list? exp 'if))

(define (if-predicate exp) (cadr exp))

(define (if-consequent exp) (caddr exp))

(define (if-alternative exp)
  (if (not (null? (cdddr exp)))
      (cadddr exp)
      'false))
@end lisp

We also provide a constructor for @code{if} expressions, to be used by
@code{cond->if} to transform @code{cond} expressions into @code{if}
expressions:

@lisp
(define (make-if predicate consequent alternative)
  (list 'if predicate consequent alternative))
@end lisp

@item
@code{Begin} packages a sequence of expressions into a single expression.  We
include syntax operations on @code{begin} expressions to extract the actual
sequence from the @code{begin} expression, as well as selectors that return the
first expression and the rest of the expressions in the
sequence.@footnote{These selectors for a list of expressions---and the
corresponding ones for a list of operands---are not intended as a data
abstraction.  They are introduced as mnemonic names for the basic list
operations in order to make it easier to understand the explicit-control
evaluator in section @ref{5-4}.}

@lisp
(define (begin? exp) (tagged-list? exp 'begin))

(define (begin-actions exp) (cdr exp))

(define (last-exp? seq) (null? (cdr seq)))

(define (first-exp seq) (car seq))

(define (rest-exps seq) (cdr seq))
@end lisp

We also include a constructor @code{sequence->exp} (for use by @code{cond->if})
that transforms a sequence into a single expression, using @code{begin} if
necessary:

@lisp
(define (sequence->exp seq)
  (cond ((null? seq) seq)
        ((last-exp? seq) (first-exp seq))
        (else (make-begin seq))))

(define (make-begin seq) (cons 'begin seq))
@end lisp

@item
A procedure application is any compound expression that is not one of the above
expression types.  The @code{car} of the expression is the operator, and the
@code{cdr} is the list of operands:

@lisp
(define (application? exp) (pair? exp))

(define (operator exp) (car exp))

(define (operands exp) (cdr exp))

(define (no-operands? ops) (null? ops))

(define (first-operand ops) (car ops))

(define (rest-operands ops) (cdr ops))
@end lisp

@end itemize

@subsubheading Derived expressions

Some special forms in our language can be defined in terms of expressions
involving other special forms, rather than being implemented directly.  One
example is @code{cond}, which can be implemented as a nest of @code{if}
expressions.  For example, we can reduce the problem of evaluating the
expression

@lisp
(cond ((> x 0) x)
      ((= x 0) (display 'zero) 0)
      (else (- x)))
@end lisp

@noindent
to the problem of evaluating the following expression involving @code{if} and
@code{begin} expressions:

@lisp
(if (> x 0)
    x
    (if (= x 0)
        (begin (display 'zero)
               0)
        (- x)))
@end lisp

Implementing the evaluation of @code{cond} in this way simplifies the evaluator
because it reduces the number of special forms for which the evaluation process
must be explicitly specified.

We include syntax procedures that extract the parts of a @code{cond}
expression, and a procedure @code{cond->if} that transforms @code{cond}
expressions into @code{if} expressions.  A case analysis begins with
@code{cond} and has a list of predicate-action clauses.  A clause is an
@code{else} clause if its predicate is the symbol @code{else}.@footnote{The
value of a @code{cond} expression when all the predicates are false and there
is no @code{else} clause is unspecified in Scheme; we have chosen here to make
it false.}

@lisp
(define (cond? exp) (tagged-list? exp 'cond))

(define (cond-clauses exp) (cdr exp))

(define (cond-else-clause? clause)
  (eq? (cond-predicate clause) 'else))

(define (cond-predicate clause) (car clause))

(define (cond-actions clause) (cdr clause))

(define (cond->if exp)
  (expand-clauses (cond-clauses exp)))

(define (expand-clauses clauses)
  (if (null? clauses)
      'false                          @r{; no @code{else} clause}
      (let ((first (car clauses))
            (rest (cdr clauses)))
        (if (cond-else-clause? first)
            (if (null? rest)
                (sequence->exp (cond-actions first))
                (error "ELSE clause isn't last -- COND->IF"
                       clauses))
            (make-if (cond-predicate first)
                     (sequence->exp (cond-actions first))
                     (expand-clauses rest))))))
@end lisp

Expressions (such as @code{cond}) that we choose to implement as syntactic
transformations are called @newterm{derived expressions}.  @code{Let}
expressions are also derived expressions (see @ref{Exercise
4-6}).@footnote{Practical Lisp systems provide a mechanism that allows a user
to add new derived expressions and specify their implementation as syntactic
transformations without modifying the evaluator.  Such a user-defined
transformation is called a @newterm{macro}.  Although it is easy to add an
elementary mechanism for defining macros, the resulting language has subtle
name-conflict problems.  There has been much research on mechanisms for macro
definition that do not cause these difficulties.  See, for example, Kohlbecker
1986, Clinger and Rees 1991, and Hanson 1991.}

@quotation
@strong{@anchor{Exercise 4-2}Exercise 4.2:} Louis Reasoner plans to reorder the
@code{cond} clauses in @code{eval} so that the clause for procedure
applications appears before the clause for assignments.  He argues that this
will make the interpreter more efficient: Since programs usually contain more
applications than assignments, definitions, and so on, his modified @code{eval}
will usually check fewer clauses than the original @code{eval} before
identifying the type of an expression.

@enumerate a

@item
What is wrong with Louis's plan?  (Hint: What will Louis's evaluator do with
the expression @code{(define x 3)}?)

@item
Louis is upset that his plan didn't work.  He is willing to go to any lengths
to make his evaluator recognize procedure applications before it checks for
most other kinds of expressions.  Help him by changing the syntax of the
evaluated language so that procedure applications start with @code{call}.  For
example, instead of @code{(factorial 3)} we will now have to write @code{(call
factorial 3)} and instead of @code{(+ 1 2)} we will have to write @code{(call +
1 2)}.

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-3}Exercise 4.3:} Rewrite @code{eval} so that the
dispatch is done in data-directed style.  Compare this with the data-directed
differentiation procedure of @ref{Exercise 2-73}.  (You may use the @code{car}
of a compound expression as the type of the expression, as is appropriate for
the syntax implemented in this section.)
@end quotation

@quotation
@strong{@anchor{Exercise 4-4}Exercise 4.4:} Recall the definitions of the
special forms @code{and} and @code{or} from @ref{Chapter 1}:

@itemize @bullet

@item
@code{and}: The expressions are evaluated from left to right.  If any
expression evaluates to false, false is returned; any remaining expressions are
not evaluated.  If all the expressions evaluate to true values, the value of
the last expression is returned.  If there are no expressions then true is
returned.

@item
@code{or}: The expressions are evaluated from left to right.  If any expression
evaluates to a true value, that value is returned; any remaining expressions
are not evaluated.  If all expressions evaluate to false, or if there are no
expressions, then false is returned.

@end itemize

Install @code{and} and @code{or} as new special forms for the evaluator by
defining appropriate syntax procedures and evaluation procedures
@code{eval-and} and @code{eval-or}.  Alternatively, show how to implement
@code{and} and @code{or} as derived expressions.
@end quotation

@quotation
@strong{@anchor{Exercise 4-5}Exercise 4.5:} Scheme allows an additional syntax
for @code{cond} clauses, @code{(<@var{test}> => <@var{recipient}>)}.  If
<@var{test}> evaluates to a true value, then <@var{recipient}> is evaluated.
Its value must be a procedure of one argument; this procedure is then invoked
on the value of the <@var{test}>, and the result is returned as the value of
the @code{cond} expression.  For example

@lisp
(cond ((assoc 'b '((a 1) (b 2))) => cadr)
      (else false))
@end lisp

@noindent
returns 2.  Modify the handling of @code{cond} so that it supports this
extended syntax.
@end quotation

@quotation
@strong{@anchor{Exercise 4-6}Exercise 4.6:} @code{Let} expressions are derived
expressions, because

@lisp
(let ((<@var{var_1}> <@var{exp_1}>) @dots{} (<@var{var_@i{n}}> <@var{exp_@i{n}}>))
  <@var{body}>)
@end lisp

@noindent
is equivalent to

@lisp
((lambda (<@var{var_1}> @dots{} <@var{var_@i{n}}>)
   <@var{body}>)
 <@var{exp_1}>
 @dots{}
 <@var{exp_@i{n}}>)
@end lisp

Implement a syntactic transformation @code{let->combination} that reduces
evaluating @code{let} expressions to evaluating combinations of the type shown
above, and add the appropriate clause to @code{eval} to handle @code{let}
expressions.
@end quotation

@quotation
@strong{@anchor{Exercise 4-7}Exercise 4.7:} @code{Let*} is similar to
@code{let}, except that the bindings of the @code{let} variables are performed
sequentially from left to right, and each binding is made in an environment in
which all of the preceding bindings are visible.  For example

@lisp
(let* ((x 3)
       (y (+ x 2))
       (z (+ x y 5)))
  (* x z))
@end lisp

@noindent
returns 39.  Explain how a @code{let*} expression can be rewritten as a set of
nested @code{let} expressions, and write a procedure @code{let*->nested-lets}
that performs this transformation.  If we have already implemented @code{let}
(@ref{Exercise 4-6}) and we want to extend the evaluator to handle @code{let*},
is it sufficient to add a clause to @code{eval} whose action is

@lisp
(eval (let*->nested-lets exp) env)
@end lisp

@noindent
or must we explicitly expand @code{let*} in terms of non-derived expressions?
@end quotation

@quotation
@strong{@anchor{Exercise 4-8}Exercise 4.8:} ``Named @code{let}'' is a variant
of @code{let} that has the form

@lisp
(let <@var{var}> <@var{bindings}> <@var{body}>)
@end lisp

The <@var{bindings}> and <@var{body}> are just as in ordinary @code{let},
except that <@var{var}> is bound within <@var{body}> to a procedure whose body
is <@var{body}> and whose parameters are the variables in the <@var{bindings}>.
Thus, one can repeatedly execute the <@var{body}> by invoking the procedure
named <@var{var}>.  For example, the iterative Fibonacci procedure (section
@ref{1-2-2}) can be rewritten using named @code{let} as follows:

@lisp
(define (fib n)
  (let fib-iter ((a 1)
                 (b 0)
                 (count n))
    (if (= count 0)
        b
        (fib-iter (+ a b) a (- count 1)))))
@end lisp

Modify @code{let->combination} of @ref{Exercise 4-6} to also support named
@code{let}.
@end quotation

@quotation
@strong{@anchor{Exercise 4-9}Exercise 4.9:} Many languages support a variety of
iteration constructs, such as @code{do}, @code{for}, @code{while}, and
@code{until}.  In Scheme, iterative processes can be expressed in terms of
ordinary procedure calls, so special iteration constructs provide no essential
gain in computational power.  On the other hand, such constructs are often
convenient.  Design some iteration constructs, give examples of their use, and
show how to implement them as derived expressions.
@end quotation

@quotation
@strong{@anchor{Exercise 4-10}Exercise 4.10:} By using data abstraction, we
were able to write an @code{eval} procedure that is independent of the
particular syntax of the language to be evaluated.  To illustrate this, design
and implement a new syntax for Scheme by modifying the procedures in this
section, without changing @code{eval} or @code{apply}.
@end quotation

@node	4-1-3, 4-1-4, 4-1-2, 4-1
@subsection Evaluator Data Structures

In addition to defining the external syntax of expressions, the evaluator
implementation must also define the data structures that the evaluator
manipulates internally, as part of the execution of a program, such as the
representation of procedures and environments and the representation of true
and false.

@subsubheading Testing of predicates

For conditionals, we accept anything to be true that is not the explicit
@code{false} object.

@lisp
(define (true? x)
  (not (eq? x false)))

(define (false? x)
  (eq? x false))
@end lisp

@subsubheading Representing procedures

To handle primitives, we assume that we have available the following
procedures:

@itemize @bullet

@item
@code{(apply-primitive-procedure <@var{proc}> <@var{args}>)}

@noindent
applies the given primitive procedure to the argument values in the list
<@var{args}> and returns the result of the application.

@item
@code{(primitive-procedure? <@var{proc}>)}

@noindent
tests whether <@var{proc}> is a primitive procedure.

@end itemize

These mechanisms for handling primitives are further described in section
@ref{4-1-4}.

Compound procedures are constructed from parameters, procedure bodies, and
environments using the constructor @code{make-procedure}:

@lisp
(define (make-procedure parameters body env)
  (list 'procedure parameters body env))

(define (compound-procedure? p)
  (tagged-list? p 'procedure))

(define (procedure-parameters p) (cadr p))

(define (procedure-body p) (caddr p))

(define (procedure-environment p) (cadddr p))
@end lisp

@subsubheading Operations on Environments

The evaluator needs operations for manipulating environments.  As explained in
section @ref{3-2}, an environment is a sequence of frames, where each frame is
a table of bindings that associate variables with their corresponding values.
We use the following operations for manipulating environments:

@itemize @bullet

@item
@code{(lookup-variable-value <@var{var}> <@var{env}>)}

returns the value that is bound to the symbol <@var{var}> in the environment
<@var{env}>, or signals an error if the variable is unbound.

@item
@code{(extend-environment <@var{variables}> <@var{values}> <@var{base-env}>)}

returns a new environment, consisting of a new frame in which the symbols in
the list <@var{variables}> are bound to the corresponding elements in the list
<@var{values}>, where the enclosing environment is the environment
<@var{base-env}>.

@item
@code{(define-variable! <@var{var}> <@var{value}> <@var{env}>)}

adds to the first frame in the environment <@var{env}> a new binding that
associates the variable <@var{var}> with the value <@var{value}>.

@item
@code{(set-variable-value! <@var{var}> <@var{value}> <@var{env}>)}

changes the binding of the variable <@var{var}> in the environment <@var{env}>
so that the variable is now bound to the value <@var{value}>, or signals an
error if the variable is unbound.

@end itemize

To implement these operations we represent an environment as a list of frames.
The enclosing environment of an environment is the @code{cdr} of the list.  The
empty environment is simply the empty list.

@lisp
(define (enclosing-environment env) (cdr env))

(define (first-frame env) (car env))

(define the-empty-environment '())
@end lisp

Each frame of an environment is represented as a pair of lists: a list of the
variables bound in that frame and a list of the associated
values.@footnote{Frames are not really a data abstraction in the following
code: @code{Set-variable-value!} and @code{define-variable!} use
@code{set-car!}  to directly modify the values in a frame.  The purpose of the
frame procedures is to make the environment-manipulation procedures easy to
read.}

@lisp
(define (make-frame variables values)
  (cons variables values))

(define (frame-variables frame) (car frame))

(define (frame-values frame) (cdr frame))

(define (add-binding-to-frame! var val frame)
  (set-car! frame (cons var (car frame)))
  (set-cdr! frame (cons val (cdr frame))))
@end lisp

To extend an environment by a new frame that associates variables with values,
we make a frame consisting of the list of variables and the list of values, and
we adjoin this to the environment.  We signal an error if the number of
variables does not match the number of values.

@lisp
(define (extend-environment vars vals base-env)
  (if (= (length vars) (length vals))
      (cons (make-frame vars vals) base-env)
      (if (< (length vars) (length vals))
          (error "Too many arguments supplied" vars vals)
          (error "Too few arguments supplied" vars vals))))
@end lisp

To look up a variable in an environment, we scan the list of variables in the
first frame.  If we find the desired variable, we return the corresponding
element in the list of values.  If we do not find the variable in the current
frame, we search the enclosing environment, and so on.  If we reach the empty
environment, we signal an ``unbound variable'' error.

@lisp
(define (lookup-variable-value var env)
  (define (env-loop env)
    (define (scan vars vals)
      (cond ((null? vars)
             (env-loop (enclosing-environment env)))
            ((eq? var (car vars))
             (car vals))
            (else (scan (cdr vars) (cdr vals)))))
    (if (eq? env the-empty-environment)
        (error "Unbound variable" var)
        (let ((frame (first-frame env)))
          (scan (frame-variables frame)
                (frame-values frame)))))
  (env-loop env))
@end lisp

To set a variable to a new value in a specified environment, we scan for the
variable, just as in @code{lookup-variable-value}, and change the corresponding
value when we find it.

@lisp
(define (set-variable-value! var val env)
  (define (env-loop env)
    (define (scan vars vals)
      (cond ((null? vars)
             (env-loop (enclosing-environment env)))
            ((eq? var (car vars))
             (set-car! vals val))
            (else (scan (cdr vars) (cdr vals)))))
    (if (eq? env the-empty-environment)
        (error "Unbound variable -- SET!" var)
        (let ((frame (first-frame env)))
          (scan (frame-variables frame)
                (frame-values frame)))))
  (env-loop env))
@end lisp

To define a variable, we search the first frame for a binding for the variable,
and change the binding if it exists (just as in @code{set-variable-value!}).
If no such binding exists, we adjoin one to the first frame.

@lisp
(define (define-variable! var val env)
  (let ((frame (first-frame env)))
    (define (scan vars vals)
      (cond ((null? vars)
             (add-binding-to-frame! var val frame))
            ((eq? var (car vars))
             (set-car! vals val))
            (else (scan (cdr vars) (cdr vals)))))
    (scan (frame-variables frame)
          (frame-values frame))))
@end lisp

The method described here is only one of many plausible ways to represent
environments.  Since we used data abstraction to isolate the rest of the
evaluator from the detailed choice of representation, we could change the
environment representation if we wanted to.  (See @ref{Exercise 4-11}.)  In a
production-quality Lisp system, the speed of the evaluator's environment
operations---especially that of variable lookup---has a major impact on the
performance of the system.  The representation described here, although
conceptually simple, is not efficient and would not ordinarily be used in a
production system.@footnote{The drawback of this representation (as well as the
variant in @ref{Exercise 4-11}) is that the evaluator may have to search
through many frames in order to find the binding for a given variable.  (Such
an approach is referred to as @newterm{deep binding}.)  One way to avoid this
inefficiency is to make use of a strategy called @newterm{lexical addressing},
which will be discussed in section @ref{5-5-6}.}

@quotation
@strong{@anchor{Exercise 4-11}Exercise 4.11:} Instead of representing a frame
as a pair of lists, we can represent a frame as a list of bindings, where each
binding is a name-value pair.  Rewrite the environment operations to use this
alternative representation.
@end quotation

@quotation
@strong{@anchor{Exercise 4-12}Exercise 4.12:} The procedures
@code{set-variable-value!}, @code{define-variable!}, and
@code{lookup-variable-value} can be expressed in terms of more abstract
procedures for traversing the environment structure.  Define abstractions that
capture the common patterns and redefine the three procedures in terms of these
abstractions.
@end quotation

@quotation
@strong{@anchor{Exercise 4-13}Exercise 4.13:} Scheme allows us to create new
bindings for variables by means of @code{define}, but provides no way to get
rid of bindings.  Implement for the evaluator a special form
@code{make-unbound!} that removes the binding of a given symbol from the
environment in which the @code{make-unbound!} expression is evaluated.  This
problem is not completely specified.  For example, should we remove only the
binding in the first frame of the environment?  Complete the specification and
justify any choices you make.
@end quotation

@node	4-1-4, 4-1-5, 4-1-3, 4-1
@subsection Running the Evaluator as a Program

Given the evaluator, we have in our hands a description (expressed in Lisp) of
the process by which Lisp expressions are evaluated.  One advantage of
expressing the evaluator as a program is that we can run the program.  This
gives us, running within Lisp, a working model of how Lisp itself evaluates
expressions.  This can serve as a framework for experimenting with evaluation
rules, as we shall do later in this chapter.

Our evaluator program reduces expressions ultimately to the application of
primitive procedures.  Therefore, all that we need to run the evaluator is to
create a mechanism that calls on the underlying Lisp system to model the
application of primitive procedures.

There must be a binding for each primitive procedure name, so that when
@code{eval} evaluates the operator of an application of a primitive, it will
find an object to pass to @code{apply}.  We thus set up a global environment
that associates unique objects with the names of the primitive procedures that
can appear in the expressions we will be evaluating.  The global environment
also includes bindings for the symbols @code{true} and @code{false}, so that
they can be used as variables in expressions to be evaluated.

@lisp
(define (setup-environment)
  (let ((initial-env
         (extend-environment (primitive-procedure-names)
                             (primitive-procedure-objects)
                             the-empty-environment)))
    (define-variable! 'true true initial-env)
    (define-variable! 'false false initial-env)
    initial-env))

(define the-global-environment (setup-environment))
@end lisp

It does not matter how we represent the primitive procedure objects, so long as
@code{apply} can identify and apply them by using the procedures
@code{primitive-procedure?} and @code{apply-primitive-procedure}.  We have
chosen to represent a primitive procedure as a list beginning with the symbol
@code{primitive} and containing a procedure in the underlying Lisp that
implements that primitive.

@lisp
(define (primitive-procedure? proc)
  (tagged-list? proc 'primitive))

(define (primitive-implementation proc) (cadr proc))
@end lisp

@code{Setup-environment} will get the primitive names and implementation
procedures from a list:@footnote{Any procedure defined in the underlying Lisp
can be used as a primitive for the metacircular evaluator.  The name of a
primitive installed in the evaluator need not be the same as the name of its
implementation in the underlying Lisp; the names are the same here because the
metacircular evaluator implements Scheme itself.  Thus, for example, we could
put @code{(list 'first car)} or @code{(list 'square (lambda (x) (* x x)))} in
the list of @code{primitive-procedures}.}

@lisp
(define primitive-procedures
  (list (list 'car car)
        (list 'cdr cdr)
        (list 'cons cons)
        (list 'null? null?)
        <@var{more primitives}>
        ))

(define (primitive-procedure-names)
  (map car
       primitive-procedures))

(define (primitive-procedure-objects)
  (map (lambda (proc) (list 'primitive (cadr proc)))
       primitive-procedures))
@end lisp

To apply a primitive procedure, we simply apply the implementation procedure to
the arguments, using the underlying Lisp
system:@footnote{@code{Apply-in-underlying-scheme} is the @code{apply}
procedure we have used in earlier chapters.  The metacircular evaluator's
@code{apply} procedure (section @ref{4-1-1}) models the working of this
primitive.  Having two different things called @code{apply} leads to a
technical problem in running the metacircular evaluator, because defining the
metacircular evaluator's @code{apply} will mask the definition of the
primitive.  One way around this is to rename the metacircular @code{apply} to
avoid conflict with the name of the primitive procedure.  We have assumed
instead that we have saved a reference to the underlying @code{apply} by doing

@lisp
(define apply-in-underlying-scheme apply)
@end lisp

@noindent
before defining the metacircular @code{apply}.  This allows us to access the
original version of @code{apply} under a different name.}

@lisp
(define (apply-primitive-procedure proc args)
  (apply-in-underlying-scheme
   (primitive-implementation proc) args))
@end lisp

For convenience in running the metacircular evaluator, we provide a
@newterm{driver loop} that models the read-eval-print loop of the underlying
Lisp system.  It prints a @newterm{prompt}, reads an input expression,
evaluates this expression in the global environment, and prints the result.  We
precede each printed result by an @newterm{output prompt} so as to distinguish
the value of the expression from other output that may be printed.@footnote{The
primitive procedure @code{read} waits for input from the user, and returns the
next complete expression that is typed.  For example, if the user types
@code{(+ 23 x)}, @code{read} returns a three-element list containing the symbol
@code{+}, the number 23, and the symbol @code{x}.  If the user types @code{'x},
@code{read} returns a two-element list containing the symbol @code{quote} and
the symbol @code{x}.}

@lisp
(define input-prompt ";;; M-Eval input:")
(define output-prompt ";;; M-Eval value:")

(define (driver-loop)
  (prompt-for-input input-prompt)
  (let ((input (read)))
    (let ((output (eval input the-global-environment)))
      (announce-output output-prompt)
      (user-print output)))
  (driver-loop))

(define (prompt-for-input string)
  (newline) (newline) (display string) (newline))

(define (announce-output string)
  (newline) (display string) (newline))
@end lisp

We use a special printing procedure, @code{user-print}, to avoid printing the
environment part of a compound procedure, which may be a very long list (or may
even contain cycles).

@lisp
(define (user-print object)
  (if (compound-procedure? object)
      (display (list 'compound-procedure
                     (procedure-parameters object)
                     (procedure-body object)
                     '<procedure-env>))
      (display object)))
@end lisp

Now all we need to do to run the evaluator is to initialize the global
environment and start the driver loop.  Here is a sample interaction:

@lisp
(define the-global-environment (setup-environment))

(driver-loop)

@i{;;; M-Eval input:}
(define (append x y)
  (if (null? x)
      y
      (cons (car x)
            (append (cdr x) y))))
@i{;;; M-Eval value:}
@i{ok}

@i{;;; M-Eval input:}
(append '(a b c) '(d e f))
@i{;;; M-Eval value:}
@i{(a b c d e f)}
@end lisp

@quotation
@strong{@anchor{Exercise 4-14}Exercise 4.14:} Eva Lu Ator and Louis Reasoner
are each experimenting with the metacircular evaluator.  Eva types in the
definition of @code{map}, and runs some test programs that use it.  They work
fine.  Louis, in contrast, has installed the system version of @code{map} as a
primitive for the metacircular evaluator.  When he tries it, things go terribly
wrong.  Explain why Louis's @code{map} fails even though Eva's works.
@end quotation

@node	4-1-5, 4-1-6, 4-1-4, 4-1
@subsection Data as Programs

In thinking about a Lisp program that evaluates Lisp expressions, an analogy
might be helpful.  One operational view of the meaning of a program is that a
program is a description of an abstract (perhaps infinitely large) machine.
For example, consider the familiar program to compute factorials:

@lisp
(define (factorial n)
  (if (= n 1)
      1
      (* (factorial (- n 1)) n)))
@end lisp

We may regard this program as the description of a machine containing parts
that decrement, multiply, and test for equality, together with a two-position
switch and another factorial machine. (The factorial machine is infinite
because it contains another factorial machine within it.)  @ref{Figure 4-2} is
a flow diagram for the factorial machine, showing how the parts are wired
together.

@quotation
@strong{@anchor{Figure 4-2}Figure 4.2:} The factorial program, viewed as an
abstract machine.

@example
    +-----------------------------------+
    | factorial                   |1    |
    |              |1             V     |
    |              |           +-----+  |
    |              V           | #   |  |
    |           +-----+        |     |  |
6 --------*-----|  =  |------->|   #-+-----> 720
    |     |     +-----+        |  /  |  |
    |     |                    | #   |  |
    |     |                    +-----+  |
    |     |                       ^     |
    |     |                       |     |
    |     |                    +--+--+  |
    |     *------------------->|  *  |  |
    |     |                    +-----+  |
    |     V                       ^     |
    |  +-----+    +-----------+   |     |
    |  |  -  +--->| factorial +---+     |
    |  +-----+    +-----------+         |
    |     ^                             |
    |     |1                            |
    +-----------------------------------+
@end example
@end quotation

In a similar way, we can regard the evaluator as a very special machine that
takes as input a description of a machine.  Given this input, the evaluator
configures itself to emulate the machine described.  For example, if we feed
our evaluator the definition of @code{factorial}, as shown in @ref{Figure 4-3},
the evaluator will be able to compute factorials.

@quotation
@strong{@anchor{Figure 4-3}Figure 4.3:} The evaluator emulating a factorial
machine.

@example
                   +--------+
            6 ---->|  eval  |----> 720
                   +--------+
                       /
             . . .    /  . . .
       . . .       ../. .      .
     .                           ..
    .   (define (factorial n)      . . .
   .      (if (= n 1)                   . .
    .         1                            .
    .         (* (factorial (- n 1)) n)))   .
      . .                       . .        .
          . .  . .      . . . .     . . . .
                   . ..
@end example
@end quotation

From this perspective, our evaluator is seen to be a @newterm{universal
machine}.  It mimics other machines when these are described as Lisp
programs.@footnote{The fact that the machines are described in Lisp is
inessential.  If we give our evaluator a Lisp program that behaves as an
evaluator for some other language, say C, the Lisp evaluator will emulate the C
evaluator, which in turn can emulate any machine described as a C program.
Similarly, writing a Lisp evaluator in C produces a C program that can execute
any Lisp program.  The deep idea here is that any evaluator can emulate any
other.  Thus, the notion of ``what can in principle be computed'' (ignoring
practicalities of time and memory required) is independent of the language or
the computer, and instead reflects an underlying notion of
@newterm{computability}.  This was first demonstrated in a clear way by Alan
M. Turing (1912-1954), whose 1936 paper laid the foundations for theoretical
computer science.  In the paper, Turing presented a simple computational
model---now known as a @newterm{Turing machine}---and argued that any
``effective process'' can be formulated as a program for such a machine.  (This
argument is known as the @newterm{Church-Turing thesis}.)  Turing then
implemented a universal machine, i.e., a Turing machine that behaves as an
evaluator for Turing-machine programs.  He used this framework to demonstrate
that there are well-posed problems that cannot be computed by Turing machines
(see @ref{Exercise 4-15}), and so by implication cannot be formulated as
``effective processes.''  Turing went on to make fundamental contributions to
practical computer science as well.  For example, he invented the idea of
structuring programs using general-purpose subroutines.  See Hodges 1983 for a
biography of Turing.} This is striking. Try to imagine an analogous evaluator
for electrical circuits.  This would be a circuit that takes as input a signal
encoding the plans for some other circuit, such as a filter.  Given this input,
the circuit evaluator would then behave like a filter with the same
description.  Such a universal electrical circuit is almost unimaginably
complex.  It is remarkable that the program evaluator is a rather simple
program.@footnote{Some people find it counterintuitive that an evaluator, which
is implemented by a relatively simple procedure, can emulate programs that are
more complex than the evaluator itself.  The existence of a universal evaluator
machine is a deep and wonderful property of computation.  @newterm{Recursion
theory}, a branch of mathematical logic, is concerned with logical limits of
computation.  Douglas Hofstadter's beautiful book @cite{G@"odel, Escher, Bach}
(1979) explores some of these ideas.}

Another striking aspect of the evaluator is that it acts as a bridge between
the data objects that are manipulated by our programming language and the
programming language itself.  Imagine that the evaluator program (implemented
in Lisp) is running, and that a user is typing expressions to the evaluator and
observing the results.  From the perspective of the user, an input expression
such as @code{(* x x)} is an expression in the programming language, which the
evaluator should execute.  From the perspective of the evaluator, however, the
expression is simply a list (in this case, a list of three symbols: @code{*},
@code{x}, and @code{x}) that is to be manipulated according to a well-defined
set of rules.

That the user's programs are the evaluator's data need not be a source of
confusion.  In fact, it is sometimes convenient to ignore this distinction, and
to give the user the ability to explicitly evaluate a data object as a Lisp
expression, by making @code{eval} available for use in programs.  Many Lisp
dialects provide a primitive @code{eval} procedure that takes as arguments an
expression and an environment and evaluates the expression relative to the
environment.@footnote{Warning: This @code{eval} primitive is not identical to
the @code{eval} procedure we implemented in section @ref{4-1-1}, because it
uses @emph{actual} Scheme environments rather than the sample environment
structures we built in section @ref{4-1-3}.  These actual environments cannot
be manipulated by the user as ordinary lists; they must be accessed via
@code{eval} or other special operations.  Similarly, the @code{apply} primitive
we saw earlier is not identical to the metacircular @code{apply}, because it
uses actual Scheme procedures rather than the procedure objects we constructed
in sections @ref{4-1-3} and @ref{4-1-4}.} Thus,

@lisp
(eval '(* 5 5) user-initial-environment)
@end lisp

@noindent
and

@lisp
(eval (cons '* (list 5 5)) user-initial-environment)
@end lisp

@noindent
will both return 25.@footnote{The @acronym{MIT} implementation of Scheme
includes @code{eval}, as well as a symbol @code{user-initial-environment} that
is bound to the initial environment in which the user's input expressions are
evaluated.}

@quotation
@strong{@anchor{Exercise 4-15}Exercise 4.15:} Given a one-argument procedure
@code{p} and an object @code{a}, @code{p} is said to ``halt'' on @code{a} if
evaluating the expression @code{(p a)} returns a value (as opposed to
terminating with an error message or running forever).  Show that it is
impossible to write a procedure @code{halts?} that correctly determines whether
@code{p} halts on @code{a} for any procedure @code{p} and object @code{a}.  Use
the following reasoning: If you had such a procedure @code{halts?}, you could
implement the following program:

@lisp
(define (run-forever) (run-forever))

(define (try p)
  (if (halts? p p)
      (run-forever)
      'halted))
@end lisp

Now consider evaluating the expression @code{(try try)} and show that any
possible outcome (either halting or running forever) violates the intended
behavior of @code{halts?}.@footnote{Although we stipulated that @code{halts?}
is given a procedure object, notice that this reasoning still applies even if
@code{halts?} can gain access to the procedure's text and its environment.
This is Turing's celebrated @newterm{Halting Theorem}, which gave the first
clear example of a @newterm{non-computable} problem, i.e., a well-posed task
that cannot be carried out as a computational procedure.}
@end quotation

@node	4-1-6, 4-1-7, 4-1-5, 4-1
@subsection Internal Definitions

Our environment model of evaluation and our metacircular evaluator execute
definitions in sequence, extending the environment frame one definition at a
time.  This is particularly convenient for interactive program development, in
which the programmer needs to freely mix the application of procedures with the
definition of new procedures.  However, if we think carefully about the
internal definitions used to implement block structure (introduced in section
@ref{1-1-8}), we will find that name-by-name extension of the environment may
not be the best way to define local variables.

Consider a procedure with internal definitions, such as

@lisp
(define (f x)
  (define (even? n)
    (if (= n 0)
        true
        (odd? (- n 1))))
  (define (odd? n)
    (if (= n 0)
        false
        (even? (- n 1))))
  <@var{rest of body of @code{f}}>)
@end lisp

Our intention here is that the name @code{odd?} in the body of the procedure
@code{even?} should refer to the procedure @code{odd?} that is defined after
@code{even?}.  The scope of the name @code{odd?} is the entire body of
@code{f}, not just the portion of the body of @code{f} starting at the point
where the @code{define} for @code{odd?} occurs.  Indeed, when we consider that
@code{odd?} is itself defined in terms of @code{even?}---so that @code{even?}
and @code{odd?} are mutually recursive procedures---we see that the only
satisfactory interpretation of the two @code{define}s is to regard them as if
the names @code{even?} and @code{odd?} were being added to the environment
simultaneously.  More generally, in block structure, the scope of a local name
is the entire procedure body in which the @code{define} is evaluated.

As it happens, our interpreter will evaluate calls to @code{f} correctly, but
for an ``accidental'' reason: Since the definitions of the internal procedures
come first, no calls to these procedures will be evaluated until all of them
have been defined.  Hence, @code{odd?}  will have been defined by the time
@code{even?} is executed.  In fact, our sequential evaluation mechanism will
give the same result as a mechanism that directly implements simultaneous
definition for any procedure in which the internal definitions come first in a
body and evaluation of the value expressions for the defined variables doesn't
actually use any of the defined variables.  (For an example of a procedure that
doesn't obey these restrictions, so that sequential definition isn't equivalent
to simultaneous definition, see @ref{Exercise 4-19}.)@footnote{Wanting programs
to not depend on this evaluation mechanism is the reason for the ``management
is not responsible'' remark in @ref{Footnote 28} of @ref{Chapter 1}.  By
insisting that internal definitions come first and do not use each other while
the definitions are being evaluated, the @acronym{IEEE} standard for Scheme
leaves implementors some choice in the mechanism used to evaluate these
definitions.  The choice of one evaluation rule rather than another here may
seem like a small issue, affecting only the interpretation of ``badly formed''
programs.  However, we will see in section @ref{5-5-6} that moving to a model
of simultaneous scoping for internal definitions avoids some nasty difficulties
that would otherwise arise in implementing a compiler.}

There is, however, a simple way to treat definitions so that internally defined
names have truly simultaneous scope---just create all local variables that will
be in the current environment before evaluating any of the value expressions.
One way to do this is by a syntax transformation on @code{lambda} expressions.
Before evaluating the body of a @code{lambda} expression, we ``scan out'' and
eliminate all the internal definitions in the body.  The internally defined
variables will be created with a @code{let} and then set to their values by
assignment.  For example, the procedure

@lisp
(lambda <@var{vars}>
  (define u <@var{e1}>)
  (define v <@var{e2}>)
  <@var{e3}>)
@end lisp

@noindent
would be transformed into

@lisp
(lambda <@var{vars}>
  (let ((u '*unassigned*)
        (v '*unassigned*))
    (set! u <@var{e1}>)
    (set! v <@var{e2}>)
    <@var{e3}>))
@end lisp

@noindent
where @code{*unassigned*} is a special symbol that causes looking up a variable
to signal an error if an attempt is made to use the value of the
not-yet-assigned variable.

An alternative strategy for scanning out internal definitions is shown in
@ref{Exercise 4-18}.  Unlike the transformation shown above, this enforces the
restriction that the defined variables' values can be evaluated without using
any of the variables' values.@footnote{The @acronym{IEEE} standard for Scheme
allows for different implementation strategies by specifying that it is up to
the programmer to obey this restriction, not up to the implementation to
enforce it.  Some Scheme implementations, including @acronym{MIT} Scheme, use
the transformation shown above.  Thus, some programs that don't obey this
restriction will in fact run in such implementations.}

@quotation
@strong{@anchor{Exercise 4-16}Exercise 4.16:} In this exercise we implement the
method just described for interpreting internal definitions.  We assume that
the evaluator supports @code{let} (see @ref{Exercise 4-6}).

@enumerate a

@item
Change @code{lookup-variable-value} (section @ref{4-1-3}) to signal an error if
the value it finds is the symbol @code{*unassigned*}.

@item
Write a procedure @code{scan-out-defines} that takes a procedure body and
returns an equivalent one that has no internal definitions, by making the
transformation described above.

@item
Install @code{scan-out-defines} in the interpreter, either in
@code{make-procedure} or in @code{procedure-body} (see section @ref{4-1-3}).
Which place is better?  Why?

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-17}Exercise 4.17:} Draw diagrams of the environment
in effect when evaluating the expression <@var{e3}> in the procedure in the
text, comparing how this will be structured when definitions are interpreted
sequentially with how it will be structured if definitions are scanned out as
described.  Why is there an extra frame in the transformed program?  Explain
why this difference in environment structure can never make a difference in the
behavior of a correct program.  Design a way to make the interpreter implement
the ``simultaneous'' scope rule for internal definitions without constructing
the extra frame.
@end quotation

@quotation
@strong{@anchor{Exercise 4-18}Exercise 4.18:} Consider an alternative strategy
for scanning out definitions that translates the example in the text to

@lisp
(lambda <@var{vars}>
  (let ((u '*unassigned*)
        (v '*unassigned*))
    (let ((a <@var{e1}>)
          (b <@var{e2}>))
      (set! u a)
      (set! v b))
    <@var{e3}>))
@end lisp

Here @code{a} and @code{b} are meant to represent new variable names, created
by the interpreter, that do not appear in the user's program.  Consider the
@code{solve} procedure from section @ref{3-5-4}:

@lisp
(define (solve f y0 dt)
  (define y (integral (delay dy) y0 dt))
  (define dy (stream-map f y))
  y)
@end lisp

Will this procedure work if internal definitions are scanned out as shown in
this exercise?  What if they are scanned out as shown in the text?  Explain.
@end quotation

@quotation
@strong{@anchor{Exercise 4-19}Exercise 4.19:} Ben Bitdiddle, Alyssa P. Hacker,
and Eva Lu Ator are arguing about the desired result of evaluating the
expression

@lisp
(let ((a 1))
  (define (f x)
    (define b (+ a x))
    (define a 5)
    (+ a b))
  (f 10))
@end lisp

Ben asserts that the result should be obtained using the sequential rule for
@code{define}: @code{b} is defined to be 11, then @code{a} is defined to be 5,
so the result is 16.  Alyssa objects that mutual recursion requires the
simultaneous scope rule for internal procedure definitions, and that it is
unreasonable to treat procedure names differently from other names.  Thus, she
argues for the mechanism implemented in @ref{Exercise 4-16}.  This would lead
to @code{a} being unassigned at the time that the value for @code{b} is to be
computed.  Hence, in Alyssa's view the procedure should produce an error.  Eva
has a third opinion.  She says that if the definitions of @code{a} and @code{b}
are truly meant to be simultaneous, then the value 5 for @code{a} should be
used in evaluating @code{b}.  Hence, in Eva's view @code{a} should be 5,
@code{b} should be 15, and the result should be 20.  Which (if any) of these
viewpoints do you support?  Can you devise a way to implement internal
definitions so that they behave as Eva prefers?@footnote{The @acronym{MIT}
implementors of Scheme support Alyssa on the following grounds: Eva is in
principle correct -- the definitions should be regarded as simultaneous.  But
it seems difficult to implement a general, efficient mechanism that does what
Eva requires.  In the absence of such a mechanism, it is better to generate an
error in the difficult cases of simultaneous definitions (Alyssa's notion) than
to produce an incorrect answer (as Ben would have it).}
@end quotation

@quotation
@strong{@anchor{Exercise 4-20}Exercise 4.20:} Because internal definitions look
sequential but are actually simultaneous, some people prefer to avoid them
entirely, and use the special form @code{letrec} instead.  @code{Letrec} looks
like @code{let}, so it is not surprising that the variables it binds are bound
simultaneously and have the same scope as each other.  The sample procedure
@code{f} above can be written without internal definitions, but with exactly
the same meaning, as

@lisp
(define (f x)
  (letrec ((even?
            (lambda (n)
              (if (= n 0)
                  true
                  (odd? (- n 1)))))
           (odd?
            (lambda (n)
              (if (= n 0)
                  false
                  (even? (- n 1))))))
    <@var{rest of body of @code{f}}>))
@end lisp

@code{Letrec} expressions, which have the form

@lisp
(letrec ((<@var{var_1}> <@var{exp_1}>) @dots{} (<@var{var_@i{n}}> <@var{exp_@i{n}}>))
  <@var{body}>)
@end lisp

@noindent
are a variation on @code{let} in which the expressions
<@var{exp_@i{k}}> that provide the initial values for the
variables <@var{var_@i{k}}> are evaluated in an environment
that includes all the @code{letrec} bindings.  This permits recursion in the
bindings, such as the mutual recursion of @code{even?} and @code{odd?} in the
example above, or the evaluation of 10 factorial with

@lisp
(letrec ((fact
          (lambda (n)
            (if (= n 1)
                1
                (* n (fact (- n 1)))))))
  (fact 10))
@end lisp

@enumerate a

@item
Implement @code{letrec} as a derived expression, by transforming a
@code{letrec} expression into a @code{let} expression as shown in the text
above or in @ref{Exercise 4-18}.  That is, the @code{letrec} variables should
be created with a @code{let} and then be assigned their values with
@code{set!}.

@item
Louis Reasoner is confused by all this fuss about internal definitions.  The
way he sees it, if you don't like to use @code{define} inside a procedure, you
can just use @code{let}.  Illustrate what is loose about his reasoning by
drawing an environment diagram that shows the environment in which the
<@var{rest of body of @code{f}}> is evaluated during evaluation of the
expression @code{(f 5)}, with @code{f} defined as in this exercise.  Draw an
environment diagram for the same evaluation, but with @code{let} in place of
@code{letrec} in the definition of @code{f}.

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-21}Exercise 4.21:} Amazingly, Louis's intuition in
@ref{Exercise 4-20} is correct.  It is indeed possible to specify recursive
procedures without using @code{letrec} (or even @code{define}), although the
method for accomplishing this is much more subtle than Louis imagined.  The
following expression computes 10 factorial by applying a recursive factorial
procedure:@footnote{This example illustrates a programming trick for
formulating recursive procedures without using @code{define}.  The most general
trick of this sort is the @i{Y} @newterm{operator}, which can be used to give a
``pure @i{[lambda]}-calculus'' implementation of recursion.  (See Stoy 1977 for
details on the @i{[lambda]} calculus, and Gabriel 1988 for an exposition of the
@i{Y} operator in Scheme.)}

@lisp
((lambda (n)
   ((lambda (fact)
      (fact fact n))
    (lambda (ft k)
      (if (= k 1)
          1
          (* k (ft ft (- k 1)))))))
 10)
@end lisp

@enumerate a

@item
Check (by evaluating the expression) that this really does compute factorials.
Devise an analogous expression for computing Fibonacci numbers.

@item
Consider the following procedure, which includes mutually recursive internal
definitions:

@lisp
(define (f x)
  (define (even? n)
    (if (= n 0)
        true
        (odd? (- n 1))))
  (define (odd? n)
    (if (= n 0)
        false
        (even? (- n 1))))
  (even? x))
@end lisp

Fill in the missing expressions to complete an alternative definition of
@code{f}, which uses neither internal definitions nor @code{letrec}:

@lisp
(define (f x)
  ((lambda (even? odd?)
     (even? even? odd? x))
   (lambda (ev? od? n)
     (if (= n 0) true (od? <??> <??> <??>)))
   (lambda (ev? od? n)
     (if (= n 0) false (ev? <??> <??> <??>)))))
@end lisp
@end enumerate
@end quotation

@node	4-1-7,  , 4-1-6, 4-1
@subsection Separating Syntactic Analysis from Execution

The evaluator implemented above is simple, but it is very inefficient, because
the syntactic analysis of expressions is interleaved with their execution.
Thus if a program is executed many times, its syntax is analyzed many times.
Consider, for example, evaluating @code{(factorial 4)} using the following
definition of @code{factorial}:

@lisp
(define (factorial n)
  (if (= n 1)
      1
      (* (factorial (- n 1)) n)))
@end lisp

Each time @code{factorial} is called, the evaluator must determine that the
body is an @code{if} expression and extract the predicate.  Only then can it
evaluate the predicate and dispatch on its value.  Each time it evaluates the
expression @code{(* (factorial (- n 1)) n)}, or the subexpressions
@code{(factorial (- n 1))} and @code{(- n 1)}, the evaluator must perform the
case analysis in @code{eval} to determine that the expression is an
application, and must extract its operator and operands.  This analysis is
expensive.  Performing it repeatedly is wasteful.

We can transform the evaluator to be significantly more efficient by arranging
things so that syntactic analysis is performed only once.@footnote{This
technique is an integral part of the compilation process, which we shall
discuss in @ref{Chapter 5}.  Jonathan Rees wrote a Scheme interpreter like this
in about 1982 for the T project (Rees and Adams 1982).  Marc Feeley (1986) (see
also Feeley and Lapalme 1987) independently invented this technique in his
master's thesis.} We split @code{eval}, which takes an expression and an
environment, into two parts.  The procedure @code{analyze} takes only the
expression.  It performs the syntactic analysis and returns a new procedure,
the @newterm{execution procedure}, that encapsulates the work to be done in
executing the analyzed expression.  The execution procedure takes an
environment as its argument and completes the evaluation.  This saves work
because @code{analyze} will be called only once on an expression, while the
execution procedure may be called many times.

With the separation into analysis and execution, @code{eval} now becomes

@lisp
(define (eval exp env)
  ((analyze exp) env))
@end lisp

The result of calling @code{analyze} is the execution procedure to be applied
to the environment.  The @code{analyze} procedure is the same case analysis as
performed by the original @code{eval} of section @ref{4-1-1}, except that the
procedures to which we dispatch perform only analysis, not full evaluation:

@lisp
(define (analyze exp)
  (cond ((self-evaluating? exp)
         (analyze-self-evaluating exp))
        ((quoted? exp) (analyze-quoted exp))
        ((variable? exp) (analyze-variable exp))
        ((assignment? exp) (analyze-assignment exp))
        ((definition? exp) (analyze-definition exp))
        ((if? exp) (analyze-if exp))
        ((lambda? exp) (analyze-lambda exp))
        ((begin? exp) (analyze-sequence (begin-actions exp)))
        ((cond? exp) (analyze (cond->if exp)))
        ((application? exp) (analyze-application exp))
        (else
         (error "Unknown expression type -- ANALYZE" exp))))
@end lisp

Here is the simplest syntactic analysis procedure, which handles
self-evaluating expressions.  It returns an execution procedure that ignores
its environment argument and just returns the expression:

@lisp
(define (analyze-self-evaluating exp)
  (lambda (env) exp))
@end lisp

For a quoted expression, we can gain a little efficiency by extracting the text
of the quotation only once, in the analysis phase, rather than in the execution
phase.

@lisp
(define (analyze-quoted exp)
  (let ((qval (text-of-quotation exp)))
    (lambda (env) qval)))
@end lisp

Looking up a variable value must still be done in the execution phase, since
this depends upon knowing the environment.@footnote{There is, however, an
important part of the variable search that @emph{can} be done as part of the
syntactic analysis.  As we will show in section @ref{5-5-6}, one can determine
the position in the environment structure where the value of the variable will
be found, thus obviating the need to scan the environment for the entry that
matches the variable.}

@lisp
(define (analyze-variable exp)
  (lambda (env) (lookup-variable-value exp env)))
@end lisp

@code{Analyze-assignment} also must defer actually setting the variable until
the execution, when the environment has been supplied.  However, the fact that
the @code{assignment-value} expression can be analyzed (recursively) during
analysis is a major gain in efficiency, because the @code{assignment-value}
expression will now be analyzed only once.  The same holds true for
definitions.

@lisp
(define (analyze-assignment exp)
  (let ((var (assignment-variable exp))
        (vproc (analyze (assignment-value exp))))
    (lambda (env)
      (set-variable-value! var (vproc env) env)
      'ok)))

(define (analyze-definition exp)
  (let ((var (definition-variable exp))
        (vproc (analyze (definition-value exp))))
    (lambda (env)
      (define-variable! var (vproc env) env)
      'ok)))
@end lisp

For @code{if} expressions, we extract and analyze the predicate, consequent,
and alternative at analysis time.

@lisp
(define (analyze-if exp)
  (let ((pproc (analyze (if-predicate exp)))
        (cproc (analyze (if-consequent exp)))
        (aproc (analyze (if-alternative exp))))
    (lambda (env)
      (if (true? (pproc env))
          (cproc env)
          (aproc env)))))
@end lisp

Analyzing a @code{lambda} expression also achieves a major gain in efficiency:
We analyze the @code{lambda} body only once, even though procedures resulting
from evaluation of the @code{lambda} may be applied many times.

@lisp
(define (analyze-lambda exp)
  (let ((vars (lambda-parameters exp))
        (bproc (analyze-sequence (lambda-body exp))))
    (lambda (env) (make-procedure vars bproc env))))
@end lisp

Analysis of a sequence of expressions (as in a @code{begin} or the body of a
@code{lambda} expression) is more involved.@footnote{See @ref{Exercise 4-23}
for some insight into the processing of sequences.} Each expression in the
sequence is analyzed, yielding an execution procedure.  These execution
procedures are combined to produce an execution procedure that takes an
environment as argument and sequentially calls each individual execution
procedure with the environment as argument.

@lisp
(define (analyze-sequence exps)
  (define (sequentially proc1 proc2)
    (lambda (env) (proc1 env) (proc2 env)))
  (define (loop first-proc rest-procs)
    (if (null? rest-procs)
        first-proc
        (loop (sequentially first-proc (car rest-procs))
              (cdr rest-procs))))
  (let ((procs (map analyze exps)))
    (if (null? procs)
        (error "Empty sequence -- ANALYZE"))
    (loop (car procs) (cdr procs))))
@end lisp

To analyze an application, we analyze the operator and operands and construct
an execution procedure that calls the operator execution procedure (to obtain
the actual procedure to be applied) and the operand execution procedures (to
obtain the actual arguments).  We then pass these to
@code{execute-application}, which is the analog of @code{apply} in section
@ref{4-1-1}.  @code{Execute-application} differs from @code{apply} in that the
procedure body for a compound procedure has already been analyzed, so there is
no need to do further analysis.  Instead, we just call the execution procedure
for the body on the extended environment.

@lisp
(define (analyze-application exp)
  (let ((fproc (analyze (operator exp)))
        (aprocs (map analyze (operands exp))))
    (lambda (env)
      (execute-application (fproc env)
                           (map (lambda (aproc) (aproc env))
                                aprocs)))))

(define (execute-application proc args)
  (cond ((primitive-procedure? proc)
         (apply-primitive-procedure proc args))
        ((compound-procedure? proc)
         ((procedure-body proc)
          (extend-environment (procedure-parameters proc)
                              args
                              (procedure-environment proc))))
        (else
         (error
          "Unknown procedure type -- EXECUTE-APPLICATION"
          proc))))
@end lisp

Our new evaluator uses the same data structures, syntax procedures, and
run-time support procedures as in sections @ref{4-1-2}, @ref{4-1-3}, and
@ref{4-1-4}.

@quotation
@strong{@anchor{Exercise 4-22}Exercise 4.22:} Extend the evaluator in this
section to support the special form @code{let}.  (See @ref{Exercise 4-6}.)
@end quotation

@quotation
@strong{@anchor{Exercise 4-23}Exercise 4.23:} Alyssa P. Hacker doesn't
understand why @code{analyze-sequence} needs to be so complicated.  All the
other analysis procedures are straightforward transformations of the
corresponding evaluation procedures (or @code{eval} clauses) in section
@ref{4-1-1}.  She expected @code{analyze-sequence} to look like this:

@lisp
(define (analyze-sequence exps)
  (define (execute-sequence procs env)
    (cond ((null? (cdr procs)) ((car procs) env))
          (else ((car procs) env)
                (execute-sequence (cdr procs) env))))
  (let ((procs (map analyze exps)))
    (if (null? procs)
        (error "Empty sequence -- ANALYZE"))
    (lambda (env) (execute-sequence procs env))))
@end lisp

Eva Lu Ator explains to Alyssa that the version in the text does more of the
work of evaluating a sequence at analysis time.  Alyssa's sequence-execution
procedure, rather than having the calls to the individual execution procedures
built in, loops through the procedures in order to call them: In effect,
although the individual expressions in the sequence have been analyzed, the
sequence itself has not been.

Compare the two versions of @code{analyze-sequence}.  For example, consider the
common case (typical of procedure bodies) where the sequence has just one
expression.  What work will the execution procedure produced by Alyssa's
program do?  What about the execution procedure produced by the program in the
text above?  How do the two versions compare for a sequence with two
expressions?
@end quotation

@quotation
@strong{@anchor{Exercise 4-24}Exercise 4.24:} Design and carry out some
experiments to compare the speed of the original metacircular evaluator with
the version in this section.  Use your results to estimate the fraction of time
that is spent in analysis versus execution for various procedures.
@end quotation

@node	4-2, 4-3, 4-1, Chapter 4
@section Variations on a Scheme -- Lazy Evaluation

Now that we have an evaluator expressed as a Lisp program, we can experiment
with alternative choices in language design simply by modifying the evaluator.
Indeed, new languages are often invented by first writing an evaluator that
embeds the new language within an existing high-level language.  For example,
if we wish to discuss some aspect of a proposed modification to Lisp with
another member of the Lisp community, we can supply an evaluator that embodies
the change.  The recipient can then experiment with the new evaluator and send
back comments as further modifications.  Not only does the high-level
implementation base make it easier to test and debug the evaluator; in
addition, the embedding enables the designer to snarf@footnote{Snarf: ``To
grab, especially a large document or file for the purpose of using it either
with or without the owner's permission.''  Snarf down: ``To snarf, sometimes
with the connotation of absorbing, processing, or understanding.''  (These
definitions were snarfed from Steele et al. 1983.  See also Raymond 1993.)}
features from the underlying language, just as our embedded Lisp evaluator uses
primitives and control structure from the underlying Lisp.  Only later (if
ever) need the designer go to the trouble of building a complete implementation
in a low-level language or in hardware.  In this section and the next we
explore some variations on Scheme that provide significant additional
expressive power.

@menu
* 4-2-1::            Normal Order and Applicative Order
* 4-2-2::            An Interpreter with Lazy Evaluation
* 4-2-3::            Streams as Lazy Lists
@end menu

@node	4-2-1, 4-2-2, 4-2, 4-2
@subsection Normal Order and Applicative Order

In section @ref{1-1}, where we began our discussion of models of evaluation, we
noted that Scheme is an @newterm{applicative-order} language, namely, that all
the arguments to Scheme procedures are evaluated when the procedure is applied.
In contrast, @newterm{normal-order} languages delay evaluation of procedure
arguments until the actual argument values are needed.  Delaying evaluation of
procedure arguments until the last possible moment (e.g., until they are
required by a primitive operation) is called @newterm{lazy
evaluation}.@footnote{The difference between the ``lazy'' terminology and the
``normal-order'' terminology is somewhat fuzzy.  Generally, ``lazy'' refers to
the mechanisms of particular evaluators, while ``normal-order'' refers to the
semantics of languages, independent of any particular evaluation strategy.  But
this is not a hard-and-fast distinction, and the two terminologies are often
used interchangeably.}  Consider the procedure

@lisp
(define (try a b)
  (if (= a 0) 1 b))
@end lisp

Evaluating @code{(try 0 (/ 1 0))} generates an error in Scheme.  With lazy
evaluation, there would be no error.  Evaluating the expression would return 1,
because the argument @code{(/ 1 0)} would never be evaluated.

An example that exploits lazy evaluation is the definition of a procedure
@code{unless}

@lisp
(define (unless condition usual-value exceptional-value)
  (if condition exceptional-value usual-value))
@end lisp

@noindent
that can be used in expressions such as

@lisp
(unless (= b 0)
        (/ a b)
        (begin (display "exception: returning 0")
               0))
@end lisp

This won't work in an applicative-order language because both the usual value
and the exceptional value will be evaluated before @code{unless} is called
(compare @ref{Exercise 1-6}).  An advantage of lazy evaluation is that some
procedures, such as @code{unless}, can do useful computation even if evaluation
of some of their arguments would produce errors or would not terminate.

If the body of a procedure is entered before an argument has been evaluated we
say that the procedure is @newterm{non-strict} in that argument.  If the
argument is evaluated before the body of the procedure is entered we say that
the procedure is @newterm{strict} in that argument.@footnote{The ``strict''
versus ``non-strict'' terminology means essentially the same thing as
``applicative-order'' versus ``normal-order,'' except that it refers to
individual procedures and arguments rather than to the language as a whole.  At
a conference on programming languages you might hear someone say, ``The
normal-order language Hassle has certain strict primitives.  Other procedures
take their arguments by lazy evaluation.''}  In a purely applicative-order
language, all procedures are strict in each argument.  In a purely normal-order
language, all compound procedures are non-strict in each argument, and
primitive procedures may be either strict or non-strict.  There are also
languages (see @ref{Exercise 4-31}) that give programmers detailed control over
the strictness of the procedures they define.

A striking example of a procedure that can usefully be made non-strict is
@code{cons} (or, in general, almost any constructor for data structures).  One
can do useful computation, combining elements to form data structures and
operating on the resulting data structures, even if the values of the elements
are not known.  It makes perfect sense, for instance, to compute the length of
a list without knowing the values of the individual elements in the list.  We
will exploit this idea in section @ref{4-2-3} to implement the streams of
@ref{Chapter 3} as lists formed of non-strict @code{cons} pairs.

@quotation
@strong{@anchor{Exercise 4-25}Exercise 4.25:} Suppose that (in ordinary
applicative-order Scheme) we define @code{unless} as shown above and then
define @code{factorial} in terms of @code{unless} as

@lisp
(define (factorial n)
  (unless (= n 1)
          (* n (factorial (- n 1)))
          1))
@end lisp

What happens if we attempt to evaluate @code{(factorial 5)}?  Will our
definitions work in a normal-order language?
@end quotation

@quotation
@strong{@anchor{Exercise 4-26}Exercise 4.26:} Ben Bitdiddle and Alyssa
P. Hacker disagree over the importance of lazy evaluation for implementing
things such as @code{unless}.  Ben points out that it's possible to implement
@code{unless} in applicative order as a special form.  Alyssa counters that, if
one did that, @code{unless} would be merely syntax, not a procedure that could
be used in conjunction with higher-order procedures.  Fill in the details on
both sides of the argument.  Show how to implement @code{unless} as a derived
expression (like @code{cond} or @code{let}), and give an example of a situation
where it might be useful to have @code{unless} available as a procedure, rather
than as a special form.
@end quotation

@node	4-2-2, 4-2-3, 4-2-1, 4-2
@subsection An Interpreter with Lazy Evaluation

In this section we will implement a normal-order language that is the same as
Scheme except that compound procedures are non-strict in each argument.
Primitive procedures will still be strict.  It is not difficult to modify the
evaluator of section @ref{4-1-1} so that the language it interprets behaves
this way.  Almost all the required changes center around procedure application.

The basic idea is that, when applying a procedure, the interpreter must
determine which arguments are to be evaluated and which are to be delayed.  The
delayed arguments are not evaluated; instead, they are transformed into objects
called @newterm{thunks}.@footnote{The word @newterm{thunk} was invented by an
informal working group that was discussing the implementation of call-by-name
in Algol 60.  They observed that most of the analysis of (``thinking about'')
the expression could be done at compile time; thus, at run time, the expression
would already have been ``thunk'' about (Ingerman et al. 1960).} The thunk must
contain the information required to produce the value of the argument when it
is needed, as if it had been evaluated at the time of the application.  Thus,
the thunk must contain the argument expression and the environment in which the
procedure application is being evaluated.

The process of evaluating the expression in a thunk is called
@newterm{forcing}.@footnote{This is analogous to the use of @code{force} on the
delayed objects that were introduced in @ref{Chapter 3} to represent streams.
The critical difference between what we are doing here and what we did in
@ref{Chapter 3} is that we are building delaying and forcing into the
evaluator, and thus making this uniform and automatic throughout the language.}
In general, a thunk will be forced only when its value is needed: when it is
passed to a primitive procedure that will use the value of the thunk; when it
is the value of a predicate of a conditional; and when it is the value of an
operator that is about to be applied as a procedure.  One design choice we have
available is whether or not to @newterm{memoize} thunks, as we did with delayed
objects in section @ref{3-5-1}.  With memoization, the first time a thunk is
forced, it stores the value that is computed.  Subsequent forcings simply
return the stored value without repeating the computation.  We'll make our
interpreter memoize, because this is more efficient for many applications.
There are tricky considerations here, however.@footnote{Lazy evaluation
combined with memoization is sometimes referred to as @newterm{call-by-need}
argument passing, in contrast to @newterm{call-by-name} argument passing.
(Call-by-name, introduced in Algol 60, is similar to non-memoized lazy
evaluation.)  As language designers, we can build our evaluator to memoize, not
to memoize, or leave this an option for programmers (@ref{Exercise 4-31}).  As
you might expect from @ref{Chapter 3}, these choices raise issues that become
both subtle and confusing in the presence of assignments.  (See @ref{Exercise
4-27} and @ref{Exercise 4-29}.)  An excellent article by Clinger (1982)
attempts to clarify the multiple dimensions of confusion that arise here.}

@subsubheading Modifying the evaluator

The main difference between the lazy evaluator and the one in section @ref{4-1}
is in the handling of procedure applications in @code{eval} and @code{apply}.

The @code{application?} clause of @code{eval} becomes

@lisp
((application? exp)
 (apply (actual-value (operator exp) env)
        (operands exp)
        env))
@end lisp

This is almost the same as the @code{application?} clause of @code{eval} in
section @ref{4-1-1}.  For lazy evaluation, however, we call @code{apply} with
the operand expressions, rather than the arguments produced by evaluating them.
Since we will need the environment to construct thunks if the arguments are to
be delayed, we must pass this as well.  We still evaluate the operator, because
@code{apply} needs the actual procedure to be applied in order to dispatch on
its type (primitive versus compound) and apply it.

Whenever we need the actual value of an expression, we use

@lisp
(define (actual-value exp env)
  (force-it (eval exp env)))
@end lisp

@noindent
instead of just @code{eval}, so that if the expression's value is a thunk, it
will be forced.

Our new version of @code{apply} is also almost the same as the version in
section @ref{4-1-1}.  The difference is that @code{eval} has passed in
unevaluated operand expressions: For primitive procedures (which are strict),
we evaluate all the arguments before applying the primitive; for compound
procedures (which are non-strict) we delay all the arguments before applying
the procedure.

@lisp
(define (apply procedure arguments env)
  (cond ((primitive-procedure? procedure)
         (apply-primitive-procedure
          procedure
          (list-of-arg-values arguments env)))  @r{; changed}
        ((compound-procedure? procedure)
         (eval-sequence
          (procedure-body procedure)
          (extend-environment
           (procedure-parameters procedure)
           (list-of-delayed-args arguments env) @r{; changed}
           (procedure-environment procedure))))
        (else
         (error
          "Unknown procedure type -- APPLY" procedure))))
@end lisp

The procedures that process the arguments are just like @code{list-of-values}
from section @ref{4-1-1}, except that @code{list-of-delayed-args} delays the
arguments instead of evaluating them, and @code{list-of-arg-values} uses
@code{actual-value} instead of @code{eval}:

@lisp
(define (list-of-arg-values exps env)
  (if (no-operands? exps)
      '()
      (cons (actual-value (first-operand exps) env)
            (list-of-arg-values (rest-operands exps)
                                env))))

(define (list-of-delayed-args exps env)
  (if (no-operands? exps)
      '()
      (cons (delay-it (first-operand exps) env)
            (list-of-delayed-args (rest-operands exps)
                                  env))))
@end lisp

The other place we must change the evaluator is in the handling of @code{if},
where we must use @code{actual-value} instead of @code{eval} to get the value
of the predicate expression before testing whether it is true or false:

@lisp
(define (eval-if exp env)
  (if (true? (actual-value (if-predicate exp) env))
      (eval (if-consequent exp) env)
      (eval (if-alternative exp) env)))
@end lisp

Finally, we must change the @code{driver-loop} procedure (section @ref{4-1-4})
to use @code{actual-value} instead of @code{eval}, so that if a delayed value
is propagated back to the read-eval-print loop, it will be forced before being
printed.  We also change the prompts to indicate that this is the lazy
evaluator:

@lisp
(define input-prompt ";;; L-Eval input:")
(define output-prompt ";;; L-Eval value:")

(define (driver-loop)
  (prompt-for-input input-prompt)
  (let ((input (read)))
    (let ((output
           (actual-value input the-global-environment)))
      (announce-output output-prompt)
      (user-print output)))
  (driver-loop))
@end lisp

With these changes made, we can start the evaluator and test it.  The
successful evaluation of the @code{try} expression discussed in section
@ref{4-2-1} indicates that the interpreter is performing lazy evaluation:

@lisp
(define the-global-environment (setup-environment))

(driver-loop)

@i{;;; L-Eval input:}
(define (try a b)
  (if (= a 0) 1 b))
@i{;;; L-Eval value:}
@i{ok}

@i{;;; L-Eval input:}
(try 0 (/ 1 0))
@i{;;; L-Eval value:}
@i{1}
@end lisp

@subsubheading Representing thunks

Our evaluator must arrange to create thunks when procedures are applied to
arguments and to force these thunks later.  A thunk must package an expression
together with the environment, so that the argument can be produced later.  To
force the thunk, we simply extract the expression and environment from the
thunk and evaluate the expression in the environment.  We use
@code{actual-value} rather than @code{eval} so that in case the value of the
expression is itself a thunk, we will force that, and so on, until we reach
something that is not a thunk:

@lisp
(define (force-it obj)
  (if (thunk? obj)
      (actual-value (thunk-exp obj) (thunk-env obj))
      obj))
@end lisp

One easy way to package an expression with an environment is to make a list
containing the expression and the environment.  Thus, we create a thunk as
follows:

@lisp
(define (delay-it exp env)
  (list 'thunk exp env))

(define (thunk? obj)
  (tagged-list? obj 'thunk))

(define (thunk-exp thunk) (cadr thunk))

(define (thunk-env thunk) (caddr thunk))
@end lisp

Actually, what we want for our interpreter is not quite this, but rather thunks
that have been memoized.  When a thunk is forced, we will turn it into an
evaluated thunk by replacing the stored expression with its value and changing
the @code{thunk} tag so that it can be recognized as already
evaluated.@footnote{Notice that we also erase the @code{env} from the thunk
once the expression's value has been computed.  This makes no difference in the
values returned by the interpreter.  It does help save space, however, because
removing the reference from the thunk to the @code{env} once it is no longer
needed allows this structure to be @newterm{garbage-collected} and its space
recycled, as we will discuss in section @ref{5-3}.

Similarly, we could have allowed unneeded environments in the memoized delayed
objects of section @ref{3-5-1} to be garbage-collected, by having
@code{memo-proc} do something like @code{(set! proc '())} to discard the
procedure @code{proc} (which includes the environment in which the @code{delay}
was evaluated) after storing its value.}

@lisp
(define (evaluated-thunk? obj)
  (tagged-list? obj 'evaluated-thunk))

(define (thunk-value evaluated-thunk) (cadr evaluated-thunk))

(define (force-it obj)
  (cond ((thunk? obj)
         (let ((result (actual-value
                        (thunk-exp obj)
                        (thunk-env obj))))
           (set-car! obj 'evaluated-thunk)
           (set-car! (cdr obj) result)  @r{; replace @code{exp} with its value}
           (set-cdr! (cdr obj) '())     @r{; forget unneeded @code{env}}
           result))
        ((evaluated-thunk? obj)
         (thunk-value obj))
        (else obj)))
@end lisp

Notice that the same @code{delay-it} procedure works both with and without
memoization.

@quotation
@strong{@anchor{Exercise 4-27}Exercise 4.27:} Suppose we type in the following
definitions to the lazy evaluator:

@lisp
(define count 0)

(define (id x)
  (set! count (+ count 1))
  x)
@end lisp

Give the missing values in the following sequence of interactions, and explain
your answers.@footnote{This exercise demonstrates that the interaction between
lazy evaluation and side effects can be very confusing.  This is just what you
might expect from the discussion in @ref{Chapter 3}.}

@lisp
(define w (id (id 10)))

@i{;;; L-Eval input:}
count
@i{;;; L-Eval value:}
<@var{response}>

@i{;;; L-Eval input:}
w
@i{;;; L-Eval value:}
<@var{response}>

@i{;;; L-Eval input:}
count
@i{;;; L-Eval value:}
<@var{response}>
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 4-28}Exercise 4.28:} @code{Eval} uses
@code{actual-value} rather than @code{eval} to evaluate the operator before
passing it to @code{apply}, in order to force the value of the operator.  Give
an example that demonstrates the need for this forcing.

@noindent
@strong{@anchor{Exercise 4-29}Exercise 4.29:} Exhibit a program that you would
expect to run much more slowly without memoization than with memoization.
Also, consider the following interaction, where the @code{id} procedure is
defined as in @ref{Exercise 4-27} and @code{count} starts at 0:

@lisp
(define (square x)
  (* x x))

@i{;;; L-Eval input:}
(square (id 10))
@i{;;; L-Eval value:}
<@var{response}>

@i{;;; L-Eval input:}
count
@i{;;; L-Eval value:}
<@var{response}>
@end lisp

Give the responses both when the evaluator memoizes and when it does not.
@end quotation

@quotation
@strong{@anchor{Exercise 4-30}Exercise 4.30:} Cy D. Fect, a reformed C
programmer, is worried that some side effects may never take place, because the
lazy evaluator doesn't force the expressions in a sequence.  Since the value of
an expression in a sequence other than the last one is not used (the expression
is there only for its effect, such as assigning to a variable or printing),
there can be no subsequent use of this value (e.g., as an argument to a
primitive procedure) that will cause it to be forced.  Cy thus thinks that when
evaluating sequences, we must force all expressions in the sequence except the
final one.  He proposes to modify @code{eval-sequence} from section @ref{4-1-1}
to use @code{actual-value} rather than @code{eval}:

@lisp
(define (eval-sequence exps env)
  (cond ((last-exp? exps) (eval (first-exp exps) env))
        (else (actual-value (first-exp exps) env)
              (eval-sequence (rest-exps exps) env))))
@end lisp

@enumerate a

@item
Ben Bitdiddle thinks Cy is wrong.  He shows Cy the @code{for-each} procedure
described in @ref{Exercise 2-23}, which gives an important example of a
sequence with side effects:

@lisp
(define (for-each proc items)
  (if (null? items)
      'done
      (begin (proc (car items))
             (for-each proc (cdr items)))))
@end lisp

He claims that the evaluator in the text (with the original
@code{eval-sequence}) handles this correctly:

@lisp
@i{;;; L-Eval input:}
(for-each (lambda (x) (newline) (display x))
          (list 57 321 88))
@i{57}
@i{321}
@i{88}
@i{;;; L-Eval value:}
@i{done}
@end lisp

Explain why Ben is right about the behavior of @code{for-each}.

@item
Cy agrees that Ben is right about the @code{for-each} example, but says that
that's not the kind of program he was thinking about when he proposed his
change to @code{eval-sequence}.  He defines the following two procedures in the
lazy evaluator:

@lisp
(define (p1 x)
  (set! x (cons x '(2)))
  x)

(define (p2 x)
  (define (p e)
    e
    x)
  (p (set! x (cons x '(2)))))
@end lisp

What are the values of @code{(p1 1)} and @code{(p2 1)} with the original
@code{eval-sequence}?  What would the values be with Cy's proposed change to
@code{eval-sequence}?

@item
Cy also points out that changing @code{eval-sequence} as he proposes does not
affect the behavior of the example in part a.  Explain why this is true.

@item
How do you think sequences ought to be treated in the lazy evaluator?  Do you
like Cy's approach, the approach in the text, or some other approach?

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-31}Exercise 4.31:} The approach taken in this
section is somewhat unpleasant, because it makes an incompatible change to
Scheme.  It might be nicer to implement lazy evaluation as an
@newterm{upward-compatible extension}, that is, so that ordinary Scheme
programs will work as before.  We can do this by extending the syntax of
procedure declarations to let the user control whether or not arguments are to
be delayed.  While we're at it, we may as well also give the user the choice
between delaying with and without memoization.  For example, the definition

@lisp
(define (f a (b lazy) c (d lazy-memo))
  @dots{})
@end lisp

@noindent
would define @code{f} to be a procedure of four arguments, where the first and
third arguments are evaluated when the procedure is called, the second argument
is delayed, and the fourth argument is both delayed and memoized.  Thus,
ordinary procedure definitions will produce the same behavior as ordinary
Scheme, while adding the @code{lazy-memo} declaration to each parameter of
every compound procedure will produce the behavior of the lazy evaluator
defined in this section. Design and implement the changes required to produce
such an extension to Scheme.  You will have to implement new syntax procedures
to handle the new syntax for @code{define}.  You must also arrange for
@code{eval} or @code{apply} to determine when arguments are to be delayed, and
to force or delay arguments accordingly, and you must arrange for forcing to
memoize or not, as appropriate.
@end quotation

@node	4-2-3,  , 4-2-2, 4-2
@subsection Streams as Lazy Lists

In section @ref{3-5-1}, we showed how to implement streams as delayed lists.
We introduced special forms @code{delay} and @code{cons-stream}, which allowed
us to construct a ``promise'' to compute the @code{cdr} of a stream, without
actually fulfilling that promise until later.  We could use this general
technique of introducing special forms whenever we need more control over the
evaluation process, but this is awkward.  For one thing, a special form is not
a first-class object like a procedure, so we cannot use it together with
higher-order procedures.@footnote{This is precisely the issue with the
@code{unless} procedure, as in @ref{Exercise 4-26}.}  Additionally, we were
forced to create streams as a new kind of data object similar but not identical
to lists, and this required us to reimplement many ordinary list operations
(@code{map}, @code{append}, and so on) for use with streams.

With lazy evaluation, streams and lists can be identical, so there is no need
for special forms or for separate list and stream operations.  All we need to
do is to arrange matters so that @code{cons} is non-strict.  One way to
accomplish this is to extend the lazy evaluator to allow for non-strict
primitives, and to implement @code{cons} as one of these.  An easier way is to
recall (section @ref{2-1-3}) that there is no fundamental need to implement
@code{cons} as a primitive at all.  Instead, we can represent pairs as
procedures:@footnote{This is the procedural representation described in
@ref{Exercise 2-4}.  Essentially any procedural representation (e.g., a
message-passing implementation) would do as well.  Notice that we can install
these definitions in the lazy evaluator simply by typing them at the driver
loop.  If we had originally included @code{cons}, @code{car}, and @code{cdr} as
primitives in the global environment, they will be redefined.  (Also see
@ref{Exercise 4-33} and @ref{Exercise 4-34}.)}

@lisp
(define (cons x y)
  (lambda (m) (m x y)))

(define (car z)
  (z (lambda (p q) p)))

(define (cdr z)
  (z (lambda (p q) q)))
@end lisp

In terms of these basic operations, the standard definitions of the list
operations will work with infinite lists (streams) as well as finite ones, and
the stream operations can be implemented as list operations.  Here are some
examples:

@lisp
(define (list-ref items n)
  (if (= n 0)
      (car items)
      (list-ref (cdr items) (- n 1))))

(define (map proc items)
  (if (null? items)
      '()
      (cons (proc (car items))
            (map proc (cdr items)))))

(define (scale-list items factor)
  (map (lambda (x) (* x factor))
       items))

(define (add-lists list1 list2)
  (cond ((null? list1) list2)
        ((null? list2) list1)
        (else (cons (+ (car list1) (car list2))
                    (add-lists (cdr list1) (cdr list2))))))

(define ones (cons 1 ones))

(define integers (cons 1 (add-lists ones integers)))

@i{;;; L-Eval input:}
(list-ref integers 17)
@i{;;; L-Eval value:}
@i{18}
@end lisp

Note that these lazy lists are even lazier than the streams of @ref{Chapter 3}:
The @code{car} of the list, as well as the @code{cdr}, is
delayed.@footnote{This permits us to create delayed versions of more general
kinds of list structures, not just sequences.  Hughes 1990 discusses some
applications of ``lazy trees.''}  In fact, even accessing the @code{car} or
@code{cdr} of a lazy pair need not force the value of a list element.  The
value will be forced only when it is really needed -- e.g., for use as the
argument of a primitive, or to be printed as an answer.

Lazy pairs also help with the problem that arose with streams in section
@ref{3-5-4}, where we found that formulating stream models of systems with
loops may require us to sprinkle our programs with explicit @code{delay}
operations, beyond the ones supplied by @code{cons-stream}.  With lazy
evaluation, all arguments to procedures are delayed uniformly.  For instance,
we can implement procedures to integrate lists and solve differential equations
as we originally intended in section @ref{3-5-4}:

@lisp
(define (integral integrand initial-value dt)
  (define int
    (cons initial-value
          (add-lists (scale-list integrand dt)
                    int)))
  int)

(define (solve f y0 dt)
  (define y (integral dy y0 dt))
  (define dy (map f y))
  y)

@i{;;; L-Eval input:}
(list-ref (solve (lambda (x) x) 1 0.001) 1000)
@i{;;; L-Eval value:}
@i{2.716924}
@end lisp

@quotation
@strong{@anchor{Exercise 4-32}Exercise 4.32:} Give some examples that
illustrate the difference between the streams of @ref{Chapter 3} and the
``lazier'' lazy lists described in this section.  How can you take advantage of
this extra laziness?
@end quotation

@quotation
@strong{@anchor{Exercise 4-33}Exercise 4.33:} Ben Bitdiddle tests the lazy list
implementation given above by evaluating the expression

@lisp
(car '(a b c))
@end lisp

To his surprise, this produces an error.  After some thought, he realizes that
the ``lists'' obtained by reading in quoted expressions are different from the
lists manipulated by the new definitions of @code{cons}, @code{car}, and
@code{cdr}.  Modify the evaluator's treatment of quoted expressions so that
quoted lists typed at the driver loop will produce true lazy lists.
@end quotation

@quotation
@strong{@anchor{Exercise 4-34}Exercise 4.34:} Modify the driver loop for the
evaluator so that lazy pairs and lists will print in some reasonable way.
(What are you going to do about infinite lists?)  You may also need to modify
the representation of lazy pairs so that the evaluator can identify them in
order to print them.
@end quotation

@node	4-3, 4-4, 4-2, Chapter 4
@section Variations on a Scheme -- Nondeterministic Computing

In this section, we extend the Scheme evaluator to support a programming
paradigm called @newterm{nondeterministic computing} by building into the
evaluator a facility to support automatic search.  This is a much more profound
change to the language than the introduction of lazy evaluation in section
@ref{4-2}.

Nondeterministic computing, like stream processing, is useful for ``generate
and test'' applications.  Consider the task of starting with two lists of
positive integers and finding a pair of integers---one from the first list and
one from the second list---whose sum is prime.  We saw how to handle this with
finite sequence operations in section @ref{2-2-3} and with infinite streams in
section @ref{3-5-3}.  Our approach was to generate the sequence of all possible
pairs and filter these to select the pairs whose sum is prime.  Whether we
actually generate the entire sequence of pairs first as in @ref{Chapter 2}, or
interleave the generating and filtering as in @ref{Chapter 3}, is immaterial to
the essential image of how the computation is organized.

The nondeterministic approach evokes a different image.  Imagine simply that we
choose (in some way) a number from the first list and a number from the second
list and require (using some mechanism) that their sum be prime.  This is
expressed by following procedure:

@lisp
(define (prime-sum-pair list1 list2)
  (let ((a (an-element-of list1))
        (b (an-element-of list2)))
    (require (prime? (+ a b)))
    (list a b)))
@end lisp

It might seem as if this procedure merely restates the problem, rather than
specifying a way to solve it.  Nevertheless, this is a legitimate
nondeterministic program.@footnote{We assume that we have previously defined a
procedure @code{prime?} that tests whether numbers are prime.  Even with
@code{prime?} defined, the @code{prime-sum-pair} procedure may look
suspiciously like the unhelpful ``pseudo-Lisp'' attempt to define the
square-root function, which we described at the beginning of section
@ref{1-1-7}.  In fact, a square-root procedure along those lines can actually
be formulated as a nondeterministic program.  By incorporating a search
mechanism into the evaluator, we are eroding the distinction between purely
declarative descriptions and imperative specifications of how to compute
answers.  We'll go even farther in this direction in section @ref{4-4}.}

The key idea here is that expressions in a nondeterministic language can have
more than one possible value.  For instance, @code{an-element-of} might return
any element of the given list.  Our nondeterministic program evaluator will
work by automatically choosing a possible value and keeping track of the
choice.  If a subsequent requirement is not met, the evaluator will try a
different choice, and it will keep trying new choices until the evaluation
succeeds, or until we run out of choices.  Just as the lazy evaluator freed the
programmer from the details of how values are delayed and forced, the
nondeterministic program evaluator will free the programmer from the details of
how choices are made.

It is instructive to contrast the different images of time evoked by
nondeterministic evaluation and stream processing.  Stream processing uses lazy
evaluation to decouple the time when the stream of possible answers is
assembled from the time when the actual stream elements are produced.  The
evaluator supports the illusion that all the possible answers are laid out
before us in a timeless sequence.  With nondeterministic evaluation, an
expression represents the exploration of a set of possible worlds, each
determined by a set of choices.  Some of the possible worlds lead to dead ends,
while others have useful values.  The nondeterministic program evaluator
supports the illusion that time branches, and that our programs have different
possible execution histories.  When we reach a dead end, we can revisit a
previous choice point and proceed along a different branch.

The nondeterministic program evaluator implemented below is called the
@code{amb} evaluator because it is based on a new special form called
@code{amb}.  We can type the above definition of @code{prime-sum-pair} at the
@code{amb} evaluator driver loop (along with definitions of @code{prime?},
@code{an-element-of}, and @code{require}) and run the procedure as follows:

@lisp
@i{;;; Amb-Eval input:}
(prime-sum-pair '(1 3 5 8) '(20 35 110))
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{(3 20)}
@end lisp

The value returned was obtained after the evaluator repeatedly chose elements
from each of the lists, until a successful choice was made.

Section @ref{4-3-1} introduces @code{amb} and explains how it supports
nondeterminism through the evaluator's automatic search mechanism.  Section
@ref{4-3-2} presents examples of nondeterministic programs, and section
@ref{4-3-3} gives the details of how to implement the @code{amb} evaluator by
modifying the ordinary Scheme evaluator.

@menu
* 4-3-1::            Amb and Search
* 4-3-2::            Examples of Nondeterministic Programs
* 4-3-3::            Implementing the @code{Amb} Evaluator
@end menu

@node	4-3-1, 4-3-2, 4-3, 4-3
@subsection Amb and Search

To extend Scheme to support nondeterminism, we introduce a new special form
called @code{amb}.@footnote{The idea of @code{amb} for nondeterministic
programming was first described in 1961 by John McCarthy (see McCarthy 1967).}
The expression

@lisp
(amb <@var{@i{e}_1}> <@var{@i{e}_2}> @dots{} <@var{@i{e}_@i{n}}>)
@end lisp

@noindent
returns the value of one of the @i{n} expressions <@var{@i{e}_@i{i}}>
``ambiguously.''  For example, the expression

@lisp
(list (amb 1 2 3) (amb 'a 'b))
@end lisp

@noindent
can have six possible values:

@example
@code{(1 a)} @code{(1 b)} @code{(2 a)} @code{(2 b)} @code{(3 a)} @code{(3 b)}
@end example

@code{Amb} with a single choice produces an ordinary (single) value.

@code{Amb} with no choices---the expression @code{(amb)}---is an expression
with no acceptable values.  Operationally, we can think of @code{(amb)} as an
expression that when evaluated causes the computation to ``fail'': The
computation aborts and no value is produced.  Using this idea, we can express
the requirement that a particular predicate expression @code{p} must be true as
follows:

@lisp
(define (require p)
  (if (not p) (amb)))
@end lisp

With @code{amb} and @code{require}, we can implement the @code{an-element-of}
procedure used above:

@lisp
(define (an-element-of items)
  (require (not (null? items)))
  (amb (car items) (an-element-of (cdr items))))
@end lisp

@code{An-element-of} fails if the list is empty.  Otherwise it ambiguously
returns either the first element of the list or an element chosen from the rest
of the list.

We can also express infinite ranges of choices.  The following procedure
potentially returns any integer greater than or equal to some given @i{n}:

@lisp
(define (an-integer-starting-from n)
  (amb n (an-integer-starting-from (+ n 1))))
@end lisp

This is like the stream procedure @code{integers-starting-from} described in
section @ref{3-5-2}, but with an important difference: The stream procedure
returns an object that represents the sequence of all integers beginning with
@i{n}, whereas the @code{amb} procedure returns a single integer.@footnote{In
actuality, the distinction between nondeterministically returning a single
choice and returning all choices depends somewhat on our point of view.  From
the perspective of the code that uses the value, the nondeterministic choice
returns a single value.  From the perspective of the programmer designing the
code, the nondeterministic choice potentially returns all possible values, and
the computation branches so that each value is investigated separately.}

Abstractly, we can imagine that evaluating an @code{amb} expression causes time
to split into branches, where the computation continues on each branch with one
of the possible values of the expression.  We say that @code{amb} represents a
@newterm{nondeterministic choice point}.  If we had a machine with a sufficient
number of processors that could be dynamically allocated, we could implement
the search in a straightforward way.  Execution would proceed as in a
sequential machine, until an @code{amb} expression is encountered.  At this
point, more processors would be allocated and initialized to continue all of
the parallel executions implied by the choice.  Each processor would proceed
sequentially as if it were the only choice, until it either terminates by
encountering a failure, or it further subdivides, or it finishes.@footnote{One
might object that this is a hopelessly inefficient mechanism.  It might require
millions of processors to solve some easily stated problem this way, and most
of the time most of those processors would be idle.  This objection should be
taken in the context of history.  Memory used to be considered just such an
expensive commodity.  In 1964 a megabyte of RAM cost about $400,000.  Now every
personal computer has many megabytes of RAM, and most of the time most of that
RAM is unused.  It is hard to underestimate the cost of mass-produced
electronics.}

On the other hand, if we have a machine that can execute only one process (or a
few concurrent processes), we must consider the alternatives sequentially.  One
could imagine modifying an evaluator to pick at random a branch to follow
whenever it encounters a choice point.  Random choice, however, can easily lead
to failing values.  We might try running the evaluator over and over, making
random choices and hoping to find a non-failing value, but it is better to
@newterm{systematically search} all possible execution paths.  The @code{amb}
evaluator that we will develop and work with in this section implements a
systematic search as follows: When the evaluator encounters an application of
@code{amb}, it initially selects the first alternative.  This selection may
itself lead to a further choice.  The evaluator will always initially choose
the first alternative at each choice point.  If a choice results in a failure,
then the evaluator automagically@footnote{Automagically: ``Automatically, but
in a way which, for some reason (typically because it is too complicated, or
too ugly, or perhaps even too trivial), the speaker doesn't feel like
explaining.''  (Steele 1983, Raymond 1993)} @newterm{backtracks} to the most
recent choice point and tries the next alternative.  If it runs out of
alternatives at any choice point, the evaluator will back up to the previous
choice point and resume from there.  This process leads to a search strategy
known as @newterm{depth-first search} or @newterm{chronological
backtracking}.@footnote{[Footnote 4.47]@anchor{Footnote 4-47} The integration
of automatic search strategies into programming languages has had a long and
checkered history.  The first suggestions that nondeterministic algorithms
might be elegantly encoded in a programming language with search and automatic
backtracking came from Robert Floyd (1967).  Carl Hewitt (1969) invented a
programming language called Planner that explicitly supported automatic
chronological backtracking, providing for a built-in depth-first search
strategy.  Sussman, Winograd, and Charniak (1971) implemented a subset of this
language, called MicroPlanner, which was used to support work in problem
solving and robot planning.  Similar ideas, arising from logic and theorem
proving, led to the genesis in Edinburgh and Marseille of the elegant language
Prolog (which we will discuss in section @ref{4-4}).  After sufficient
frustration with automatic search, McDermott and Sussman (1972) developed a
language called Conniver, which included mechanisms for placing the search
strategy under programmer control.  This proved unwieldy, however, and Sussman
and Stallman (1975) found a more tractable approach while investigating methods
of symbolic analysis for electrical circuits.  They developed a
non-chronological backtracking scheme that was based on tracing out the logical
dependencies connecting facts, a technique that has come to be known as
@newterm{dependency-directed backtracking}.  Although their method was complex,
it produced reasonably efficient programs because it did little redundant
search.  Doyle (1979) and McAllester (1978, 1980) generalized and clarified the
methods of Stallman and Sussman, developing a new paradigm for formulating
search that is now called @newterm{truth maintenance}.  Modern problem-solving
systems all use some form of truth-maintenance system as a substrate.  See
Forbus and deKleer 1993 for a discussion of elegant ways to build
truth-maintenance systems and applications using truth maintenance.  Zabih,
McAllester, and Chapman 1987 describes a nondeterministic extension to Scheme
that is based on @code{amb}; it is similar to the interpreter described in this
section, but more sophisticated, because it uses dependency-directed
backtracking rather than chronological backtracking.  Winston 1992 gives an
introduction to both kinds of backtracking.}

@subsubheading Driver loop

The driver loop for the @code{amb} evaluator has some unusual properties.  It
reads an expression and prints the value of the first non-failing execution, as
in the @code{prime-sum-pair} example shown above.  If we want to see the value
of the next successful execution, we can ask the interpreter to backtrack and
attempt to generate a second non-failing execution.  This is signaled by typing
the symbol @code{try-again}.  If any expression except @code{try-again} is
given, the interpreter will start a new problem, discarding the unexplored
alternatives in the previous problem.  Here is a sample interaction:

@lisp
@i{;;; Amb-Eval input:}
(prime-sum-pair '(1 3 5 8) '(20 35 110))
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{(3 20)}

@i{;;; Amb-Eval input:}
try-again
@i{;;; Amb-Eval value:}
@i{(3 110)}

@i{;;; Amb-Eval input:}
try-again
@i{;;; Amb-Eval value:}
@i{(8 35)}

@i{;;; Amb-Eval input:}
try-again
@i{;;; There are no more values of}
@i{(prime-sum-pair (quote (1 3 5 8)) (quote (20 35 110)))}

@i{;;; Amb-Eval input:}
(prime-sum-pair '(19 27 30) '(11 36 58))
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{(30 11)}
@end lisp

@quotation
@strong{@anchor{Exercise 4-35}Exercise 4.35:} Write a procedure
@code{an-integer-between} that returns an integer between two given bounds.
This can be used to implement a procedure that finds Pythagorean triples, i.e.,
triples of integers (@i{i},@i{j},@i{k}) between the given bounds such that
@i{i} <= @i{j} and @i{i}^2 + @i{j}^2 = @i{k}^2, as follows:

@lisp
(define (a-pythagorean-triple-between low high)
  (let ((i (an-integer-between low high)))
    (let ((j (an-integer-between i high)))
      (let ((k (an-integer-between j high)))
        (require (= (+ (* i i) (* j j)) (* k k)))
        (list i j k)))))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 4-36}Exercise 4.36:} @ref{Exercise 3-69} discussed how
to generate the stream of @emph{all} Pythagorean triples, with no upper bound
on the size of the integers to be searched.  Explain why simply replacing
@code{an-integer-between} by @code{an-integer-starting-from} in the procedure
in @ref{Exercise 4-35} is not an adequate way to generate arbitrary Pythagorean
triples.  Write a procedure that actually will accomplish this.  (That is,
write a procedure for which repeatedly typing @code{try-again} would in
principle eventually generate all Pythagorean triples.)
@end quotation

@quotation
@strong{@anchor{Exercise 4-37}Exercise 4.37:} Ben Bitdiddle claims that the
following method for generating Pythagorean triples is more efficient than the
one in @ref{Exercise 4-35}.  Is he correct?  (Hint: Consider the number of
possibilities that must be explored.)

@lisp
(define (a-pythagorean-triple-between low high)
  (let ((i (an-integer-between low high))
        (hsq (* high high)))
    (let ((j (an-integer-between i high)))
      (let ((ksq (+ (* i i) (* j j))))
        (require (>= hsq ksq))
        (let ((k (sqrt ksq)))
          (require (integer? k))
          (list i j k))))))
@end lisp
@end quotation

@node	4-3-2, 4-3-3, 4-3-1, 4-3
@subsection Examples of Nondeterministic Programs

Section @ref{4-3-3} describes the implementation of the @code{amb} evaluator.
First, however, we give some examples of how it can be used.  The advantage of
nondeterministic programming is that we can suppress the details of how search
is carried out, thereby expressing our programs at a higher level of
abstraction.

@subsubheading Logic Puzzles

The following puzzle (taken from Dinesman 1968) is typical of a large class of
simple logic puzzles:

@quotation
Baker, Cooper, Fletcher, Miller, and Smith live on different floors of an
apartment house that contains only five floors.  Baker does not live on the top
floor.  Cooper does not live on the bottom floor.  Fletcher does not live on
either the top or the bottom floor.  Miller lives on a higher floor than does
Cooper.  Smith does not live on a floor adjacent to Fletcher's.  Fletcher does
not live on a floor adjacent to Cooper's.  Where does everyone live?
@end quotation

We can determine who lives on each floor in a straightforward way by
enumerating all the possibilities and imposing the given
restrictions:@footnote{Our program uses the following procedure to determine if
the elements of a list are distinct:

@lisp
(define (distinct? items)
  (cond ((null? items) true)
        ((null? (cdr items)) true)
        ((member (car items) (cdr items)) false)
        (else (distinct? (cdr items)))))
@end lisp

@code{Member} is like @code{memq} except that it uses @code{equal?} instead
of @code{eq?} to test for equality.}

@lisp
(define (multiple-dwelling)
  (let ((baker (amb 1 2 3 4 5))
        (cooper (amb 1 2 3 4 5))
        (fletcher (amb 1 2 3 4 5))
        (miller (amb 1 2 3 4 5))
        (smith (amb 1 2 3 4 5)))
    (require
     (distinct? (list baker cooper fletcher miller smith)))
    (require (not (= baker 5)))
    (require (not (= cooper 1)))
    (require (not (= fletcher 5)))
    (require (not (= fletcher 1)))
    (require (> miller cooper))
    (require (not (= (abs (- smith fletcher)) 1)))
    (require (not (= (abs (- fletcher cooper)) 1)))
    (list (list 'baker baker)
          (list 'cooper cooper)
          (list 'fletcher fletcher)
          (list 'miller miller)
          (list 'smith smith))))
@end lisp

Evaluating the expression @code{(multiple-dwelling)} produces the result

@lisp
((baker 3) (cooper 2) (fletcher 4) (miller 5) (smith 1))
@end lisp

Although this simple procedure works, it is very slow.  @ref{Exercise 4-39} and
@ref{Exercise 4-40} discuss some possible improvements.

@quotation
@strong{@anchor{Exercise 4-38}Exercise 4.38:} Modify the multiple-dwelling
procedure to omit the requirement that Smith and Fletcher do not live on
adjacent floors.  How many solutions are there to this modified puzzle?
@end quotation

@quotation
@strong{@anchor{Exercise 4-39}Exercise 4.39:} Does the order of the
restrictions in the multiple-dwelling procedure affect the answer? Does it
affect the time to find an answer?  If you think it matters, demonstrate a
faster program obtained from the given one by reordering the restrictions.  If
you think it does not matter, argue your case.
@end quotation

@quotation
@strong{@anchor{Exercise 4-40}Exercise 4.40:} In the multiple dwelling problem,
how many sets of assignments are there of people to floors, both before and
after the requirement that floor assignments be distinct?  It is very
inefficient to generate all possible assignments of people to floors and then
leave it to backtracking to eliminate them.  For example, most of the
restrictions depend on only one or two of the person-floor variables, and can
thus be imposed before floors have been selected for all the people.  Write and
demonstrate a much more efficient nondeterministic procedure that solves this
problem based upon generating only those possibilities that are not already
ruled out by previous restrictions.  (Hint: This will require a nest of
@code{let} expressions.)
@end quotation

@quotation
@strong{@anchor{Exercise 4-41}Exercise 4.41:} Write an ordinary Scheme program
to solve the multiple dwelling puzzle.
@end quotation

@quotation
@strong{@anchor{Exercise 4-42}Exercise 4.42:} Solve the following ``Liars''
puzzle (from Phillips 1934):

Five schoolgirls sat for an examination.  Their parents---so they
thought---showed an undue degree of interest in the result.  They therefore
agreed that, in writing home about the examination, each girl should make one
true statement and one untrue one.  The following are the relevant passages
from their letters:

@itemize @bullet

@item
Betty: ``Kitty was second in the examination.  I was only third.''

@item
Ethel: ``You'll be glad to hear that I was on top.  Joan was second.''

@item
Joan: ``I was third, and poor old Ethel was bottom.''

@item
Kitty: ``I came out second.  Mary was only fourth.''

@item
Mary: ``I was fourth.  Top place was taken by Betty.''

@end itemize

What in fact was the order in which the five girls were placed?
@end quotation

@quotation
@strong{@anchor{Exercise 4-43}Exercise 4.43:} Use the @code{amb} evaluator to
solve the following puzzle:@footnote{This is taken from a booklet called
``Problematical Recreations,'' published in the 1960s by Litton Industries,
where it is attributed to the @cite{Kansas State Engineer}.}

@quotation
Mary Ann Moore's father has a yacht and so has each of his four friends:
Colonel Downing, Mr. Hall, Sir Barnacle Hood, and Dr.  Parker.  Each of the
five also has one daughter and each has named his yacht after a daughter of one
of the others.  Sir Barnacle's yacht is the Gabrielle, Mr. Moore owns the
Lorna; Mr. Hall the Rosalind.  The Melissa, owned by Colonel Downing, is named
after Sir Barnacle's daughter.  Gabrielle's father owns the yacht that is named
after Dr.  Parker's daughter.  Who is Lorna's father?
@end quotation

Try to write the program so that it runs efficiently (see @ref{Exercise 4-40}).
Also determine how many solutions there are if we are not told that Mary Ann's
last name is Moore.
@end quotation

@quotation
@strong{@anchor{Exercise 4-44}Exercise 4.44:} @ref{Exercise 2-42} described the
``eight-queens puzzle'' of placing queens on a chessboard so that no two attack
each other.  Write a nondeterministic program to solve this puzzle.
@end quotation

@subsubheading Parsing natural language

Programs designed to accept natural language as input usually start by
attempting to @newterm{parse} the input, that is, to match the input against
some grammatical structure.  For example, we might try to recognize simple
sentences consisting of an article followed by a noun followed by a verb, such
as ``The cat eats.''  To accomplish such an analysis, we must be able to
identify the parts of speech of individual words.  We could start with some
lists that classify various words:@footnote{Here we use the convention that the
first element of each list designates the part of speech for the rest of the
words in the list.}

@lisp
(define nouns '(noun student professor cat class))

(define verbs '(verb studies lectures eats sleeps))

(define articles '(article the a))
@end lisp

We also need a @newterm{grammar}, that is, a set of rules describing how
grammatical elements are composed from simpler elements.  A very simple grammar
might stipulate that a sentence always consists of two pieces---a noun phrase
followed by a verb---and that a noun phrase consists of an article followed by
a noun.  With this grammar, the sentence ``The cat eats'' is parsed as follows:

@lisp
(sentence (noun-phrase (article the) (noun cat))
          (verb eats))
@end lisp

We can generate such a parse with a simple program that has separate procedures
for each of the grammatical rules.  To parse a sentence, we identify its two
constituent pieces and return a list of these two elements, tagged with the
symbol @code{sentence}:

@lisp
(define (parse-sentence)
  (list 'sentence
         (parse-noun-phrase)
         (parse-word verbs)))
@end lisp

A noun phrase, similarly, is parsed by finding an article followed by a
noun:

@lisp
(define (parse-noun-phrase)
  (list 'noun-phrase
        (parse-word articles)
        (parse-word nouns)))
@end lisp

At the lowest level, parsing boils down to repeatedly checking that the next
unparsed word is a member of the list of words for the required part of speech.
To implement this, we maintain a global variable @code{*unparsed*}, which is
the input that has not yet been parsed.  Each time we check a word, we require
that @code{*unparsed*} must be non-empty and that it should begin with a word
from the designated list.  If so, we remove that word from @code{*unparsed*}
and return the word together with its part of speech (which is found at the
head of the list):@footnote{Notice that @code{parse-word} uses @code{set!} to
modify the unparsed input list.  For this to work, our @code{amb} evaluator
must undo the effects of @code{set!} operations when it backtracks.}

@lisp
(define (parse-word word-list)
  (require (not (null? *unparsed*)))
  (require (memq (car *unparsed*) (cdr word-list)))
  (let ((found-word (car *unparsed*)))
    (set! *unparsed* (cdr *unparsed*))
    (list (car word-list) found-word)))
@end lisp

To start the parsing, all we need to do is set @code{*unparsed*} to be the
entire input, try to parse a sentence, and check that nothing is left over:

@lisp
(define *unparsed* '())

(define (parse input)
  (set! *unparsed* input)
  (let ((sent (parse-sentence)))
    (require (null? *unparsed*))
    sent))
@end lisp

We can now try the parser and verify that it works for our simple test
sentence:

@lisp
@i{;;; Amb-Eval input:}
(parse '(the cat eats))
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{(sentence (noun-phrase (article the) (noun cat)) (verb eats))}
@end lisp

The @code{amb} evaluator is useful here because it is convenient to express the
parsing constraints with the aid of @code{require}.  Automatic search and
backtracking really pay off, however, when we consider more complex grammars
where there are choices for how the units can be decomposed.

Let's add to our grammar a list of prepositions:

@lisp
(define prepositions '(prep for to in by with))
@end lisp

@noindent
and define a prepositional phrase (e.g., ``for the cat'') to be a preposition
followed by a noun phrase:

@lisp
(define (parse-prepositional-phrase)
  (list 'prep-phrase
        (parse-word prepositions)
        (parse-noun-phrase)))
@end lisp

Now we can define a sentence to be a noun phrase followed by a verb phrase,
where a verb phrase can be either a verb or a verb phrase extended by a
prepositional phrase:@footnote{Observe that this definition is recursive---a
verb may be followed by any number of prepositional phrases.}

@lisp
(define (parse-sentence)
  (list 'sentence
         (parse-noun-phrase)
         (parse-verb-phrase)))

(define (parse-verb-phrase)
  (define (maybe-extend verb-phrase)
    (amb verb-phrase
         (maybe-extend (list 'verb-phrase
                             verb-phrase
                             (parse-prepositional-phrase)))))
  (maybe-extend (parse-word verbs)))
@end lisp

While we're at it, we can also elaborate the definition of noun phrases to
permit such things as ``a cat in the class.''  What we used to call a noun
phrase, we'll now call a simple noun phrase, and a noun phrase will now be
either a simple noun phrase or a noun phrase extended by a prepositional
phrase:

@lisp
(define (parse-simple-noun-phrase)
  (list 'simple-noun-phrase
        (parse-word articles)
        (parse-word nouns)))

(define (parse-noun-phrase)
  (define (maybe-extend noun-phrase)
    (amb noun-phrase
         (maybe-extend (list 'noun-phrase
                             noun-phrase
                             (parse-prepositional-phrase)))))
  (maybe-extend (parse-simple-noun-phrase)))
@end lisp

Our new grammar lets us parse more complex sentences.  For example

@lisp
(parse '(the student with the cat sleeps in the class))
@end lisp

@noindent
produces

@lisp
(sentence
 (noun-phrase
  (simple-noun-phrase (article the) (noun student))
  (prep-phrase (prep with)
               (simple-noun-phrase
                (article the) (noun cat))))
 (verb-phrase
  (verb sleeps)
  (prep-phrase (prep in)
               (simple-noun-phrase
                (article the) (noun class)))))
@end lisp

Observe that a given input may have more than one legal parse.  In the sentence
``The professor lectures to the student with the cat,'' it may be that the
professor is lecturing with the cat, or that the student has the cat.  Our
nondeterministic program finds both possibilities:

@lisp
(parse '(the professor lectures to the student with the cat))
@end lisp

@noindent
produces

@lisp
(sentence
 (simple-noun-phrase (article the) (noun professor))
 (verb-phrase
  (verb-phrase
   (verb lectures)
   (prep-phrase (prep to)
                (simple-noun-phrase
                 (article the) (noun student))))
  (prep-phrase (prep with)
               (simple-noun-phrase
                (article the) (noun cat)))))
@end lisp

Asking the evaluator to try again yields

@lisp
(sentence
 (simple-noun-phrase (article the) (noun professor))
 (verb-phrase
  (verb lectures)
  (prep-phrase (prep to)
               (noun-phrase
                (simple-noun-phrase
                 (article the) (noun student))
                (prep-phrase (prep with)
                             (simple-noun-phrase
                              (article the) (noun cat)))))))
@end lisp

@quotation
@strong{@anchor{Exercise 4-45}Exercise 4.45:} With the grammar given above, the
following sentence can be parsed in five different ways: ``The professor
lectures to the student in the class with the cat.''  Give the five parses and
explain the differences in shades of meaning among them.
@end quotation

@quotation
@strong{@anchor{Exercise 4-46}Exercise 4.46:} The evaluators in sections
@ref{4-1} and @ref{4-2} do not determine what order operands are evaluated in.
We will see that the @code{amb} evaluator evaluates them from left to right.
Explain why our parsing program wouldn't work if the operands were evaluated in
some other order.
@end quotation

@quotation
@strong{@anchor{Exercise 4-47}Exercise 4.47:} Louis Reasoner suggests that,
since a verb phrase is either a verb or a verb phrase followed by a
prepositional phrase, it would be much more straightforward to define the
procedure @code{parse-verb-phrase} as follows (and similarly for noun phrases):

@lisp
(define (parse-verb-phrase)
  (amb (parse-word verbs)
       (list 'verb-phrase
             (parse-verb-phrase)
             (parse-prepositional-phrase))))
@end lisp

Does this work?  Does the program's behavior change if we interchange the order
of expressions in the @code{amb}
@end quotation

@quotation
@strong{@anchor{Exercise 4-48}Exercise 4.48:} Extend the grammar given above to
handle more complex sentences.  For example, you could extend noun phrases and
verb phrases to include adjectives and adverbs, or you could handle compound
sentences.@footnote{This kind of grammar can become arbitrarily complex, but it
is only a toy as far as real language understanding is concerned.  Real
natural-language understanding by computer requires an elaborate mixture of
syntactic analysis and interpretation of meaning.  On the other hand, even toy
parsers can be useful in supporting flexible command languages for programs
such as information-retrieval systems.  Winston 1992 discusses computational
approaches to real language understanding and also the applications of simple
grammars to command languages.}
@end quotation

@quotation
@strong{@anchor{Exercise 4-49}Exercise 4.49:} Alyssa P. Hacker is more
interested in generating interesting sentences than in parsing them.  She
reasons that by simply changing the procedure @code{parse-word} so that it
ignores the ``input sentence'' and instead always succeeds and generates an
appropriate word, we can use the programs we had built for parsing to do
generation instead.  Implement Alyssa's idea, and show the first half-dozen or
so sentences generated.@footnote{Although Alyssa's idea works just fine (and is
surprisingly simple), the sentences that it generates are a bit boring---they
don't sample the possible sentences of this language in a very interesting way.
In fact, the grammar is highly recursive in many places, and Alyssa's technique
``falls into'' one of these recursions and gets stuck.  See @ref{Exercise 4-50}
for a way to deal with this.}
@end quotation

@node	4-3-3,  , 4-3-2, 4-3
@subsection Implementing the @code{Amb} Evaluator

The evaluation of an ordinary Scheme expression may return a value, may never
terminate, or may signal an error.  In nondeterministic Scheme the evaluation
of an expression may in addition result in the discovery of a dead end, in
which case evaluation must backtrack to a previous choice point.  The
interpretation of nondeterministic Scheme is complicated by this extra case.

We will construct the @code{amb} evaluator for nondeterministic Scheme by
modifying the analyzing evaluator of section @ref{4-1-7}.@footnote{We chose to
implement the lazy evaluator in section @ref{4-2} as a modification of the
ordinary metacircular evaluator of section @ref{4-1-1}.  In contrast, we will
base the @code{amb} evaluator on the analyzing evaluator of section
@ref{4-1-7}, because the execution procedures in that evaluator provide a
convenient framework for implementing backtracking.}  As in the analyzing
evaluator, evaluation of an expression is accomplished by calling an execution
procedure produced by analysis of that expression.  The difference between the
interpretation of ordinary Scheme and the interpretation of nondeterministic
Scheme will be entirely in the execution procedures.

@subsubheading Execution procedures and continuations

Recall that the execution procedures for the ordinary evaluator take one
argument: the environment of execution.  In contrast, the execution procedures
in the @code{amb} evaluator take three arguments: the environment, and two
procedures called @newterm{continuation procedures}.  The evaluation of an
expression will finish by calling one of these two continuations: If the
evaluation results in a value, the @newterm{success continuation} is called
with that value; if the evaluation results in the discovery of a dead end, the
@newterm{failure continuation} is called.  Constructing and calling appropriate
continuations is the mechanism by which the nondeterministic evaluator
implements backtracking.

It is the job of the success continuation to receive a value and proceed with
the computation.  Along with that value, the success continuation is passed
another failure continuation, which is to be called subsequently if the use of
that value leads to a dead end.

It is the job of the failure continuation to try another branch of the
nondeterministic process.  The essence of the nondeterministic language is in
the fact that expressions may represent choices among alternatives.  The
evaluation of such an expression must proceed with one of the indicated
alternative choices, even though it is not known in advance which choices will
lead to acceptable results.  To deal with this, the evaluator picks one of the
alternatives and passes this value to the success continuation.  Together with
this value, the evaluator constructs and passes along a failure continuation
that can be called later to choose a different alternative.

A failure is triggered during evaluation (that is, a failure continuation is
called) when a user program explicitly rejects the current line of attack (for
example, a call to @code{require} may result in execution of @code{(amb)}, an
expression that always fails---see section @ref{4-3-1}).  The failure
continuation in hand at that point will cause the most recent choice point to
choose another alternative.  If there are no more alternatives to be considered
at that choice point, a failure at an earlier choice point is triggered, and so
on.  Failure continuations are also invoked by the driver loop in response to a
@code{try-again} request, to find another value of the expression.

In addition, if a side-effect operation (such as assignment to a variable)
occurs on a branch of the process resulting from a choice, it may be necessary,
when the process finds a dead end, to undo the side effect before making a new
choice.  This is accomplished by having the side-effect operation produce a
failure continuation that undoes the side effect and propagates the failure.

In summary, failure continuations are constructed by

@itemize @bullet

@item
@code{amb} expressions---to provide a mechanism to make alternative choices if
the current choice made by the @code{amb} expression leads to a dead end;

@item
the top-level driver---to provide a mechanism to report failure when the
choices are exhausted;

@item
assignments---to intercept failures and undo assignments during backtracking.

@end itemize

Failures are initiated only when a dead end is encountered.  This occurs

@itemize @bullet

@item
if the user program executes @code{(amb)};

@item
if the user types @code{try-again} at the top-level driver.

@end itemize

Failure continuations are also called during processing of a failure:

@itemize @bullet

@item
When the failure continuation created by an assignment finishes undoing a side
effect, it calls the failure continuation it intercepted, in order to propagate
the failure back to the choice point that led to this assignment or to the top
level.

@item
When the failure continuation for an @code{amb} runs out of choices, it calls
the failure continuation that was originally given to the @code{amb}, in order
to propagate the failure back to the previous choice point or to the top level.

@end itemize

@subsubheading Structure of the evaluator

The syntax- and data-representation procedures for the @code{amb} evaluator,
and also the basic @code{analyze} procedure, are identical to those in the
evaluator of section @ref{4-1-7}, except for the fact that we need additional
syntax procedures to recognize the @code{amb} special form:@footnote{We assume
that the evaluator supports @code{let} (see @ref{Exercise 4-22}), which we have
used in our nondeterministic programs.}

@lisp
(define (amb? exp) (tagged-list? exp 'amb))

(define (amb-choices exp) (cdr exp))
@end lisp

We must also add to the dispatch in @code{analyze} a clause that will recognize
this special form and generate an appropriate execution procedure:

@lisp
((amb? exp) (analyze-amb exp))
@end lisp

The top-level procedure @code{ambeval} (similar to the version of @code{eval}
given in section @ref{4-1-7}) analyzes the given expression and applies the
resulting execution procedure to the given environment, together with two given
continuations:

@lisp
(define (ambeval exp env succeed fail)
  ((analyze exp) env succeed fail))
@end lisp

A success continuation is a procedure of two arguments: the value just obtained
and another failure continuation to be used if that value leads to a subsequent
failure. A failure continuation is a procedure of no arguments.  So the general
form of an execution procedure is

@lisp
(lambda (env succeed fail)
  @r{;; @code{succeed} is @code{(lambda (value fail) @dots{})}}
  @r{;; @code{fail} is @code{(lambda () @dots{})}}
  @dots{})
@end lisp

For example, executing

@lisp
(ambeval <@var{exp}>
         the-global-environment
         (lambda (value fail) value)
         (lambda () 'failed))
@end lisp

@noindent
will attempt to evaluate the given expression and will return either the
expression's value (if the evaluation succeeds) or the symbol @code{failed} (if
the evaluation fails).  The call to @code{ambeval} in the driver loop shown
below uses much more complicated continuation procedures, which continue the
loop and support the @code{try-again} request.

Most of the complexity of the @code{amb} evaluator results from the mechanics
of passing the continuations around as the execution procedures call each
other.  In going through the following code, you should compare each of the
execution procedures with the corresponding procedure for the ordinary
evaluator given in section @ref{4-1-7}.

@subsubheading Simple expressions

The execution procedures for the simplest kinds of expressions are essentially
the same as those for the ordinary evaluator, except for the need to manage the
continuations.  The execution procedures simply succeed with the value of the
expression, passing along the failure continuation that was passed to them.

@lisp
(define (analyze-self-evaluating exp)
  (lambda (env succeed fail)
    (succeed exp fail)))

(define (analyze-quoted exp)
  (let ((qval (text-of-quotation exp)))
    (lambda (env succeed fail)
      (succeed qval fail))))

(define (analyze-variable exp)
  (lambda (env succeed fail)
    (succeed (lookup-variable-value exp env)
             fail)))

(define (analyze-lambda exp)
  (let ((vars (lambda-parameters exp))
        (bproc (analyze-sequence (lambda-body exp))))
    (lambda (env succeed fail)
      (succeed (make-procedure vars bproc env)
               fail))))
@end lisp

Notice that looking up a variable always ``succeeds.''  If
@code{lookup-variable-value} fails to find the variable, it signals an error,
as usual.  Such a ``failure'' indicates a program bug---a reference to an
unbound variable; it is not an indication that we should try another
nondeterministic choice instead of the one that is currently being tried.

@subsubheading Conditionals and sequences

Conditionals are also handled in a similar way as in the ordinary evaluator.
The execution procedure generated by @code{analyze-if} invokes the predicate
execution procedure @code{pproc} with a success continuation that checks
whether the predicate value is true and goes on to execute either the
consequent or the alternative.  If the execution of @code{pproc} fails, the
original failure continuation for the @code{if} expression is called.

@lisp
(define (analyze-if exp)
  (let ((pproc (analyze (if-predicate exp)))
        (cproc (analyze (if-consequent exp)))
        (aproc (analyze (if-alternative exp))))
    (lambda (env succeed fail)
      (pproc env
             @r{;; success continuation for evaluating the predicate}
             @r{;; to obtain @code{pred-value}}
             (lambda (pred-value fail2)
               (if (true? pred-value)
                   (cproc env succeed fail2)
                   (aproc env succeed fail2)))
             @r{;; failure continuation for evaluating the predicate}
             fail))))
@end lisp

Sequences are also handled in the same way as in the previous evaluator, except
for the machinations in the subprocedure @code{sequentially} that are required
for passing the continuations.  Namely, to sequentially execute @code{a} and
then @code{b}, we call @code{a} with a success continuation that calls
@code{b}.

@lisp
(define (analyze-sequence exps)
  (define (sequentially a b)
    (lambda (env succeed fail)
      (a env
         @r{;; success continuation for calling @code{a}}
         (lambda (a-value fail2)
           (b env succeed fail2))
         @r{;; failure continuation for calling @code{a}}
         fail)))
  (define (loop first-proc rest-procs)
    (if (null? rest-procs)
        first-proc
        (loop (sequentially first-proc (car rest-procs))
              (cdr rest-procs))))
  (let ((procs (map analyze exps)))
    (if (null? procs)
        (error "Empty sequence -- ANALYZE"))
    (loop (car procs) (cdr procs))))
@end lisp

@subsubheading Definitions and assignments

Definitions are another case where we must go to some trouble to manage the
continuations, because it is necessary to evaluate the definition-value
expression before actually defining the new variable.  To accomplish this, the
definition-value execution procedure @code{vproc} is called with the
environment, a success continuation, and the failure continuation.  If the
execution of @code{vproc} succeeds, obtaining a value @code{val} for the
defined variable, the variable is defined and the success is propagated:

@lisp
(define (analyze-definition exp)
  (let ((var (definition-variable exp))
        (vproc (analyze (definition-value exp))))
    (lambda (env succeed fail)
      (vproc env
             (lambda (val fail2)
               (define-variable! var val env)
               (succeed 'ok fail2))
             fail))))
@end lisp

Assignments are more interesting.  This is the first place where we really use
the continuations, rather than just passing them around.  The execution
procedure for assignments starts out like the one for definitions.  It first
attempts to obtain the new value to be assigned to the variable. If this
evaluation of @code{vproc} fails, the assignment fails.

If @code{vproc} succeeds, however, and we go on to make the assignment, we must
consider the possibility that this branch of the computation might later fail,
which will require us to backtrack out of the assignment.  Thus, we must
arrange to undo the assignment as part of the backtracking process.@footnote{We
didn't worry about undoing definitions, since we can assume that internal
definitions are scanned out (section @ref{4-1-6}).}

This is accomplished by giving @code{vproc} a success continuation (marked with
the comment ``*1*'' below) that saves the old value of the variable before
assigning the new value to the variable and proceeding from the assignment.
The failure continuation that is passed along with the value of the assignment
(marked with the comment ``*2*'' below) restores the old value of the variable
before continuing the failure.  That is, a successful assignment provides a
failure continuation that will intercept a subsequent failure; whatever failure
would otherwise have called @code{fail2} calls this procedure instead, to undo
the assignment before actually calling @code{fail2}.

@lisp
(define (analyze-assignment exp)
  (let ((var (assignment-variable exp))
        (vproc (analyze (assignment-value exp))))
    (lambda (env succeed fail)
      (vproc env
             (lambda (val fail2)        @r{; *1*}
               (let ((old-value
                      (lookup-variable-value var env)))
                 (set-variable-value! var val env)
                 (succeed 'ok
                          (lambda ()    @r{; *2*}
                            (set-variable-value! var
                                                 old-value
                                                 env)
                            (fail2)))))
             fail))))
@end lisp

@subsubheading Procedure applications

The execution procedure for applications contains no new ideas except for the
technical complexity of managing the continuations.  This complexity arises in
@code{analyze-application}, due to the need to keep track of the success and
failure continuations as we evaluate the operands.  We use a procedure
@code{get-args} to evaluate the list of operands, rather than a simple
@code{map} as in the ordinary evaluator.

@lisp
(define (analyze-application exp)
  (let ((fproc (analyze (operator exp)))
        (aprocs (map analyze (operands exp))))
    (lambda (env succeed fail)
      (fproc env
             (lambda (proc fail2)
               (get-args aprocs
                         env
                         (lambda (args fail3)
                           (execute-application
                            proc args succeed fail3))
                         fail2))
             fail))))
@end lisp

In @code{get-args}, notice how @code{cdr}ing down the list of @code{aproc}
execution procedures and @code{cons}ing up the resulting list of @code{args} is
accomplished by calling each @code{aproc} in the list with a success
continuation that recursively calls @code{get-args}.  Each of these recursive
calls to @code{get-args} has a success continuation whose value is the
@code{cons} of the newly obtained argument onto the list of accumulated
arguments:

@lisp
(define (get-args aprocs env succeed fail)
  (if (null? aprocs)
      (succeed '() fail)
      ((car aprocs) env
                    @r{;; success continuation for this @code{aproc}}
                    (lambda (arg fail2)
                      (get-args (cdr aprocs)
                                env
                                @r{;; success continuation for recursive}
                                @r{;; call to @code{get-args}}
                                (lambda (args fail3)
                                  (succeed (cons arg args)
                                           fail3))
                                fail2))
                    fail)))
@end lisp

The actual procedure application, which is performed by
@code{execute-application}, is accomplished in the same way as for the ordinary
evaluator, except for the need to manage the continuations.

@lisp
(define (execute-application proc args succeed fail)
  (cond ((primitive-procedure? proc)
         (succeed (apply-primitive-procedure proc args)
                  fail))
        ((compound-procedure? proc)
         ((procedure-body proc)
          (extend-environment (procedure-parameters proc)
                              args
                              (procedure-environment proc))
          succeed
          fail))
        (else
         (error
          "Unknown procedure type -- EXECUTE-APPLICATION"
          proc))))
@end lisp

@subsubheading Evaluating @code{amb} expressions

The @code{amb} special form is the key element in the nondeterministic
language.  Here we see the essence of the interpretation process and the reason
for keeping track of the continuations.  The execution procedure for @code{amb}
defines a loop @code{try-next} that cycles through the execution procedures for
all the possible values of the @code{amb} expression.  Each execution procedure
is called with a failure continuation that will try the next one.  When there
are no more alternatives to try, the entire @code{amb} expression fails.

@lisp
(define (analyze-amb exp)
  (let ((cprocs (map analyze (amb-choices exp))))
    (lambda (env succeed fail)
      (define (try-next choices)
        (if (null? choices)
            (fail)
            ((car choices) env
                           succeed
                           (lambda ()
                             (try-next (cdr choices))))))
      (try-next cprocs))))
@end lisp

@subsubheading Driver loop

The driver loop for the @code{amb} evaluator is complex, due to the mechanism
that permits the user to try again in evaluating an expression.  The driver
uses a procedure called @code{internal-loop}, which takes as argument a
procedure @code{try-again}.  The intent is that calling @code{try-again} should
go on to the next untried alternative in the nondeterministic evaluation.
@code{Internal-loop} either calls @code{try-again} in response to the user
typing @code{try-again} at the driver loop, or else starts a new evaluation by
calling @code{ambeval}.

The failure continuation for this call to @code{ambeval} informs the user that
there are no more values and re-invokes the driver loop.

The success continuation for the call to @code{ambeval} is more subtle.  We
print the obtained value and then invoke the internal loop again with a
@code{try-again} procedure that will be able to try the next alternative.  This
@code{next-alternative} procedure is the second argument that was passed to the
success continuation.  Ordinarily, we think of this second argument as a
failure continuation to be used if the current evaluation branch later fails.
In this case, however, we have completed a successful evaluation, so we can
invoke the ``failure'' alternative branch in order to search for additional
successful evaluations.

@lisp
(define input-prompt ";;; Amb-Eval input:")
(define output-prompt ";;; Amb-Eval value:")

(define (driver-loop)
  (define (internal-loop try-again)
    (prompt-for-input input-prompt)
    (let ((input (read)))
      (if (eq? input 'try-again)
          (try-again)
          (begin
            (newline)
            (display ";;; Starting a new problem ")
            (ambeval input
                     the-global-environment
                     @r{;; @code{ambeval} success}
                     (lambda (val next-alternative)
                       (announce-output output-prompt)
                       (user-print val)
                       (internal-loop next-alternative))
                     @r{;; @code{ambeval} failure}
                     (lambda ()
                       (announce-output
                        ";;; There are no more values of")
                       (user-print input)
                       (driver-loop)))))))
  (internal-loop
   (lambda ()
     (newline)
     (display ";;; There is no current problem")
     (driver-loop))))
@end lisp

The initial call to @code{internal-loop} uses a @code{try-again} procedure that
complains that there is no current problem and restarts the driver loop.  This
is the behavior that will happen if the user types @code{try-again} when there
is no evaluation in progress.

@quotation
@strong{@anchor{Exercise 4-50}Exercise 4.50:} Implement a new special form
@code{ramb} that is like @code{amb} except that it searches alternatives in a
random order, rather than from left to right.  Show how this can help with
Alyssa's problem in @ref{Exercise 4-49}.
@end quotation

@quotation
@strong{@anchor{Exercise 4-51}Exercise 4.51:} Implement a new kind of
assignment called @code{permanent-set!} that is not undone upon failure.  For
example, we can choose two distinct elements from a list and count the number
of trials required to make a successful choice as follows:

@lisp
(define count 0)

(let ((x (an-element-of '(a b c)))
      (y (an-element-of '(a b c))))
  (permanent-set! count (+ count 1))
  (require (not (eq? x y)))
  (list x y count))
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{(a b 2)}

@i{;;; Amb-Eval input:}
try-again
@i{;;; Amb-Eval value:}
@i{(a c 3)}
@end lisp

What values would have been displayed if we had used @code{set!} here
rather than @code{permanent-set!} ?
@end quotation

@quotation
@strong{@anchor{Exercise 4-52}Exercise 4.52:} Implement a new construct called
@code{if-fail} that permits the user to catch the failure of an expression.
@code{If-fail} takes two expressions.  It evaluates the first expression as
usual and returns as usual if the evaluation succeeds.  If the evaluation
fails, however, the value of the second expression is returned, as in the
following example:

@lisp
@i{;;; Amb-Eval input:}
(if-fail (let ((x (an-element-of '(1 3 5))))
           (require (even? x))
           x)
         'all-odd)
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{all-odd}

@i{;;; Amb-Eval input:}
(if-fail (let ((x (an-element-of '(1 3 5 8))))
           (require (even? x))
           x)
         'all-odd)
@i{;;; Starting a new problem}
@i{;;; Amb-Eval value:}
@i{8}
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 4-53}Exercise 4.53:} With @code{permanent-set!} as
described in @ref{Exercise 4-51} and @code{if-fail} as in @ref{Exercise 4-52},
what will be the result of evaluating

@lisp
(let ((pairs '()))
  (if-fail (let ((p (prime-sum-pair '(1 3 5 8) '(20 35 110))))
             (permanent-set! pairs (cons p pairs))
             (amb))
           pairs))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 4-54}Exercise 4.54:} If we had not realized that
@code{require} could be implemented as an ordinary procedure that uses
@code{amb}, to be defined by the user as part of a nondeterministic program, we
would have had to implement it as a special form.  This would require syntax
procedures

@lisp
(define (require? exp) (tagged-list? exp 'require))

(define (require-predicate exp) (cadr exp))
@end lisp

@noindent
and a new clause in the dispatch in @code{analyze}

@lisp
((require? exp) (analyze-require exp))
@end lisp

@noindent
as well the procedure @code{analyze-require} that handles @code{require}
expressions.  Complete the following definition of @code{analyze-require}.

@lisp
(define (analyze-require exp)
  (let ((pproc (analyze (require-predicate exp))))
    (lambda (env succeed fail)
      (pproc env
             (lambda (pred-value fail2)
               (if <@var{??}>
                   <@var{??}>
                   (succeed 'ok fail2)))
             fail))))
@end lisp
@end quotation


@node	4-4,  , 4-3, Chapter 4
@section Logic Programming

In @ref{Chapter 1} we stressed that computer science deals with imperative (how
to) knowledge, whereas mathematics deals with declarative (what is) knowledge.
Indeed, programming languages require that the programmer express knowledge in
a form that indicates the step-by-step methods for solving particular problems.
On the other hand, high-level languages provide, as part of the language
implementation, a substantial amount of methodological knowledge that frees the
user from concern with numerous details of how a specified computation will
progress.

Most programming languages, including Lisp, are organized around computing the
values of mathematical functions.  Expression-oriented languages (such as Lisp,
Fortran, and Algol) capitalize on the ``pun'' that an expression that describes
the value of a function may also be interpreted as a means of computing that
value.  Because of this, most programming languages are strongly biased toward
unidirectional computations (computations with well-defined inputs and
outputs).  There are, however, radically different programming languages that
relax this bias.  We saw one such example in section @ref{3-3-5}, where the
objects of computation were arithmetic constraints.  In a constraint system the
direction and the order of computation are not so well specified; in carrying
out a computation the system must therefore provide more detailed ``how to''
knowledge than would be the case with an ordinary arithmetic computation.  This
does not mean, however, that the user is released altogether from the
responsibility of providing imperative knowledge.  There are many constraint
networks that implement the same set of constraints, and the user must choose
from the set of mathematically equivalent networks a suitable network to
specify a particular computation.

The nondeterministic program evaluator of section @ref{4-3} also moves away
from the view that programming is about constructing algorithms for computing
unidirectional functions.  In a nondeterministic language, expressions can have
more than one value, and, as a result, the computation is dealing with
relations rather than with single-valued functions.  Logic programming extends
this idea by combining a relational vision of programming with a powerful kind
of symbolic pattern matching called @newterm{unification}.@footnote{Logic
programming has grown out of a long history of research in automatic theorem
proving.  Early theorem-proving programs could accomplish very little, because
they exhaustively searched the space of possible proofs.  The major
breakthrough that made such a search plausible was the discovery in the early
1960s of the @newterm{unification algorithm} and the @newterm{resolution
principle} (Robinson 1965).  Resolution was used, for example, by Green and
Raphael (1968) (see also Green 1969) as the basis for a deductive
question-answering system.  During most of this period, researchers
concentrated on algorithms that are guaranteed to find a proof if one exists.
Such algorithms were difficult to control and to direct toward a proof.  Hewitt
(1969) recognized the possibility of merging the control structure of a
programming language with the operations of a logic-manipulation system,
leading to the work in automatic search mentioned in section @ref{4-3-1}
(footnote @ref{Footnote 4-47}).  At the same time that this was being done,
Colmerauer, in Marseille, was developing rule-based systems for manipulating
natural language (see Colmerauer et al. 1973).  He invented a programming
language called Prolog for representing those rules.  Kowalski (1973; 1979), in
Edinburgh, recognized that execution of a Prolog program could be interpreted
as proving theorems (using a proof technique called linear Horn-clause
resolution).  The merging of the last two strands led to the logic-programming
movement.  Thus, in assigning credit for the development of logic programming,
the French can point to Prolog's genesis at the University of Marseille, while
the British can highlight the work at the University of Edinburgh.  According
to people at @acronym{MIT}, logic programming was developed by these groups in
an attempt to figure out what Hewitt was talking about in his brilliant but
impenetrable Ph.D. thesis.  For a history of logic programming, see Robinson
1983.}

This approach, when it works, can be a very powerful way to write programs.
Part of the power comes from the fact that a single ``what is'' fact can be
used to solve a number of different problems that would have different ``how
to'' components.  As an example, consider the @code{append} operation, which
takes two lists as arguments and combines their elements to form a single list.
In a procedural language such as Lisp, we could define @code{append} in terms
of the basic list constructor @code{cons}, as we did in section @ref{2-2-1}:

@lisp
(define (append x y)
  (if (null? x)
      y
      (cons (car x) (append (cdr x) y))))
@end lisp

This procedure can be regarded as a translation into Lisp of the following two
rules, the first of which covers the case where the first list is empty and the
second of which handles the case of a nonempty list, which is a @code{cons} of
two parts:

@itemize @bullet

@item
For any list @code{y}, the empty list and @code{y} @code{append} to form
@code{y}.

@item
For any @code{u}, @code{v}, @code{y}, and @code{z}, @code{(cons u v)} and
@code{y} @code{append} to form @code{(cons u z)} if @code{v} and @code{y}
@code{append} to form @code{z}.@footnote{To see the correspondence between the
rules and the procedure, let @code{x} in the procedure (where @code{x} is
nonempty) correspond to @code{(cons u v)} in the rule.  Then @code{z} in the
rule corresponds to the @code{append} of @code{(cdr x)} and @code{y}.}

@end itemize

Using the @code{append} procedure, we can answer questions such as

@quotation
Find the @code{append} of @code{(a b)} and @code{(c d)}.
@end quotation

But the same two rules are also sufficient for answering the following sorts of
questions, which the procedure can't answer:

@quotation
Find a list @code{y} that @code{append}s with @code{(a b)} to produce @code{(a
b c d)}.

Find all @code{x} and @code{y} that @code{append} to form @code{(a b c d)}.
@end quotation

In a logic programming language, the programmer writes an @code{append}
``procedure'' by stating the two rules about @code{append} given above.  ``How
to'' knowledge is provided automatically by the interpreter to allow this
single pair of rules to be used to answer all three types of questions about
@code{append}.@footnote{This certainly does not relieve the user of the entire
problem of how to compute the answer.  There are many different mathematically
equivalent sets of rules for formulating the @code{append} relation, only some
of which can be turned into effective devices for computing in any direction.
In addition, sometimes ``what is'' information gives no clue ``how to'' compute
an answer.  For example, consider the problem of computing the @i{y} such that
@i{y}^2 = @i{x}.}

Contemporary logic programming languages (including the one we implement here)
have substantial deficiencies, in that their general ``how to'' methods can
lead them into spurious infinite loops or other undesirable behavior.  Logic
programming is an active field of research in computer
science.@footnote{Interest in logic programming peaked during the early 80s
when the Japanese government began an ambitious project aimed at building
superfast computers optimized to run logic programming languages.  The speed of
such computers was to be measured in LIPS (Logical Inferences Per Second)
rather than the usual FLOPS (FLoating-point Operations Per Second).  Although
the project succeeded in developing hardware and software as originally
planned, the international computer industry moved in a different direction.
See Feigenbaum and Shrobe 1993 for an overview evaluation of the Japanese
project.  The logic programming community has also moved on to consider
relational programming based on techniques other than simple pattern matching,
such as the ability to deal with numerical constraints such as the ones
illustrated in the constraint-propagation system of section @ref{3-3-5}.}

Earlier in this chapter we explored the technology of implementing interpreters
and described the elements that are essential to an interpreter for a Lisp-like
language (indeed, to an interpreter for any conventional language).  Now we
will apply these ideas to discuss an interpreter for a logic programming
language.  We call this language the @newterm{query language}, because it is
very useful for retrieving information from data bases by formulating
@newterm{queries}, or questions, expressed in the language.  Even though the
query language is very different from Lisp, we will find it convenient to
describe the language in terms of the same general framework we have been using
all along: as a collection of primitive elements, together with means of
combination that enable us to combine simple elements to create more complex
elements and means of abstraction that enable us to regard complex elements as
single conceptual units.  An interpreter for a logic programming language is
considerably more complex than an interpreter for a language like Lisp.
Nevertheless, we will see that our query-language interpreter contains many of
the same elements found in the interpreter of section @ref{4-1}.  In
particular, there will be an ``eval'' part that classifies expressions
according to type and an ``apply'' part that implements the language's
abstraction mechanism (procedures in the case of Lisp, and @newterm{rules} in
the case of logic programming).  Also, a central role is played in the
implementation by a frame data structure, which determines the correspondence
between symbols and their associated values.  One additional interesting aspect
of our query-language implementation is that we make substantial use of
streams, which were introduced in @ref{Chapter 3}.

@menu
* 4-4-1::            Deductive Information Retrieval
* 4-4-2::            How the Query System Works
* 4-4-3::            Is Logic Programming Mathematical Logic?
* 4-4-4::            Implementing the Query System
@end menu

@node	4-4-1, 4-4-2, 4-4, 4-4
@subsection Deductive Information Retrieval

Logic programming excels in providing interfaces to data bases for information
retrieval.  The query language we shall implement in this chapter is designed
to be used in this way.

In order to illustrate what the query system does, we will show how it can be
used to manage the data base of personnel records for Microshaft, a thriving
high-technology company in the Boston area.  The language provides
pattern-directed access to personnel information and can also take advantage of
general rules in order to make logical deductions.

@subsubheading A sample data base

The personnel data base for Microshaft contains @newterm{assertions} about
company personnel.  Here is the information about Ben Bitdiddle, the resident
computer wizard:

@lisp
(address (Bitdiddle Ben) (Slumerville (Ridge Road) 10))
(job (Bitdiddle Ben) (computer wizard))
(salary (Bitdiddle Ben) 60000)
@end lisp

Each assertion is a list (in this case a triple) whose elements can themselves
be lists.

As resident wizard, Ben is in charge of the company's computer division, and he
supervises two programmers and one technician.  Here is the information about
them:

@lisp
(address (Hacker Alyssa P) (Cambridge (Mass Ave) 78))
(job (Hacker Alyssa P) (computer programmer))
(salary (Hacker Alyssa P) 40000)
(supervisor (Hacker Alyssa P) (Bitdiddle Ben))

(address (Fect Cy D) (Cambridge (Ames Street) 3))
(job (Fect Cy D) (computer programmer))
(salary (Fect Cy D) 35000)
(supervisor (Fect Cy D) (Bitdiddle Ben))

(address (Tweakit Lem E) (Boston (Bay State Road) 22))
(job (Tweakit Lem E) (computer technician))
(salary (Tweakit Lem E) 25000)
(supervisor (Tweakit Lem E) (Bitdiddle Ben))
@end lisp

There is also a programmer trainee, who is supervised by Alyssa:

@lisp
(address (Reasoner Louis) (Slumerville (Pine Tree Road) 80))
(job (Reasoner Louis) (computer programmer trainee))
(salary (Reasoner Louis) 30000)
(supervisor (Reasoner Louis) (Hacker Alyssa P))
@end lisp

All of these people are in the computer division, as indicated by the word
@code{computer} as the first item in their job descriptions.

Ben is a high-level employee.  His supervisor is the company's big wheel
himself:

@lisp
(supervisor (Bitdiddle Ben) (Warbucks Oliver))

(address (Warbucks Oliver) (Swellesley (Top Heap Road)))
(job (Warbucks Oliver) (administration big wheel))
(salary (Warbucks Oliver) 150000)
@end lisp

Besides the computer division supervised by Ben, the company has an accounting
division, consisting of a chief accountant and his assistant:

@lisp
(address (Scrooge Eben) (Weston (Shady Lane) 10))
(job (Scrooge Eben) (accounting chief accountant))
(salary (Scrooge Eben) 75000)
(supervisor (Scrooge Eben) (Warbucks Oliver))

(address (Cratchet Robert) (Allston (N Harvard Street) 16))
(job (Cratchet Robert) (accounting scrivener))
(salary (Cratchet Robert) 18000)
(supervisor (Cratchet Robert) (Scrooge Eben))
@end lisp

There is also a secretary for the big wheel:

@lisp
(address (Aull DeWitt) (Slumerville (Onion Square) 5))
(job (Aull DeWitt) (administration secretary))
(salary (Aull DeWitt) 25000)
(supervisor (Aull DeWitt) (Warbucks Oliver))
@end lisp

The data base also contains assertions about which kinds of jobs can be done by
people holding other kinds of jobs.  For instance, a computer wizard can do the
jobs of both a computer programmer and a computer technician:

@lisp
(can-do-job (computer wizard) (computer programmer))
(can-do-job (computer wizard) (computer technician))
@end lisp

A computer programmer could fill in for a trainee:

@lisp
(can-do-job (computer programmer)
            (computer programmer trainee))
@end lisp

Also, as is well known,

@lisp
(can-do-job (administration secretary)
            (administration big wheel))
@end lisp

@subsubheading Simple queries

The query language allows users to retrieve information from the data base by
posing queries in response to the system's prompt.  For example, to find all
computer programmers one can say

@lisp
@i{;;; Query input:}
(job ?x (computer programmer))
@end lisp

The system will respond with the following items:

@lisp
@i{;;; Query results:}
(job (Hacker Alyssa P) (computer programmer))
(job (Fect Cy D) (computer programmer))
@end lisp

The input query specifies that we are looking for entries in the data base that
match a certain @newterm{pattern}.  In this example, the pattern specifies
entries consisting of three items, of which the first is the literal symbol
@code{job}, the second can be anything, and the third is the literal list
@code{(computer programmer)}.  The ``anything'' that can be the second item in
the matching list is specified by a @newterm{pattern variable}, @code{?x}.  The
general form of a pattern variable is a symbol, taken to be the name of the
variable, preceded by a question mark.  We will see below why it is useful to
specify names for pattern variables rather than just putting @code{?} into
patterns to represent ``anything.''  The system responds to a simple query by
showing all entries in the data base that match the specified pattern.

A pattern can have more than one variable.  For example, the query

@lisp
(address ?x ?y)
@end lisp

@noindent
will list all the employees' addresses.

A pattern can have no variables, in which case the query simply determines
whether that pattern is an entry in the data base.  If so, there will be one
match; if not, there will be no matches.

The same pattern variable can appear more than once in a query, specifying that
the same ``anything'' must appear in each position.  This is why variables have
names.  For example,

@lisp
(supervisor ?x ?x)
@end lisp

@noindent
finds all people who supervise themselves (though there are no such assertions
in our sample data base).

The query

@lisp
(job ?x (computer ?type))
@end lisp

@noindent
matches all job entries whose third item is a two-element list whose first item
is @code{computer}:

@lisp
(job (Bitdiddle Ben) (computer wizard))
(job (Hacker Alyssa P) (computer programmer))
(job (Fect Cy D) (computer programmer))
(job (Tweakit Lem E) (computer technician))
@end lisp

This same pattern does @emph{not} match

@lisp
(job (Reasoner Louis) (computer programmer trainee))
@end lisp

@noindent
because the third item in the entry is a list of three elements, and the
pattern's third item specifies that there should be two elements.  If we wanted
to change the pattern so that the third item could be any list beginning with
@code{computer}, we could specify@footnote{This uses the dotted-tail notation
introduced in @ref{Exercise 2-20}.}

@lisp
(job ?x (computer . ?type))
@end lisp

For example,

@lisp
(computer . ?type)
@end lisp

@noindent
matches the data

@lisp
(computer programmer trainee)
@end lisp

@noindent
with @code{?type} as the list @code{(programmer trainee)}.  It also
matches the data

@lisp
(computer programmer)
@end lisp

@noindent
with @code{?type} as the list @code{(programmer)}, and matches the data

@lisp
(computer)
@end lisp

@noindent
with @code{?type} as the empty list @code{()}.

We can describe the query language's processing of simple queries as follows:

@itemize @bullet

@item
The system finds all assignments to variables in the query pattern that
@newterm{satisfy} the pattern---that is, all sets of values for the variables
such that if the pattern variables are @newterm{instantiated with} (replaced
by) the values, the result is in the data base.

@item
The system responds to the query by listing all instantiations of the query
pattern with the variable assignments that satisfy it.

@end itemize

Note that if the pattern has no variables, the query reduces to a determination
of whether that pattern is in the data base.  If so, the empty assignment,
which assigns no values to variables, satisfies that pattern for that data
base.

@quotation
@strong{@anchor{Exercise 4-55}Exercise 4.55:} Give simple queries that retrieve
the following information from the data base:

@enumerate

@item
all people supervised by Ben Bitdiddle;

@item
the names and jobs of all people in the accounting division;

@item
the names and addresses of all people who live in Slumerville.

@end enumerate
@end quotation

@subsubheading Compound queries

Simple queries form the primitive operations of the query language.  In order
to form compound operations, the query language provides means of combination.
One thing that makes the query language a logic programming language is that
the means of combination mirror the means of combination used in forming
logical expressions: @code{and}, @code{or}, and @code{not}.  (Here @code{and},
@code{or}, and @code{not} are not the Lisp primitives, but rather operations
built into the query language.)

We can use @code{and} as follows to find the addresses of all the computer
programmers:

@lisp
(and (job ?person (computer programmer))
     (address ?person ?where))
@end lisp

The resulting output is

@lisp
(and (job (Hacker Alyssa P) (computer programmer))
     (address (Hacker Alyssa P) (Cambridge (Mass Ave) 78)))

(and (job (Fect Cy D) (computer programmer))
     (address (Fect Cy D) (Cambridge (Ames Street) 3)))
@end lisp

In general,

@lisp
(and <@var{query_1}> <@var{query_2}> @dots{} <@var{query_@i{n}}>)
@end lisp

@noindent
is satisfied by all sets of values for the pattern variables that
simultaneously satisfy <@var{query_1}> @dots{} <@var{query_@i{n}}>.

As for simple queries, the system processes a compound query by finding all
assignments to the pattern variables that satisfy the query, then displaying
instantiations of the query with those values.

Another means of constructing compound queries is through @code{or}.  For
example,

@lisp
(or (supervisor ?x (Bitdiddle Ben))
    (supervisor ?x (Hacker Alyssa P)))
@end lisp

@noindent
will find all employees supervised by Ben Bitdiddle or Alyssa P.  Hacker:

@lisp
(or (supervisor (Hacker Alyssa P) (Bitdiddle Ben))
    (supervisor (Hacker Alyssa P) (Hacker Alyssa P)))

(or (supervisor (Fect Cy D) (Bitdiddle Ben))
    (supervisor (Fect Cy D) (Hacker Alyssa P)))

(or (supervisor (Tweakit Lem E) (Bitdiddle Ben))
    (supervisor (Tweakit Lem E) (Hacker Alyssa P)))

(or (supervisor (Reasoner Louis) (Bitdiddle Ben))
    (supervisor (Reasoner Louis) (Hacker Alyssa P)))
@end lisp

In general,

@lisp
(or <@var{query_1}> <@var{query_2}> @dots{} <@var{query_@i{n}}>)
@end lisp

@noindent
is satisfied by all sets of values for the pattern variables that satisfy at
least one of <@var{query_1}> @dots{} <@var{query_@i{n}}>.

Compound queries can also be formed with @code{not}. For example,

@lisp
(and (supervisor ?x (Bitdiddle Ben))
     (not (job ?x (computer programmer))))
@end lisp

@noindent
finds all people supervised by Ben Bitdiddle who are not computer programmers.
In general,

@lisp
(not <@var{query_1}>)
@end lisp

@noindent
is satisfied by all assignments to the pattern variables that do not satisfy
<@var{query_1}>.@footnote{Actually, this description of @code{not} is valid
only for simple cases.  The real behavior of @code{not} is more complex.  We
will examine @code{not}'s peculiarities in sections @ref{4-4-2} and
@ref{4-4-3}.}

The final combining form is called @code{lisp-value}.  When @code{lisp-value}
is the first element of a pattern, it specifies that the next element is a Lisp
predicate to be applied to the rest of the (instantiated) elements as
arguments.  In general,

@lisp
(lisp-value <@var{predicate}> <@var{arg_1}> @dots{} <@var{arg_@i{n}}>)
@end lisp

@noindent
will be satisfied by assignments to the pattern variables for which the
<@var{predicate}> applied to the instantiated <@var{arg_1}> @dots{}
<@var{arg_@i{n}}> is true.  For example, to find all people whose salary is
greater than $30,000 we could write@footnote{@code{Lisp-value} should be used
only to perform an operation not provided in the query language.  In
particular, it should not be used to test equality (since that is what the
matching in the query language is designed to do) or inequality (since that can
be done with the @code{same} rule shown below).}

@lisp
(and (salary ?person ?amount)
     (lisp-value > ?amount 30000))
@end lisp

@quotation
@strong{@anchor{Exercise 4-56}Exercise 4.56:} Formulate compound queries that
retrieve the following information:

@enumerate a

@item
the names of all people who are supervised by Ben Bitdiddle, together with
their addresses;

@item
all people whose salary is less than Ben Bitdiddle's, together with their
salary and Ben Bitdiddle's salary;

@item
all people who are supervised by someone who is not in the computer division,
together with the supervisor's name and job.

@end enumerate
@end quotation

@subsubheading Rules

In addition to primitive queries and compound queries, the query language
provides means for abstracting queries.  These are given by @newterm{rules}.
The rule

@lisp
(rule (lives-near ?person-1 ?person-2)
      (and (address ?person-1 (?town . ?rest-1))
           (address ?person-2 (?town . ?rest-2))
           (not (same ?person-1 ?person-2))))
@end lisp

@noindent
specifies that two people live near each other if they live in the same town.
The final @code{not} clause prevents the rule from saying that all people live
near themselves.  The @code{same} relation is defined by a very simple
rule:@footnote{Notice that we do not need @code{same} in order to make two
things be the same: We just use the same pattern variable for each---in effect,
we have one thing instead of two things in the first place.  For example, see
@code{?town} in the @code{lives-near} rule and @code{?middle-manager} in the
@code{wheel} rule below.  @code{Same} is useful when we want to force two
things to be different, such as @code{?person-1} and @code{?person-2} in the
@code{lives-near} rule.  Although using the same pattern variable in two parts
of a query forces the same value to appear in both places, using different
pattern variables does not force different values to appear.  (The values
assigned to different pattern variables may be the same or different.)}

@lisp
(rule (same ?x ?x))
@end lisp

The following rule declares that a person is a ``wheel'' in an organization if
he supervises someone who is in turn a supervisor:

@lisp
(rule (wheel ?person)
      (and (supervisor ?middle-manager ?person)
           (supervisor ?x ?middle-manager)))
@end lisp

The general form of a rule is

@lisp
(rule <@var{conclusion}> <@var{body}>)
@end lisp

@noindent
where <@var{conclusion}> is a pattern and <@var{body}> is any
query.@footnote{We will also allow rules without bodies, as in @code{same}, and
we will interpret such a rule to mean that the rule conclusion is satisfied by
any values of the variables.} We can think of a rule as representing a large
(even infinite) set of assertions, namely all instantiations of the rule
conclusion with variable assignments that satisfy the rule body.  When we
described simple queries (patterns), we said that an assignment to variables
satisfies a pattern if the instantiated pattern is in the data base.  But the
pattern needn't be explicitly in the data base as an assertion.  It can be an
implicit assertion implied by a rule.  For example, the query

@lisp
(lives-near ?x (Bitdiddle Ben))
@end lisp

@noindent
results in

@lisp
(lives-near (Reasoner Louis) (Bitdiddle Ben))
(lives-near (Aull DeWitt) (Bitdiddle Ben))
@end lisp

To find all computer programmers who live near Ben Bitdiddle, we can ask

@lisp
(and (job ?x (computer programmer))
     (lives-near ?x (Bitdiddle Ben)))
@end lisp

As in the case of compound procedures, rules can be used as parts of other
rules (as we saw with the @code{lives-near} rule above) or even be defined
recursively.  For instance, the rule

@lisp
(rule (outranked-by ?staff-person ?boss)
      (or (supervisor ?staff-person ?boss)
          (and (supervisor ?staff-person ?middle-manager)
               (outranked-by ?middle-manager ?boss))))
@end lisp

@noindent
says that a staff person is outranked by a boss in the organization if the boss
is the person's supervisor or (recursively) if the person's supervisor is
outranked by the boss.

@quotation
@strong{@anchor{Exercise 4-57}Exercise 4.57:} Define a rule that says that
person 1 can replace person 2 if either person 1 does the same job as person 2
or someone who does person 1's job can also do person 2's job, and if person 1
and person 2 are not the same person. Using your rule, give queries that find
the following:

@enumerate a

@item
all people who can replace Cy D. Fect;

@item
all people who can replace someone who is being paid more than they are,
together with the two salaries.

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-58}Exercise 4.58:} Define a rule that says that a
person is a ``big shot'' in a division if the person works in the division but
does not have a supervisor who works in the division.
@end quotation

@quotation
@strong{@anchor{Exercise 4-59}Exercise 4.59:} Ben Bitdiddle has missed one
meeting too many.  Fearing that his habit of forgetting meetings could cost him
his job, Ben decides to do something about it.  He adds all the weekly meetings
of the firm to the Microshaft data base by asserting the following:

@lisp
(meeting accounting (Monday 9am))
(meeting administration (Monday 10am))
(meeting computer (Wednesday 3pm))
(meeting administration (Friday 1pm))
@end lisp

Each of the above assertions is for a meeting of an entire division.  Ben also
adds an entry for the company-wide meeting that spans all the divisions.  All
of the company's employees attend this meeting.

@lisp
(meeting whole-company (Wednesday 4pm))
@end lisp

@enumerate a

@item
On Friday morning, Ben wants to query the data base for all the meetings that
occur that day.  What query should he use?

@item
Alyssa P. Hacker is unimpressed.  She thinks it would be much more useful to be
able to ask for her meetings by specifying her name.  So she designs a rule
that says that a person's meetings include all @code{whole-company} meetings
plus all meetings of that person's division.  Fill in the body of Alyssa's
rule.

@lisp
(rule (meeting-time ?person ?day-and-time)
      <@var{rule-body}>)
@end lisp

@item
Alyssa arrives at work on Wednesday morning and wonders what meetings she has
to attend that day.  Having defined the above rule, what query should she make
to find this out?

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-60}Exercise 4.60:} By giving the query

@lisp
(lives-near ?person (Hacker Alyssa P))
@end lisp

Alyssa P. Hacker is able to find people who live near her, with whom she can
ride to work.  On the other hand, when she tries to find all pairs of people
who live near each other by querying

@lisp
(lives-near ?person-1 ?person-2)
@end lisp

@noindent
she notices that each pair of people who live near each other is listed twice;
for example,

@lisp
(lives-near (Hacker Alyssa P) (Fect Cy D))
(lives-near (Fect Cy D) (Hacker Alyssa P))
@end lisp

Why does this happen?  Is there a way to find a list of people who live near
each other, in which each pair appears only once?  Explain.
@end quotation

@subsubheading Logic as programs

We can regard a rule as a kind of logical implication: @emph{If} an assignment
of values to pattern variables satisfies the body, @emph{then} it satisfies the
conclusion.  Consequently, we can regard the query language as having the
ability to perform @newterm{logical deductions} based upon the rules.  As an
example, consider the @code{append} operation described at the beginning of
section @ref{4-4}.  As we said, @code{append} can be characterized by the
following two rules:

@itemize @bullet

@item
For any list @code{y}, the empty list and @code{y} @code{append} to form
@code{y}.

@item
For any @code{u}, @code{v}, @code{y}, and @code{z}, @code{(cons u v)} and
@code{y} @code{append} to form @code{(cons u z)} if @code{v} and @code{y}
@code{append} to form @code{z}.

@end itemize

To express this in our query language, we define two rules for a relation

@lisp
(append-to-form x y z)
@end lisp

@noindent
which we can interpret to mean ``@code{x} and @code{y} @code{append} to form
@code{z}'':

@lisp
(rule (append-to-form () ?y ?y))

(rule (append-to-form (?u . ?v) ?y (?u . ?z))
      (append-to-form ?v ?y ?z))
@end lisp

The first rule has no body, which means that the conclusion holds for any value
of @code{?y}.  Note how the second rule makes use of dotted-tail notation to
name the @code{car} and @code{cdr} of a list.

Given these two rules, we can formulate queries that compute the @code{append}
of two lists:

@lisp
@i{;;; Query input:}
(append-to-form (a b) (c d) ?z)
@i{;;; Query results:}
(append-to-form (a b) (c d) (a b c d))
@end lisp

What is more striking, we can use the same rules to ask the question ``Which
list, when @code{append}ed to @code{(a b)}, yields @code{(a b c d)}?''  This is
done as follows:

@lisp
@i{;;; Query input:}
(append-to-form (a b) ?y (a b c d))
@i{;;; Query results:}
(append-to-form (a b) (c d) (a b c d))
@end lisp

We can also ask for all pairs of lists that @code{append} to form @code{(a b c
d)}:

@lisp
@i{;;; Query input:}
(append-to-form ?x ?y (a b c d))
@i{;;; Query results:}
(append-to-form () (a b c d) (a b c d))
(append-to-form (a) (b c d) (a b c d))
(append-to-form (a b) (c d) (a b c d))
(append-to-form (a b c) (d) (a b c d))
(append-to-form (a b c d) () (a b c d))
@end lisp

The query system may seem to exhibit quite a bit of intelligence in using the
rules to deduce the answers to the queries above.  Actually, as we will see in
the next section, the system is following a well-determined algorithm in
unraveling the rules.  Unfortunately, although the system works impressively in
the @code{append} case, the general methods may break down in more complex
cases, as we will see in section @ref{4-4-3}.

@quotation
@strong{@anchor{Exercise 4-61}Exercise 4.61:} The following rules implement a
@code{next-to} relation that finds adjacent elements of a list:

@lisp
(rule (?x next-to ?y in (?x ?y . ?u)))

(rule (?x next-to ?y in (?v . ?z))
      (?x next-to ?y in ?z))
@end lisp

What will the response be to the following queries?

@lisp
(?x next-to ?y in (1 (2 3) 4))

(?x next-to 1 in (2 1 3 1))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 4-62}Exercise 4.62:} Define rules to implement the
@code{last-pair} operation of @ref{Exercise 2-17}, which returns a list
containing the last element of a nonempty list.  Check your rules on queries
such as @code{(last-pair (3) ?x)}, @code{(last-pair (1 2 3) ?x)}, and
@code{(last-pair (2 ?x) (3))}.  Do your rules work correctly on queries such as
@code{(last-pair ?x (3))} ?
@end quotation

@quotation
@strong{@anchor{Exercise 4-63}Exercise 4.63:} The following data base (see
Genesis 4) traces the genealogy of the descendants of Ada back to Adam, by way
of Cain:

@lisp
(son Adam Cain)
(son Cain Enoch)
(son Enoch Irad)
(son Irad Mehujael)
(son Mehujael Methushael)
(son Methushael Lamech)
(wife Lamech Ada)
(son Ada Jabal)
(son Ada Jubal)
@end lisp

Formulate rules such as ``If @i{S} is the son of @i{f}, and @i{f} is the son of
@i{G}, then @i{S} is the grandson of @i{G}'' and ``If @i{W} is the wife of
@i{M}, and @i{S} is the son of @i{W}, then @i{S} is the son of @i{M}'' (which
was supposedly more true in biblical times than today) that will enable the
query system to find the grandson of Cain; the sons of Lamech; the grandsons of
Methushael.  (See @ref{Exercise 4-69} for some rules to deduce more complicated
relationships.)
@end quotation

@node	4-4-2, 4-4-3, 4-4-1, 4-4
@subsection How the Query System Works

In section @ref{4-4-4} we will present an implementation of the query
interpreter as a collection of procedures.  In this section we give an overview
that explains the general structure of the system independent of low-level
implementation details.  After describing the implementation of the
interpreter, we will be in a position to understand some of its limitations and
some of the subtle ways in which the query language's logical operations differ
from the operations of mathematical logic.

It should be apparent that the query evaluator must perform some kind of search
in order to match queries against facts and rules in the data base.  One way to
do this would be to implement the query system as a nondeterministic program,
using the @code{amb} evaluator of section @ref{4-3} (see @ref{Exercise 4-78}).
Another possibility is to manage the search with the aid of streams.  Our
implementation follows this second approach.

The query system is organized around two central operations called
@newterm{pattern matching} and @newterm{unification}.  We first describe
pattern matching and explain how this operation, together with the organization
of information in terms of streams of frames, enables us to implement both
simple and compound queries.  We next discuss unification, a generalization of
pattern matching needed to implement rules.  Finally, we show how the entire
query interpreter fits together through a procedure that classifies expressions
in a manner analogous to the way @code{eval} classifies expressions for the
interpreter described in section @ref{4-1}.

@subsubheading Pattern matching

A @newterm{pattern matcher} is a program that tests whether some datum fits a
specified pattern.  For example, the data list @code{((a b) c (a b))} matches
the pattern @code{(?x c ?x)} with the pattern variable @code{?x} bound to
@code{(a b)}.  The same data list matches the pattern @code{(?x ?y ?z)} with
@code{?x} and @code{?z} both bound to @code{(a b)} and @code{?y} bound to
@code{c}.  It also matches the pattern @code{((?x ?y) c (?x ?y))} with
@code{?x} bound to @code{a} and @code{?y} bound to @code{b}.  However, it does
not match the pattern @code{(?x a ?y)}, since that pattern specifies a list
whose second element is the symbol @code{a}.

The pattern matcher used by the query system takes as inputs a pattern, a
datum, and a @newterm{frame} that specifies bindings for various pattern
variables.  It checks whether the datum matches the pattern in a way that is
consistent with the bindings already in the frame.  If so, it returns the given
frame augmented by any bindings that may have been determined by the match.
Otherwise, it indicates that the match has failed.

For example, using the pattern @code{(?x ?y ?x)} to match @code{(a b a)} given
an empty frame will return a frame specifying that @code{?x} is bound to
@code{a} and @code{?y} is bound to @code{b}.  Trying the match with the same
pattern, the same datum, and a frame specifying that @code{?y} is bound to
@code{a} will fail.  Trying the match with the same pattern, the same datum,
and a frame in which @code{?y} is bound to @code{b} and @code{?x} is unbound
will return the given frame augmented by a binding of @code{?x} to @code{a}.

The pattern matcher is all the mechanism that is needed to process simple
queries that don't involve rules.  For instance, to process the query

@lisp
(job ?x (computer programmer))
@end lisp

@noindent
we scan through all assertions in the data base and select those that match the
pattern with respect to an initially empty frame.  For each match we find, we
use the frame returned by the match to instantiate the pattern with a value for
@code{?x}.

@subsubheading Streams of frames

The testing of patterns against frames is organized through the use of streams.
Given a single frame, the matching process runs through the data-base entries
one by one.  For each data-base entry, the matcher generates either a special
symbol indicating that the match has failed or an extension to the frame.  The
results for all the data-base entries are collected into a stream, which is
passed through a filter to weed out the failures.  The result is a stream of
all the frames that extend the given frame via a match to some assertion in the
data base.@footnote{Because matching is generally very expensive, we would like
to avoid applying the full matcher to every element of the data base.  This is
usually arranged by breaking up the process into a fast, coarse match and the
final match.  The coarse match filters the data base to produce a small set of
candidates for the final match.  With care, we can arrange our data base so
that some of the work of coarse matching can be done when the data base is
constructed rather then when we want to select the candidates.  This is called
@newterm{indexing} the data base.  There is a vast technology built around
data-base-indexing schemes.  Our implementation, described in section
@ref{4-4-4}, contains a simple-minded form of such an optimization.}

In our system, a query takes an input stream of frames and performs the above
matching operation for every frame in the stream, as indicated in @ref{Figure
4-4}.  That is, for each frame in the input stream, the query generates a new
stream consisting of all extensions to that frame by matches to assertions in
the data base.  All these streams are then combined to form one huge stream,
which contains all possible extensions of every frame in the input stream.
This stream is the output of the query.

@quotation
@strong{@anchor{Figure 4-4}Figure 4.4:} A query processes a stream of frames.

@example
                                  output stream
  input stream   +-------------+  of frames,
  of frames      |    query    |  filtered and extended
---------------->|             +------------------------->
                 | (job ?x ?y) |
                 +-------------+
                        ^
                        |
               stream of assertions
                  from data base
@end example
@end quotation

To answer a simple query, we use the query with an input stream consisting of a
single empty frame.  The resulting output stream contains all extensions to the
empty frame (that is, all answers to our query).  This stream of frames is then
used to generate a stream of copies of the original query pattern with the
variables instantiated by the values in each frame, and this is the stream that
is finally printed.

@subsubheading Compound queries

The real elegance of the stream-of-frames implementation is evident when we
deal with compound queries.  The processing of compound queries makes use of
the ability of our matcher to demand that a match be consistent with a
specified frame.  For example, to handle the @code{and} of two queries, such as

@lisp
(and (can-do-job ?x (computer programmer trainee))
     (job ?person ?x))
@end lisp

@noindent
(informally, ``Find all people who can do the job of a computer programmer
trainee''), we first find all entries that match the pattern

@lisp
(can-do-job ?x (computer programmer trainee))
@end lisp

This produces a stream of frames, each of which contains a binding for
@code{?x}.  Then for each frame in the stream we find all entries that match

@lisp
(job ?person ?x)
@end lisp

@noindent
in a way that is consistent with the given binding for @code{?x}.  Each such
match will produce a frame containing bindings for @code{?x} and
@code{?person}.  The @code{and} of two queries can be viewed as a series
combination of the two component queries, as shown in @ref{Figure 4-5}.  The
frames that pass through the first query filter are filtered and further
extended by the second query.

@quotation
@strong{@anchor{Figure 4-5}Figure 4.5:} The @code{and} combination of two
queries is produced by operating on the stream of frames in series.

@example
                +----------------------+
                |       (and A B)      |
  input stream  |                      |  output stream
  of frames     |   +---+       +---+  |  of frames
------------------->| A +------>| B +-------------------->
                |   +---+       +---+  |
                |     ^           ^    |
                |     |           |    |
                |     +-----*-----+    |
                +-----------|----------+
                            |
                        data base
@end example
@end quotation

@ref{Figure 4-6} shows the analogous method for computing the @code{or} of two
queries as a parallel combination of the two component queries.  The input
stream of frames is extended separately by each query.  The two resulting
streams are then merged to produce the final output stream.

@quotation
@strong{@anchor{Figure 4-6}Figure 4.6:} The @code{or} combination of two
queries is produced by operating on the stream of frames in parallel and
merging the results.

@example
           +---------------------------+
           |          (or A B)         |
           |    +---+                  |
input      | +->| A |------------+     |  output
stream of  | |  +---+            V     |  stream of
frames     | |    ^          +-------+ |  frames
-------------*    |          | merge +--------------->
           | |    |          +-------+ |
           | |    |              ^     |
           | |    |   +---+      |     |
           | +------->| B +------+     |
           |      |   +---+            |
           |      |     ^              |
           |      |     |              |
           |      +--*--+              |
           +---------|-----------------+
                     |
                 data base
@end example
@end quotation

Even from this high-level description, it is apparent that the processing of
compound queries can be slow.  For example, since a query may produce more than
one output frame for each input frame, and each query in an @code{and} gets its
input frames from the previous query, an @code{and} query could, in the worst
case, have to perform a number of matches that is exponential in the number of
queries (see @ref{Exercise 4-76}).@footnote{But this kind of exponential
explosion is not common in @code{and} queries because the added conditions tend
to reduce rather than expand the number of frames produced.} Though systems for
handling only simple queries are quite practical, dealing with complex queries
is extremely difficult.@footnote{There is a large literature on
data-base-management systems that is concerned with how to handle complex
queries efficiently.}

From the stream-of-frames viewpoint, the @code{not} of some query acts as a
filter that removes all frames for which the query can be satisfied.  For
instance, given the pattern

@lisp
(not (job ?x (computer programmer)))
@end lisp

@noindent
we attempt, for each frame in the input stream, to produce extension frames
that satisfy @code{(job ?x (computer programmer))}.  We remove from the input
stream all frames for which such extensions exist.  The result is a stream
consisting of only those frames in which the binding for @code{?x} does not
satisfy @code{(job ?x (computer programmer))}.  For example, in processing the
query

@lisp
(and (supervisor ?x ?y)
     (not (job ?x (computer programmer))))
@end lisp

@noindent
the first clause will generate frames with bindings for @code{?x} and
@code{?y}.  The @code{not} clause will then filter these by removing all frames
in which the binding for @code{?x} satisfies the restriction that @code{?x} is
a computer programmer.@footnote{There is a subtle difference between this
filter implementation of @code{not} and the usual meaning of @code{not} in
mathematical logic.  See section @ref{4-4-3}.}

The @code{lisp-value} special form is implemented as a similar filter on frame
streams.  We use each frame in the stream to instantiate any variables in the
pattern, then apply the Lisp predicate.  We remove from the input stream all
frames for which the predicate fails.

@subsubheading Unification

In order to handle rules in the query language, we must be able to find the
rules whose conclusions match a given query pattern.  Rule conclusions are like
assertions except that they can contain variables, so we will need a
generalization of pattern matching---called @newterm{unification}---in which
both the ``pattern'' and the ``datum'' may contain variables.

A unifier takes two patterns, each containing constants and variables, and
determines whether it is possible to assign values to the variables that will
make the two patterns equal.  If so, it returns a frame containing these
bindings.  For example, unifying @code{(?x a ?y)} and @code{(?y ?z a)} will
specify a frame in which @code{?x}, @code{?y}, and @code{?z} must all be bound
to @code{a}.  On the other hand, unifying @code{(?x ?y a)} and @code{(?x b ?y)}
will fail, because there is no value for @code{?y} that can make the two
patterns equal.  (For the second elements of the patterns to be equal,
@code{?y} would have to be @code{b}; however, for the third elements to be
equal, @code{?y} would have to be @code{a}.)  The unifier used in the query
system, like the pattern matcher, takes a frame as input and performs
unifications that are consistent with this frame.

The unification algorithm is the most technically difficult part of the query
system.  With complex patterns, performing unification may seem to require
deduction.  To unify @code{(?x ?x)} and @code{((a ?y c) (a b ?z))}, for
example, the algorithm must infer that @code{?x} should be @code{(a b c)},
@code{?y} should be @code{b}, and @code{?z} should be @code{c}.  We may think
of this process as solving a set of equations among the pattern components.  In
general, these are simultaneous equations, which may require substantial
manipulation to solve.@footnote{In one-sided pattern matching, all the
equations that contain pattern variables are explicit and already solved for
the unknown (the pattern variable).} For example, unifying @code{(?x ?x)} and
@code{((a ?y c) (a b ?z))} may be thought of as specifying the simultaneous
equations

@lisp
?x  =  (a ?y c)
?x  =  (a b ?z)
@end lisp

These equations imply that

@lisp
(a ?y c)  =  (a b ?z)
@end lisp

@noindent
which in turn implies that

@lisp
a  =  a, ?y  =  b, c  =  ?z,
@end lisp

@noindent
and hence that

@lisp
?x  =  (a b c)
@end lisp

In a successful pattern match, all pattern variables become bound, and the
values to which they are bound contain only constants.  This is also true of
all the examples of unification we have seen so far.  In general, however, a
successful unification may not completely determine the variable values; some
variables may remain unbound and others may be bound to values that contain
variables.

Consider the unification of @code{(?x a)} and @code{((b ?y) ?z)}.  We can
deduce that @code{?x = (b ?y)} and @code{a = ?z}, but we cannot further solve
for @code{?x} or @code{?y}.  The unification doesn't fail, since it is
certainly possible to make the two patterns equal by assigning values to
@code{?x} and @code{?y}.  Since this match in no way restricts the values
@code{?y} can take on, no binding for @code{?y} is put into the result frame.
The match does, however, restrict the value of @code{?x}.  Whatever value
@code{?y} has, @code{?x} must be @code{(b ?y)}.  A binding of @code{?x} to the
pattern @code{(b ?y)} is thus put into the frame.  If a value for @code{?y} is
later determined and added to the frame (by a pattern match or unification that
is required to be consistent with this frame), the previously bound @code{?x}
will refer to this value.@footnote{Another way to think of unification is that
it generates the most general pattern that is a specialization of the two input
patterns.  That is, the unification of @code{(?x a)} and @code{((b ?y) ?z)} is
@code{((b ?y) a)}, and the unification of @code{(?x a ?y)} and @code{(?y ?z
a)}, discussed above, is @code{(a a a)}.  For our implementation, it is more
convenient to think of the result of unification as a frame rather than a
pattern.}

@subsubheading Applying rules

Unification is the key to the component of the query system that makes
inferences from rules. To see how this is accomplished, consider processing a
query that involves applying a rule, such as

@lisp
(lives-near ?x (Hacker Alyssa P))
@end lisp

To process this query, we first use the ordinary pattern-match procedure
described above to see if there are any assertions in the data base that match
this pattern.  (There will not be any in this case, since our data base
includes no direct assertions about who lives near whom.)  The next step is to
attempt to unify the query pattern with the conclusion of each rule.  We find
that the pattern unifies with the conclusion of the rule

@lisp
(rule (lives-near ?person-1 ?person-2)
      (and (address ?person-1 (?town . ?rest-1))
           (address ?person-2 (?town . ?rest-2))
           (not (same ?person-1 ?person-2))))
@end lisp

@noindent
resulting in a frame specifying that @code{?person-2} is bound to @code{(Hacker
Alyssa P)} and that @code{?x} should be bound to (have the same value as)
@code{?person-1}.  Now, relative to this frame, we evaluate the compound query
given by the body of the rule.  Successful matches will extend this frame by
providing a binding for @code{?person-1}, and consequently a value for
@code{?x}, which we can use to instantiate the original query pattern.

In general, the query evaluator uses the following method to apply a rule when
trying to establish a query pattern in a frame that specifies bindings for some
of the pattern variables:

@itemize @bullet

@item
Unify the query with the conclusion of the rule to form, if successful, an
extension of the original frame.

@item
Relative to the extended frame, evaluate the query formed by the body of the
rule.

@end itemize

Notice how similar this is to the method for applying a procedure in the
@code{eval}/@code{apply} evaluator for Lisp:

@itemize @bullet

@item
Bind the procedure's parameters to its arguments to form a frame that extends
the original procedure environment.

@item
Relative to the extended environment, evaluate the expression formed by the
body of the procedure.

@end itemize

The similarity between the two evaluators should come as no surprise.  Just as
procedure definitions are the means of abstraction in Lisp, rule definitions
are the means of abstraction in the query language.  In each case, we unwind
the abstraction by creating appropriate bindings and evaluating the rule or
procedure body relative to these.

@subsubheading Simple queries

We saw earlier in this section how to evaluate simple queries in the absence of
rules.  Now that we have seen how to apply rules, we can describe how to
evaluate simple queries by using both rules and assertions.

Given the query pattern and a stream of frames, we produce, for each frame in
the input stream, two streams:

@itemize @bullet

@item
a stream of extended frames obtained by matching the pattern against all
assertions in the data base (using the pattern matcher), and

@item
a stream of extended frames obtained by applying all possible rules (using the
unifier).@footnote{Since unification is a generalization of matching, we could
simplify the system by using the unifier to produce both streams.  Treating the
easy case with the simple matcher, however, illustrates how matching (as
opposed to full-blown unification) can be useful in its own right.}

@end itemize

Appending these two streams produces a stream that consists of all the ways
that the given pattern can be satisfied consistent with the original frame.
These streams (one for each frame in the input stream) are now all combined to
form one large stream, which therefore consists of all the ways that any of the
frames in the original input stream can be extended to produce a match with the
given pattern.

@subsubheading The query evaluator and the driver loop

Despite the complexity of the underlying matching operations, the system is
organized much like an evaluator for any language.  The procedure that
coordinates the matching operations is called @code{qeval}, and it plays a role
analogous to that of the @code{eval} procedure for Lisp.  @code{Qeval} takes as
inputs a query and a stream of frames.  Its output is a stream of frames,
corresponding to successful matches to the query pattern, that extend some
frame in the input stream, as indicated in @ref{Figure 4-4}.  Like @code{eval},
@code{qeval} classifies the different types of expressions (queries) and
dispatches to an appropriate procedure for each.  There is a procedure for each
special form (@code{and}, @code{or}, @code{not}, and @code{lisp-value}) and one
for simple queries.

The driver loop, which is analogous to the @code{driver-loop} procedure for the
other evaluators in this chapter, reads queries from the terminal.  For each
query, it calls @code{qeval} with the query and a stream that consists of a
single empty frame.  This will produce the stream of all possible matches (all
possible extensions to the empty frame).  For each frame in the resulting
stream, it instantiates the original query using the values of the variables
found in the frame.  This stream of instantiated queries is then
printed.@footnote{The reason we use streams (rather than lists) of frames is
that the recursive application of rules can generate infinite numbers of values
that satisfy a query.  The delayed evaluation embodied in streams is crucial
here: The system will print responses one by one as they are generated,
regardless of whether there are a finite or infinite number of responses.}

The driver also checks for the special command @code{assert!}, which signals
that the input is not a query but rather an assertion or rule to be added to
the data base.  For instance,

@lisp
(assert! (job (Bitdiddle Ben) (computer wizard)))

(assert! (rule (wheel ?person)
               (and (supervisor ?middle-manager ?person)
                    (supervisor ?x ?middle-manager))))
@end lisp

@node	4-4-3, 4-4-4, 4-4-2, 4-4
@subsection Is Logic Programming Mathematical Logic?

The means of combination used in the query language may at first seem identical
to the operations @code{and}, @code{or}, and @code{not} of mathematical logic,
and the application of query-language rules is in fact accomplished through a
legitimate method of inference.@footnote{That a particular method of inference
is legitimate is not a trivial assertion.  One must prove that if one starts
with true premises, only true conclusions can be derived.  The method of
inference represented by rule applications is @newterm{modus ponens}, the
familiar method of inference that says that if @i{A} is true and @emph{A
implies B} is true, then we may conclude that @i{B} is true.} This
identification of the query language with mathematical logic is not really
valid, though, because the query language provides a @newterm{control
structure} that interprets the logical statements procedurally.  We can often
take advantage of this control structure.  For example, to find all of the
supervisors of programmers we could formulate a query in either of two
logically equivalent forms:

@lisp
(and (job ?x (computer programmer))
     (supervisor ?x ?y))
@end lisp

@noindent
or

@lisp
(and (supervisor ?x ?y)
     (job ?x (computer programmer)))
@end lisp

If a company has many more supervisors than programmers (the usual case), it is
better to use the first form rather than the second because the data base must
be scanned for each intermediate result (frame) produced by the first clause of
the @code{and}.

The aim of logic programming is to provide the programmer with techniques for
decomposing a computational problem into two separate problems: ``what'' is to
be computed, and ``how'' this should be computed.  This is accomplished by
selecting a subset of the statements of mathematical logic that is powerful
enough to be able to describe anything one might want to compute, yet weak
enough to have a controllable procedural interpretation.  The intention here is
that, on the one hand, a program specified in a logic programming language
should be an effective program that can be carried out by a computer.  Control
(``how'' to compute) is effected by using the order of evaluation of the
language.  We should be able to arrange the order of clauses and the order of
subgoals within each clause so that the computation is done in an order deemed
to be effective and efficient.  At the same time, we should be able to view the
result of the computation (``what'' to compute) as a simple consequence of the
laws of logic.

Our query language can be regarded as just such a procedurally interpretable
subset of mathematical logic.  An assertion represents a simple fact (an atomic
proposition).  A rule represents the implication that the rule conclusion holds
for those cases where the rule body holds.  A rule has a natural procedural
interpretation: To establish the conclusion of the rule, establish the body of
the rule.  Rules, therefore, specify computations.  However, because rules can
also be regarded as statements of mathematical logic, we can justify any
``inference'' accomplished by a logic program by asserting that the same result
could be obtained by working entirely within mathematical logic.@footnote{We
must qualify this statement by agreeing that, in speaking of the ``inference''
accomplished by a logic program, we assume that the computation terminates.
Unfortunately, even this qualified statement is false for our implementation of
the query language (and also false for programs in Prolog and most other
current logic programming languages) because of our use of @code{not} and
@code{lisp-value}.  As we will describe below, the @code{not} implemented in
the query language is not always consistent with the @code{not} of mathematical
logic, and @code{lisp-value} introduces additional complications.  We could
implement a language consistent with mathematical logic by simply removing
@code{not} and @code{lisp-value} from the language and agreeing to write
programs using only simple queries, @code{and}, and @code{or}.  However, this
would greatly restrict the expressive power of the language.  One of the major
concerns of research in logic programming is to find ways to achieve more
consistency with mathematical logic without unduly sacrificing expressive
power.}

@subsubheading Infinite loops

A consequence of the procedural interpretation of logic programs is that it is
possible to construct hopelessly inefficient programs for solving certain
problems.  An extreme case of inefficiency occurs when the system falls into
infinite loops in making deductions.  As a simple example, suppose we are
setting up a data base of famous marriages, including

@lisp
(assert! (married Minnie Mickey))
@end lisp

If we now ask

@lisp
(married Mickey ?who)
@end lisp

@noindent
we will get no response, because the system doesn't know that if @i{A} is
married to @i{B}, then @i{B} is married to @i{A}.  So we assert the rule

@lisp
(assert! (rule (married ?x ?y)
               (married ?y ?x)))
@end lisp

@noindent
and again query

@lisp
(married Mickey ?who)
@end lisp

Unfortunately, this will drive the system into an infinite loop, as follows:

@itemize @bullet

@item
The system finds that the @code{married} rule is applicable; that is, the rule
conclusion @code{(married ?x ?y)} successfully unifies with the query pattern
@code{(married Mickey ?who)} to produce a frame in which @code{?x} is bound to
@code{Mickey} and @code{?y} is bound to @code{?who}.  So the interpreter
proceeds to evaluate the rule body @code{(married ?y ?x)} in this frame---in
effect, to process the query @code{(married ?who Mickey)}.

@item
One answer appears directly as an assertion in the data base: @code{(married
Minnie Mickey)}.

@item
The @code{married} rule is also applicable, so the interpreter again evaluates
the rule body, which this time is equivalent to @code{(married Mickey ?who)}.

@end itemize

The system is now in an infinite loop.  Indeed, whether the system will find
the simple answer @code{(married Minnie Mickey)} before it goes into the loop
depends on implementation details concerning the order in which the system
checks the items in the data base.  This is a very simple example of the kinds
of loops that can occur.  Collections of interrelated rules can lead to loops
that are much harder to anticipate, and the appearance of a loop can depend on
the order of clauses in an @code{and} (see @ref{Exercise 4-64}) or on low-level
details concerning the order in which the system processes
queries.@footnote{This is not a problem of the logic but one of the procedural
interpretation of the logic provided by our interpreter.  We could write an
interpreter that would not fall into a loop here.  For example, we could
enumerate all the proofs derivable from our assertions and our rules in a
breadth-first rather than a depth-first order.  However, such a system makes it
more difficult to take advantage of the order of deductions in our programs.
One attempt to build sophisticated control into such a program is described in
deKleer et al. 1977.  Another technique, which does not lead to such serious
control problems, is to put in special knowledge, such as detectors for
particular kinds of loops (@ref{Exercise 4-67}).  However, there can be no
general scheme for reliably preventing a system from going down infinite paths
in performing deductions.  Imagine a diabolical rule of the form ``To show
@i{P}(@i{x}) is true, show that @i{P}(@i{f}(@i{x})) is true,'' for some
suitably chosen function @i{f}.}

@subsubheading Problems with @code{not}

Another quirk in the query system concerns @code{not}.  Given the data base of
section @ref{4-4-1}, consider the following two queries:

@lisp
(and (supervisor ?x ?y)
     (not (job ?x (computer programmer))))

(and (not (job ?x (computer programmer)))
     (supervisor ?x ?y))
@end lisp

These two queries do not produce the same result.  The first query begins by
finding all entries in the data base that match @code{(supervisor ?x ?y)}, and
then filters the resulting frames by removing the ones in which the value of
@code{?x} satisfies @code{(job ?x (computer programmer))}.  The second query
begins by filtering the incoming frames to remove those that can satisfy
@code{(job ?x (computer programmer))}.  Since the only incoming frame is empty,
it checks the data base to see if there are any patterns that satisfy
@code{(job ?x (computer programmer))}.  Since there generally are entries of
this form, the @code{not} clause filters out the empty frame and returns an
empty stream of frames.  Consequently, the entire compound query returns an
empty stream.

The trouble is that our implementation of @code{not} really is meant to serve
as a filter on values for the variables.  If a @code{not} clause is processed
with a frame in which some of the variables remain unbound (as does @code{?x}
in the example above), the system will produce unexpected results. Similar
problems occur with the use of @code{lisp-value}---the Lisp predicate can't
work if some of its arguments are unbound.  See @ref{Exercise 4-77}.

There is also a much more serious way in which the @code{not} of the query
language differs from the @code{not} of mathematical logic.  In logic, we
interpret the statement ``not @i{P}'' to mean that @i{P} is not true.  In the
query system, however, ``not @i{P}'' means that @i{P} is not deducible from the
knowledge in the data base.  For example, given the personnel data base of
section @ref{4-4-1}, the system would happily deduce all sorts of @code{not}
statements, such as that Ben Bitdiddle is not a baseball fan, that it is not
raining outside, and that 2 + 2 is not 4.@footnote{Consider the query
@code{(not (baseball-fan (Bitdiddle Ben)))}.  The system finds that
@code{(baseball-fan (Bitdiddle Ben))} is not in the data base, so the empty
frame does not satisfy the pattern and is not filtered out of the initial
stream of frames.  The result of the query is thus the empty frame, which is
used to instantiate the input query to produce @code{(not (baseball-fan
(Bitdiddle Ben)))}.} In other words, the @code{not} of logic programming
languages reflects the so-called @newterm{closed world assumption} that all
relevant information has been included in the data base.@footnote{A discussion
and justification of this treatment of @code{not} can be found in the article
by Clark (1978).}

@quotation
@strong{@anchor{Exercise 4-64}Exercise 4.64:} Louis Reasoner mistakenly deletes
the @code{outranked-by} rule (section @ref{4-4-1}) from the data base.  When he
realizes this, he quickly reinstalls it.  Unfortunately, he makes a slight
change in the rule, and types it in as

@lisp
(rule (outranked-by ?staff-person ?boss)
      (or (supervisor ?staff-person ?boss)
          (and (outranked-by ?middle-manager ?boss)
               (supervisor ?staff-person ?middle-manager))))
@end lisp

Just after Louis types this information into the system, DeWitt Aull comes by
to find out who outranks Ben Bitdiddle. He issues the query

@lisp
(outranked-by (Bitdiddle Ben) ?who)
@end lisp

After answering, the system goes into an infinite loop.  Explain why.
@end quotation

@quotation
@strong{@anchor{Exercise 4-65}Exercise 4.65:} Cy D. Fect, looking forward to
the day when he will rise in the organization, gives a query to find all the
wheels (using the @code{wheel} rule of section @ref{4-4-1}):

@lisp
(wheel ?who)
@end lisp

To his surprise, the system responds

@lisp
@i{;;; Query results:}
(wheel (Warbucks Oliver))
(wheel (Bitdiddle Ben))
(wheel (Warbucks Oliver))
(wheel (Warbucks Oliver))
(wheel (Warbucks Oliver))
@end lisp

Why is Oliver Warbucks listed four times?
@end quotation

@quotation
@strong{@anchor{Exercise 4-66}Exercise 4.66:} Ben has been generalizing the
query system to provide statistics about the company.  For example, to find the
total salaries of all the computer programmers one will be able to say

@lisp
(sum ?amount
     (and (job ?x (computer programmer))
          (salary ?x ?amount)))
@end lisp

In general, Ben's new system allows expressions of the form

@lisp
(accumulation-function <@var{variable}>
                       <@var{query pattern}>)
@end lisp

@noindent
where @code{accumulation-function} can be things like @code{sum},
@code{average}, or @code{maximum}.  Ben reasons that it should be a cinch to
implement this.  He will simply feed the query pattern to @code{qeval}.  This
will produce a stream of frames.  He will then pass this stream through a
mapping function that extracts the value of the designated variable from each
frame in the stream and feed the resulting stream of values to the accumulation
function.  Just as Ben completes the implementation and is about to try it out,
Cy walks by, still puzzling over the @code{wheel} query result in exercise
@ref{Exercise 4-65}.  When Cy shows Ben the system's response, Ben groans,
``Oh, no, my simple accumulation scheme won't work!''

What has Ben just realized?  Outline a method he can use to salvage the
situation.
@end quotation

@quotation
@strong{@anchor{Exercise 4-67}Exercise 4.67:} Devise a way to install a loop
detector in the query system so as to avoid the kinds of simple loops
illustrated in the text and in @ref{Exercise 4-64}.  The general idea is that
the system should maintain some sort of history of its current chain of
deductions and should not begin processing a query that it is already working
on.  Describe what kind of information (patterns and frames) is included in
this history, and how the check should be made.  (After you study the details
of the query-system implementation in section @ref{4-4-4}, you may want to
modify the system to include your loop detector.)
@end quotation

@quotation
@strong{@anchor{Exercise 4-68}Exercise 4.68:} Define rules to implement the
@code{reverse} operation of @ref{Exercise 2-18}, which returns a list
containing the same elements as a given list in reverse order.  (Hint: Use
@code{append-to-form}.)  Can your rules answer both @code{(reverse (1 2 3) ?x)}
and @code{(reverse ?x (1 2 3))} ?
@end quotation

@quotation
@strong{@anchor{Exercise 4-69}Exercise 4.69:} Beginning with the data base and
the rules you formulated in @ref{Exercise 4-63}, devise a rule for adding
``greats'' to a grandson relationship. This should enable the system to deduce
that Irad is the great-grandson of Adam, or that Jabal and Jubal are the
great-great-great-great-great-grandsons of Adam.  (Hint: Represent the fact
about Irad, for example, as @code{((great grandson) Adam Irad)}.  Write rules
that determine if a list ends in the word @code{grandson}.  Use this to express
a rule that allows one to derive the relationship @code{((great .  ?rel) ?x
?y)}, where @code{?rel} is a list ending in @code{grandson}.)  Check your rules
on queries such as @code{((great grandson) ?g ?ggs)} and @code{(?relationship
Adam Irad)}.
@end quotation

@node	4-4-4,  , 4-4-3, 4-4
@subsection Implementing the Query System

Section @ref{4-4-2} described how the query system works. Now we fill in the
details by presenting a complete implementation of the system.

@menu
* 4-4-4-1::          The Driver Loop and Instantiation
* 4-4-4-2::          The Evaluator
* 4-4-4-3::          Finding Assertions by Pattern Matching
* 4-4-4-4::          Rules and Unification
* 4-4-4-5::          Maintaining the Data Base
* 4-4-4-6::          Stream Operations
* 4-4-4-7::          Query Syntax Procedures
* 4-4-4-8::          Frames and Bindings
@end menu

@node 4-4-4-1, 4-4-4-2, 4-4-4, 4-4-4
@subsubsection The Driver Loop and Instantiation

The driver loop for the query system repeatedly reads input expressions.  If
the expression is a rule or assertion to be added to the data base, then the
information is added.  Otherwise the expression is assumed to be a query.  The
driver passes this query to the evaluator @code{qeval} together with an initial
frame stream consisting of a single empty frame.  The result of the evaluation
is a stream of frames generated by satisfying the query with variable values
found in the data base.  These frames are used to form a new stream consisting
of copies of the original query in which the variables are instantiated with
values supplied by the stream of frames, and this final stream is printed at
the terminal:

@lisp
(define input-prompt ";;; Query input:")
(define output-prompt ";;; Query results:")

(define (query-driver-loop)
  (prompt-for-input input-prompt)
  (let ((q (query-syntax-process (read))))
    (cond ((assertion-to-be-added? q)
           (add-rule-or-assertion! (add-assertion-body q))
           (newline)
           (display "Assertion added to data base.")
           (query-driver-loop))
          (else
           (newline)
           (display output-prompt)
           (display-stream
            (stream-map
             (lambda (frame)
               (instantiate q
                            frame
                            (lambda (v f)
                              (contract-question-mark v))))
             (qeval q (singleton-stream '()))))
           (query-driver-loop)))))
@end lisp

Here, as in the other evaluators in this chapter, we use an abstract syntax for
the expressions of the query language.  The implementation of the expression
syntax, including the predicate @code{assertion-to-be-added?} and the selector
@code{add-assertion-body}, is given in section @ref{4-4-4-7}.
@code{Add-rule-or-assertion!} is defined in section @ref{4-4-4-5}.

Before doing any processing on an input expression, the driver loop transforms
it syntactically into a form that makes the processing more efficient.  This
involves changing the representation of pattern variables.  When the query is
instantiated, any variables that remain unbound are transformed back to the
input representation before being printed.  These transformations are performed
by the two procedures @code{query-syntax-process} and
@code{contract-question-mark} (section @ref{4-4-4-7}).

To instantiate an expression, we copy it, replacing any variables in the
expression by their values in a given frame.  The values are themselves
instantiated, since they could contain variables (for example, if @code{?x} in
@code{exp} is bound to @code{?y} as the result of unification and @code{?y} is
in turn bound to 5).  The action to take if a variable cannot be instantiated
is given by a procedural argument to @code{instantiate}.

@lisp
(define (instantiate exp frame unbound-var-handler)
  (define (copy exp)
    (cond ((var? exp)
           (let ((binding (binding-in-frame exp frame)))
             (if binding
                 (copy (binding-value binding))
                 (unbound-var-handler exp frame))))
          ((pair? exp)
           (cons (copy (car exp)) (copy (cdr exp))))
          (else exp)))
  (copy exp))
@end lisp

The procedures that manipulate bindings are defined in section @ref{4-4-4-8}.

@node 4-4-4-2, 4-4-4-3, 4-4-4-1, 4-4-4
@subsubsection The Evaluator

The @code{qeval} procedure, called by the @code{query-driver-loop}, is the
basic evaluator of the query system.  It takes as inputs a query and a stream
of frames, and it returns a stream of extended frames.  It identifies special
forms by a data-directed dispatch using @code{get} and @code{put}, just as we
did in implementing generic operations in @ref{Chapter 2}.  Any query that is
not identified as a special form is assumed to be a simple query, to be
processed by @code{simple-query}.

@lisp
(define (qeval query frame-stream)
  (let ((qproc (get (type query) 'qeval)))
    (if qproc
        (qproc (contents query) frame-stream)
        (simple-query query frame-stream))))
@end lisp

@code{Type} and @code{contents}, defined in section @ref{4-4-4-7}, implement
the abstract syntax of the special forms.

@subsubheading Simple queries

The @code{simple-query} procedure handles simple queries.  It takes as
arguments a simple query (a pattern) together with a stream of frames, and it
returns the stream formed by extending each frame by all data-base matches of
the query.

@lisp
(define (simple-query query-pattern frame-stream)
  (stream-flatmap
   (lambda (frame)
     (stream-append-delayed
      (find-assertions query-pattern frame)
      (delay (apply-rules query-pattern frame))))
   frame-stream))
@end lisp

For each frame in the input stream, we use @code{find-assertions} (section
@ref{4-4-4-3}) to match the pattern against all assertions in the data base,
producing a stream of extended frames, and we use @code{apply-rules} (section
@ref{4-4-4-4}) to apply all possible rules, producing another stream of
extended frames.  These two streams are combined (using
@code{stream-append-delayed}, section @ref{4-4-4-6}) to make a stream of all
the ways that the given pattern can be satisfied consistent with the original
frame (see @ref{Exercise 4-71}).  The streams for the individual input frames
are combined using @code{stream-flatmap} (section @ref{4-4-4-6}) to form one
large stream of all the ways that any of the frames in the original input
stream can be extended to produce a match with the given pattern.

@subsubheading Compound queries

@code{And} queries are handled as illustrated in @ref{Figure 4-5} by the
@code{conjoin} procedure.  @code{Conjoin} takes as inputs the conjuncts and the
frame stream and returns the stream of extended frames.  First, @code{conjoin}
processes the stream of frames to find the stream of all possible frame
extensions that satisfy the first query in the conjunction.  Then, using this
as the new frame stream, it recursively applies @code{conjoin} to the rest of
the queries.

@lisp
(define (conjoin conjuncts frame-stream)
  (if (empty-conjunction? conjuncts)
      frame-stream
      (conjoin (rest-conjuncts conjuncts)
               (qeval (first-conjunct conjuncts)
                      frame-stream))))
@end lisp

The expression

@lisp
(put 'and 'qeval conjoin)
@end lisp

@noindent
sets up @code{qeval} to dispatch to @code{conjoin} when an @code{and} form is
encountered.

@code{Or} queries are handled similarly, as shown in @ref{Figure 4-6}.  The
output streams for the various disjuncts of the @code{or} are computed
separately and merged using the @code{interleave-delayed} procedure from
section @ref{4-4-4-6}.  (See @ref{Exercise 4-71} and @ref{Exercise 4-72}.)

@lisp
(define (disjoin disjuncts frame-stream)
  (if (empty-disjunction? disjuncts)
      the-empty-stream
      (interleave-delayed
       (qeval (first-disjunct disjuncts) frame-stream)
       (delay (disjoin (rest-disjuncts disjuncts)
                       frame-stream)))))

(put 'or 'qeval disjoin)
@end lisp

The predicates and selectors for the syntax of conjuncts and disjuncts are
given in section @ref{4-4-4-7}.

@subsubheading Filters

@code{Not} is handled by the method outlined in section @ref{4-4-2}.  We
attempt to extend each frame in the input stream to satisfy the query being
negated, and we include a given frame in the output stream only if it cannot be
extended.

@lisp
(define (negate operands frame-stream)
  (stream-flatmap
   (lambda (frame)
     (if (stream-null? (qeval (negated-query operands)
                              (singleton-stream frame)))
         (singleton-stream frame)
         the-empty-stream))
   frame-stream))

(put 'not 'qeval negate)
@end lisp

@code{Lisp-value} is a filter similar to @code{not}.  Each frame in the stream
is used to instantiate the variables in the pattern, the indicated predicate is
applied, and the frames for which the predicate returns false are filtered out
of the input stream.  An error results if there are unbound pattern variables.

@lisp
(define (lisp-value call frame-stream)
  (stream-flatmap
   (lambda (frame)
     (if (execute
          (instantiate
           call
           frame
           (lambda (v f)
             (error "Unknown pat var -- LISP-VALUE" v))))
         (singleton-stream frame)
         the-empty-stream))
   frame-stream))

(put 'lisp-value 'qeval lisp-value)
@end lisp

@code{Execute}, which applies the predicate to the arguments, must @code{eval}
the predicate expression to get the procedure to apply.  However, it must not
evaluate the arguments, since they are already the actual arguments, not
expressions whose evaluation (in Lisp) will produce the arguments.  Note that
@code{execute} is implemented using @code{eval} and @code{apply} from the
underlying Lisp system.

@lisp
(define (execute exp)
  (apply (eval (predicate exp) user-initial-environment)
         (args exp)))
@end lisp

The @code{always-true} special form provides for a query that is always
satisfied.  It ignores its contents (normally empty) and simply passes through
all the frames in the input stream.  @code{Always-true} is used by the
@code{rule-body} selector (section @ref{4-4-4-7}) to provide bodies for rules
that were defined without bodies (that is, rules whose conclusions are always
satisfied).

@lisp
(define (always-true ignore frame-stream) frame-stream)

(put 'always-true 'qeval always-true)
@end lisp

The selectors that define the syntax of @code{not} and @code{lisp-value} are
given in section @ref{4-4-4-7}.

@node 4-4-4-3, 4-4-4-4, 4-4-4-2, 4-4-4
@subsubsection Finding Assertions by Pattern Matching

@code{Find-assertions}, called by @code{simple-query} (section @ref{4-4-4-2}),
takes as input a pattern and a frame.  It returns a stream of frames, each
extending the given one by a data-base match of the given pattern.  It uses
@code{fetch-assertions} (section @ref{4-4-4-5}) to get a stream of all the
assertions in the data base that should be checked for a match against the
pattern and the frame.  The reason for @code{fetch-assertions} here is that we
can often apply simple tests that will eliminate many of the entries in the
data base from the pool of candidates for a successful match.  The system would
still work if we eliminated @code{fetch-assertions} and simply checked a stream
of all assertions in the data base, but the computation would be less efficient
because we would need to make many more calls to the matcher.

@lisp
(define (find-assertions pattern frame)
  (stream-flatmap (lambda (datum)
                    (check-an-assertion datum pattern frame))
                  (fetch-assertions pattern frame)))
@end lisp

@code{Check-an-assertion} takes as arguments a pattern, a data object
(assertion), and a frame and returns either a one-element stream containing the
extended frame or @code{the-empty-stream} if the match fails.

@lisp
(define (check-an-assertion assertion query-pat query-frame)
  (let ((match-result
         (pattern-match query-pat assertion query-frame)))
    (if (eq? match-result 'failed)
        the-empty-stream
        (singleton-stream match-result))))
@end lisp

The basic pattern matcher returns either the symbol @code{failed} or an
extension of the given frame.  The basic idea of the matcher is to check the
pattern against the data, element by element, accumulating bindings for the
pattern variables.  If the pattern and the data object are the same, the match
succeeds and we return the frame of bindings accumulated so far.  Otherwise, if
the pattern is a variable we extend the current frame by binding the variable
to the data, so long as this is consistent with the bindings already in the
frame.  If the pattern and the data are both pairs, we (recursively) match the
@code{car} of the pattern against the @code{car} of the data to produce a
frame; in this frame we then match the @code{cdr} of the pattern against the
@code{cdr} of the data.  If none of these cases are applicable, the match fails
and we return the symbol @code{failed}.

@lisp
(define (pattern-match pat dat frame)
  (cond ((eq? frame 'failed) 'failed)
        ((equal? pat dat) frame)
        ((var? pat) (extend-if-consistent pat dat frame))
        ((and (pair? pat) (pair? dat))
         (pattern-match (cdr pat)
                        (cdr dat)
                        (pattern-match (car pat)
                                       (car dat)
                                       frame)))
        (else 'failed)))
@end lisp

Here is the procedure that extends a frame by adding a new binding, if this is
consistent with the bindings already in the frame:

@lisp
(define (extend-if-consistent var dat frame)
  (let ((binding (binding-in-frame var frame)))
    (if binding
        (pattern-match (binding-value binding) dat frame)
        (extend var dat frame))))
@end lisp

If there is no binding for the variable in the frame, we simply add the binding
of the variable to the data.  Otherwise we match, in the frame, the data
against the value of the variable in the frame.  If the stored value contains
only constants, as it must if it was stored during pattern matching by
@code{extend-if-consistent}, then the match simply tests whether the stored and
new values are the same.  If so, it returns the unmodified frame; if not, it
returns a failure indication.  The stored value may, however, contain pattern
variables if it was stored during unification (see section @ref{4-4-4-4}).  The
recursive match of the stored pattern against the new data will add or check
bindings for the variables in this pattern.  For example, suppose we have a
frame in which @code{?x} is bound to @code{(f ?y)} and @code{?y} is unbound,
and we wish to augment this frame by a binding of @code{?x} to @code{(f b)}.
We look up @code{?x} and find that it is bound to @code{(f ?y)}.  This leads us
to match @code{(f ?y)} against the proposed new value @code{(f b)} in the same
frame.  Eventually this match extends the frame by adding a binding of
@code{?y} to @code{b}.  @code{?X} remains bound to @code{(f ?y)}.  We never
modify a stored binding and we never store more than one binding for a given
variable.

The procedures used by @code{extend-if-consistent} to manipulate bindings are
defined in section @ref{4-4-4-8}.

@subsubheading Patterns with dotted tails

If a pattern contains a dot followed by a pattern variable, the pattern
variable matches the rest of the data list (rather than the next element of the
data list), just as one would expect with the dotted-tail notation described in
@ref{Exercise 2-20}.  Although the pattern matcher we have just implemented
doesn't look for dots, it does behave as we want.  This is because the Lisp
@code{read} primitive, which is used by @code{query-driver-loop} to read the
query and represent it as a list structure, treats dots in a special way.

When @code{read} sees a dot, instead of making the next item be the next
element of a list (the @code{car} of a @code{cons} whose @code{cdr} will be the
rest of the list) it makes the next item be the @code{cdr} of the list
structure.  For example, the list structure produced by @code{read} for the
pattern @code{(computer ?type)} could be constructed by evaluating the
expression @code{(cons 'computer (cons '?type '()))}, and that for
@code{(computer . ?type)} could be constructed by evaluating the expression
@code{(cons 'computer '?type)}.

Thus, as @code{pattern-match} recursively compares @code{car}s and @code{cdr}s
of a data list and a pattern that had a dot, it eventually matches the variable
after the dot (which is a @code{cdr} of the pattern) against a sublist of the
data list, binding the variable to that list.  For example, matching the
pattern @code{(computer . ?type)} against @code{(computer programmer trainee)}
will match @code{?type} against the list @code{(programmer trainee)}.

@node 4-4-4-4, 4-4-4-5, 4-4-4-3, 4-4-4
@subsubsection Rules and Unification

@code{Apply-rules} is the rule analog of @code{find-assertions} (section
@ref{4-4-4-3}).  It takes as input a pattern and a frame, and it forms a stream
of extension frames by applying rules from the data base.
@code{Stream-flatmap} maps @code{apply-a-rule} down the stream of possibly
applicable rules (selected by @code{fetch-rules}, section @ref{4-4-4-5}) and
combines the resulting streams of frames.

@lisp
(define (apply-rules pattern frame)
  (stream-flatmap (lambda (rule)
                    (apply-a-rule rule pattern frame))
                  (fetch-rules pattern frame)))
@end lisp

@code{Apply-a-rule} applies rules using the method outlined in section
@ref{4-4-2}.  It first augments its argument frame by unifying the rule
conclusion with the pattern in the given frame.  If this succeeds, it evaluates
the rule body in this new frame.

Before any of this happens, however, the program renames all the variables in
the rule with unique new names.  The reason for this is to prevent the
variables for different rule applications from becoming confused with each
other.  For instance, if two rules both use a variable named @code{?x}, then
each one may add a binding for @code{?x} to the frame when it is applied.
These two @code{?x}'s have nothing to do with each other, and we should not be
fooled into thinking that the two bindings must be consistent.  Rather than
rename variables, we could devise a more clever environment structure; however,
the renaming approach we have chosen here is the most straightforward, even if
not the most efficient.  (See @ref{Exercise 4-79}.)  Here is the
@code{apply-a-rule} procedure:

@lisp
(define (apply-a-rule rule query-pattern query-frame)
  (let ((clean-rule (rename-variables-in rule)))
    (let ((unify-result
           (unify-match query-pattern
                        (conclusion clean-rule)
                        query-frame)))
      (if (eq? unify-result 'failed)
          the-empty-stream
          (qeval (rule-body clean-rule)
                 (singleton-stream unify-result))))))
@end lisp

The selectors @code{rule-body} and @code{conclusion} that extract parts of a
rule are defined in section @ref{4-4-4-7}.

We generate unique variable names by associating a unique identifier (such as a
number) with each rule application and combining this identifier with the
original variable names.  For example, if the rule-application identifier is 7,
we might change each @code{?x} in the rule to @code{?x-7} and each @code{?y} in
the rule to @code{?y-7}.  (@code{Make-new-variable} and
@code{new-rule-application-id} are included with the syntax procedures in
section @ref{4-4-4-7}.)

@lisp
(define (rename-variables-in rule)
  (let ((rule-application-id (new-rule-application-id)))
    (define (tree-walk exp)
      (cond ((var? exp)
             (make-new-variable exp rule-application-id))
            ((pair? exp)
             (cons (tree-walk (car exp))
                   (tree-walk (cdr exp))))
            (else exp)))
    (tree-walk rule)))
@end lisp

The unification algorithm is implemented as a procedure that takes as inputs
two patterns and a frame and returns either the extended frame or the symbol
@code{failed}.  The unifier is like the pattern matcher except that it is
symmetrical---variables are allowed on both sides of the match.
@code{Unify-match} is basically the same as @code{pattern-match}, except that
there is extra code (marked ``@code{***}'' below) to handle the case where the
object on the right side of the match is a variable.

@lisp
(define (unify-match p1 p2 frame)
  (cond ((eq? frame 'failed) 'failed)
        ((equal? p1 p2) frame)
        ((var? p1) (extend-if-possible p1 p2 frame))
        ((var? p2) (extend-if-possible p2 p1 frame))  @r{; ***}
        ((and (pair? p1) (pair? p2))
         (unify-match (cdr p1)
                      (cdr p2)
                      (unify-match (car p1)
                                   (car p2)
                                   frame)))
        (else 'failed)))
@end lisp

In unification, as in one-sided pattern matching, we want to accept a proposed
extension of the frame only if it is consistent with existing bindings.  The
procedure @code{extend-if-possible} used in unification is the same as the
@code{extend-if-consistent} used in pattern matching except for two special
checks, marked ``@code{***}'' in the program below.  In the first case, if the
variable we are trying to match is not bound, but the value we are trying to
match it with is itself a (different) variable, it is necessary to check to see
if the value is bound, and if so, to match its value.  If both parties to the
match are unbound, we may bind either to the other.

The second check deals with attempts to bind a variable to a pattern that
includes that variable.  Such a situation can occur whenever a variable is
repeated in both patterns.  Consider, for example, unifying the two patterns
@code{(?x ?x)} and @code{(?y <@var{expression involving @code{?y}}>)} in a
frame where both @code{?x} and @code{?y} are unbound.  First @code{?x} is
matched against @code{?y}, making a binding of @code{?x} to @code{?y}.  Next,
the same @code{?x} is matched against the given expression involving @code{?y}.
Since @code{?x} is already bound to @code{?y}, this results in matching
@code{?y} against the expression.  If we think of the unifier as finding a set
of values for the pattern variables that make the patterns the same, then these
patterns imply instructions to find a @code{?y} such that @code{?y} is equal to
the expression involving @code{?y}.  There is no general method for solving
such equations, so we reject such bindings; these cases are recognized by the
predicate @code{depends-on?}.@footnote{In general, unifying @code{?y} with an
expression involving @code{?y} would require our being able to find a fixed
point of the equation @code{?y} = <@var{expression involving @code{?y}}>.  It
is sometimes possible to syntactically form an expression that appears to be
the solution.  For example, @code{?y} = @code{(f ?y)} seems to have the fixed
point @code{(f (f (f @dots{} )))}, which we can produce by beginning with the
expression @code{(f ?y)} and repeatedly substituting @code{(f ?y)} for
@code{?y}.  Unfortunately, not every such equation has a meaningful fixed
point.  The issues that arise here are similar to the issues of manipulating
infinite series in mathematics.  For example, we know that 2 is the solution to
the equation @i{y} = 1 + @i{y}/2.  Beginning with the expression 1 + @i{y}/2
and repeatedly substituting 1 + @i{y}/2 for @i{y} gives

@example
2 = y = 1 + y/2 = 1 + (1 + y/2)/2 = 1 + 1/2 + y/4 = ...
@end example

@noindent
which leads to

@example
2 = 1 + 1/2 + 1/4 + 1/8 + ...
@end example

@noindent
However, if we try the same manipulation beginning with the observation that -
1 is the solution to the equation @i{y} = 1 + 2@i{y}, we obtain

@example
-1 = y = 1 + 2y = 1 + 2(1 + 2y) = 1 + 2 + 4y = ...
@end example

@noindent
which leads to

@example
-1 = 1 + 2 + 4 + 8 + ...
@end example

@noindent
Although the formal manipulations used in deriving these two equations are
identical, the first result is a valid assertion about infinite series but the
second is not.  Similarly, for our unification results, reasoning with an
arbitrary syntactically constructed expression may lead to errors.}  On the
other hand, we do not want to reject attempts to bind a variable to itself.
For example, consider unifying @code{(?x ?x)} and @code{(?y ?y)}.  The second
attempt to bind @code{?x} to @code{?y} matches @code{?y} (the stored value of
@code{?x}) against @code{?y} (the new value of @code{?x}).  This is taken care
of by the @code{equal?}  clause of @code{unify-match}.

@lisp
(define (extend-if-possible var val frame)
  (let ((binding (binding-in-frame var frame)))
    (cond (binding
           (unify-match
            (binding-value binding) val frame))
          ((var? val)                      @r{; ***}
           (let ((binding (binding-in-frame val frame)))
             (if binding
                 (unify-match
                  var (binding-value binding) frame)
                 (extend var val frame))))
          ((depends-on? val var frame)     @r{; ***}
           'failed)
          (else (extend var val frame)))))
@end lisp

@code{Depends-on?} is a predicate that tests whether an expression proposed to
be the value of a pattern variable depends on the variable.  This must be done
relative to the current frame because the expression may contain occurrences of
a variable that already has a value that depends on our test variable.  The
structure of @code{depends-on?} is a simple recursive tree walk in which we
substitute for the values of variables whenever necessary.

@lisp
(define (depends-on? exp var frame)
  (define (tree-walk e)
    (cond ((var? e)
           (if (equal? var e)
               true
               (let ((b (binding-in-frame e frame)))
                 (if b
                     (tree-walk (binding-value b))
                     false))))
          ((pair? e)
           (or (tree-walk (car e))
               (tree-walk (cdr e))))
          (else false)))
  (tree-walk exp))
@end lisp

@node 4-4-4-5, 4-4-4-6, 4-4-4-4, 4-4-4
@subsubsection Maintaining the Data Base

One important problem in designing logic programming languages is that of
arranging things so that as few irrelevant data-base entries as possible will
be examined in checking a given pattern.  In our system, in addition to storing
all assertions in one big stream, we store all assertions whose @code{car}s are
constant symbols in separate streams, in a table indexed by the symbol.  To
fetch an assertion that may match a pattern, we first check to see if the
@code{car} of the pattern is a constant symbol.  If so, we return (to be tested
using the matcher) all the stored assertions that have the same @code{car}.  If
the pattern's @code{car} is not a constant symbol, we return all the stored
assertions.  Cleverer methods could also take advantage of information in the
frame, or try also to optimize the case where the @code{car} of the pattern is
not a constant symbol.  We avoid building our criteria for indexing (using the
@code{car}, handling only the case of constant symbols) into the program;
instead we call on predicates and selectors that embody our criteria.

@lisp
(define THE-ASSERTIONS the-empty-stream)

(define (fetch-assertions pattern frame)
  (if (use-index? pattern)
      (get-indexed-assertions pattern)
      (get-all-assertions)))

(define (get-all-assertions) THE-ASSERTIONS)

(define (get-indexed-assertions pattern)
  (get-stream (index-key-of pattern) 'assertion-stream))
@end lisp

@code{Get-stream} looks up a stream in the table and returns an empty stream if
nothing is stored there.

@lisp
(define (get-stream key1 key2)
  (let ((s (get key1 key2)))
    (if s s the-empty-stream)))
@end lisp

Rules are stored similarly, using the @code{car} of the rule conclusion.  Rule
conclusions are arbitrary patterns, however, so they differ from assertions in
that they can contain variables.  A pattern whose @code{car} is a constant
symbol can match rules whose conclusions start with a variable as well as rules
whose conclusions have the same @code{car}.  Thus, when fetching rules that
might match a pattern whose @code{car} is a constant symbol we fetch all rules
whose conclusions start with a variable as well as those whose conclusions have
the same @code{car} as the pattern.  For this purpose we store all rules whose
conclusions start with a variable in a separate stream in our table, indexed by
the symbol @code{?}.

@lisp
(define THE-RULES the-empty-stream)

(define (fetch-rules pattern frame)
  (if (use-index? pattern)
      (get-indexed-rules pattern)
      (get-all-rules)))

(define (get-all-rules) THE-RULES)

(define (get-indexed-rules pattern)
  (stream-append
   (get-stream (index-key-of pattern) 'rule-stream)
   (get-stream '? 'rule-stream)))
@end lisp

@code{Add-rule-or-assertion!} is used by @code{query-driver-loop} to add
assertions and rules to the data base.  Each item is stored in the index, if
appropriate, and in a stream of all assertions or rules in the data base.

@lisp
(define (add-rule-or-assertion! assertion)
  (if (rule? assertion)
      (add-rule! assertion)
      (add-assertion! assertion)))

(define (add-assertion! assertion)
  (store-assertion-in-index assertion)
  (let ((old-assertions THE-ASSERTIONS))
    (set! THE-ASSERTIONS
          (cons-stream assertion old-assertions))
    'ok))

(define (add-rule! rule)
  (store-rule-in-index rule)
  (let ((old-rules THE-RULES))
    (set! THE-RULES (cons-stream rule old-rules))
    'ok))
@end lisp

To actually store an assertion or a rule, we check to see if it can be indexed.
If so, we store it in the appropriate stream.

@lisp
(define (store-assertion-in-index assertion)
  (if (indexable? assertion)
      (let ((key (index-key-of assertion)))
        (let ((current-assertion-stream
               (get-stream key 'assertion-stream)))
          (put key
               'assertion-stream
               (cons-stream assertion
                            current-assertion-stream))))))

(define (store-rule-in-index rule)
  (let ((pattern (conclusion rule)))
    (if (indexable? pattern)
        (let ((key (index-key-of pattern)))
          (let ((current-rule-stream
                 (get-stream key 'rule-stream)))
            (put key
                 'rule-stream
                 (cons-stream rule
                              current-rule-stream)))))))
@end lisp

The following procedures define how the data-base index is used.  A pattern (an
assertion or a rule conclusion) will be stored in the table if it starts with a
variable or a constant symbol.

@lisp
(define (indexable? pat)
  (or (constant-symbol? (car pat))
      (var? (car pat))))
@end lisp

The key under which a pattern is stored in the table is either @code{?} (if it
starts with a variable) or the constant symbol with which it starts.

@lisp
(define (index-key-of pat)
  (let ((key (car pat)))
    (if (var? key) '? key)))
@end lisp

The index will be used to retrieve items that might match a pattern if the
pattern starts with a constant symbol.

@lisp
(define (use-index? pat)
  (constant-symbol? (car pat)))
@end lisp

@quotation
@strong{@anchor{Exercise 4-70}Exercise 4.70:} What is the purpose of the
@code{let} bindings in the procedures @code{add-assertion!} and
@code{add-rule!} ?  What would be wrong with the following implementation of
@code{add-assertion!} ?  Hint: Recall the definition of the infinite stream of
ones in section @ref{3-5-2}: @code{(define ones (cons-stream 1 ones))}.

@lisp
(define (add-assertion! assertion)
  (store-assertion-in-index assertion)
  (set! THE-ASSERTIONS
        (cons-stream assertion THE-ASSERTIONS))
  'ok)
@end lisp
@end quotation

@node 4-4-4-6, 4-4-4-7, 4-4-4-5, 4-4-4
@subsubsection Stream Operations

The query system uses a few stream operations that were not presented in
@ref{Chapter 3}.

@code{Stream-append-delayed} and @code{interleave-delayed} are just like
@code{stream-append} and @code{interleave} (section @ref{3-5-3}), except that
they take a delayed argument (like the @code{integral} procedure in section
@ref{3-5-4}).  This postpones looping in some cases (see @ref{Exercise 4-71}).

@lisp
(define (stream-append-delayed s1 delayed-s2)
  (if (stream-null? s1)
      (force delayed-s2)
      (cons-stream
       (stream-car s1)
       (stream-append-delayed (stream-cdr s1) delayed-s2))))

(define (interleave-delayed s1 delayed-s2)
  (if (stream-null? s1)
      (force delayed-s2)
      (cons-stream
       (stream-car s1)
       (interleave-delayed (force delayed-s2)
                           (delay (stream-cdr s1))))))
@end lisp

@code{Stream-flatmap}, which is used throughout the query evaluator to map a
procedure over a stream of frames and combine the resulting streams of frames,
is the stream analog of the @code{flatmap} procedure introduced for ordinary
lists in section @ref{2-2-3}.  Unlike ordinary @code{flatmap}, however, we
accumulate the streams with an interleaving process, rather than simply
appending them (see @ref{Exercise 4-72} and @ref{Exercise 4-73}).

@lisp
(define (stream-flatmap proc s)
  (flatten-stream (stream-map proc s)))

(define (flatten-stream stream)
  (if (stream-null? stream)
      the-empty-stream
      (interleave-delayed
       (stream-car stream)
       (delay (flatten-stream (stream-cdr stream))))))
@end lisp

The evaluator also uses the following simple procedure to generate a stream
consisting of a single element:

@lisp
(define (singleton-stream x)
  (cons-stream x the-empty-stream))
@end lisp

@node 4-4-4-7, 4-4-4-8, 4-4-4-6, 4-4-4
@subsubsection Query Syntax Procedures

@code{Type} and @code{contents}, used by @code{qeval} (section @ref{4-4-4-2}),
specify that a special form is identified by the symbol in its @code{car}.
They are the same as the @code{type-tag} and @code{contents} procedures in
section @ref{2-4-2}, except for the error message.

@lisp
(define (type exp)
  (if (pair? exp)
      (car exp)
      (error "Unknown expression TYPE" exp)))

(define (contents exp)
  (if (pair? exp)
      (cdr exp)
      (error "Unknown expression CONTENTS" exp)))
@end lisp

The following procedures, used by @code{query-driver-loop} (in section
@ref{4-4-4-1}), specify that rules and assertions are added to the data base by
expressions of the form @code{(assert! <@var{rule-or-assertion}>)}:

@lisp
(define (assertion-to-be-added? exp)
  (eq? (type exp) 'assert!))

(define (add-assertion-body exp)
  (car (contents exp)))
@end lisp

Here are the syntax definitions for the @code{and}, @code{or}, @code{not}, and
@code{lisp-value} special forms (section @ref{4-4-4-2}):

@lisp
(define (empty-conjunction? exps) (null? exps))
(define (first-conjunct exps) (car exps))
(define (rest-conjuncts exps) (cdr exps))

(define (empty-disjunction? exps) (null? exps))
(define (first-disjunct exps) (car exps))
(define (rest-disjuncts exps) (cdr exps))

(define (negated-query exps) (car exps))

(define (predicate exps) (car exps))
(define (args exps) (cdr exps))
@end lisp

The following three procedures define the syntax of rules:

@lisp
(define (rule? statement)
  (tagged-list? statement 'rule))

(define (conclusion rule) (cadr rule))

(define (rule-body rule)
  (if (null? (cddr rule))
      '(always-true)
      (caddr rule)))
@end lisp

@code{Query-driver-loop} (section @ref{4-4-4-1}) calls
@code{query-syntax-process} to transform pattern variables in the expression,
which have the form @code{?symbol}, into the internal format @code{(? symbol)}.
That is to say, a pattern such as @code{(job ?x ?y)} is actually represented
internally by the system as @code{(job (? x) (? y))}.  This increases the
efficiency of query processing, since it means that the system can check to see
if an expression is a pattern variable by checking whether the @code{car} of
the expression is the symbol @code{?}, rather than having to extract characters
from the symbol.  The syntax transformation is accomplished by the following
procedure:@footnote{Most Lisp systems give the user the ability to modify the
ordinary @code{read} procedure to perform such transformations by defining
@newterm{reader macro characters}.  Quoted expressions are already handled in
this way: The reader automatically translates @code{'expression} into
@code{(quote expression)} before the evaluator sees it.  We could arrange for
@code{?expression} to be transformed into @code{(? expression)} in the same
way; however, for the sake of clarity we have included the transformation
procedure here explicitly.

@code{Expand-question-mark} and @code{contract-question-mark} use several
procedures with @code{string} in their names.  These are Scheme primitives.}

@lisp
(define (query-syntax-process exp)
  (map-over-symbols expand-question-mark exp))

(define (map-over-symbols proc exp)
  (cond ((pair? exp)
         (cons (map-over-symbols proc (car exp))
               (map-over-symbols proc (cdr exp))))
        ((symbol? exp) (proc exp))
        (else exp)))

(define (expand-question-mark symbol)
  (let ((chars (symbol->string symbol)))
    (if (string=? (substring chars 0 1) "?")
        (list '?
              (string->symbol
               (substring chars 1 (string-length chars))))
        symbol)))
@end lisp

Once the variables are transformed in this way, the variables in a pattern are
lists starting with @code{?}, and the constant symbols (which need to be
recognized for data-base indexing, section @ref{4-4-4-5}) are just the symbols.

@lisp
(define (var? exp)
  (tagged-list? exp '?))

(define (constant-symbol? exp) (symbol? exp))
@end lisp

Unique variables are constructed during rule application (in section
@ref{4-4-4-4}) by means of the following procedures.  The unique identifier for
a rule application is a number, which is incremented each time a rule is
applied.

@lisp
(define rule-counter 0)

(define (new-rule-application-id)
  (set! rule-counter (+ 1 rule-counter))
  rule-counter)

(define (make-new-variable var rule-application-id)
  (cons '? (cons rule-application-id (cdr var))))
@end lisp

When @code{query-driver-loop} instantiates the query to print the answer, it
converts any unbound pattern variables back to the right form for printing,
using

@lisp
(define (contract-question-mark variable)
  (string->symbol
   (string-append "?"
     (if (number? (cadr variable))
         (string-append (symbol->string (caddr variable))
                        "-"
                        (number->string (cadr variable)))
         (symbol->string (cadr variable))))))
@end lisp

@node 4-4-4-8,  , 4-4-4-7, 4-4-4
@subsubsection Frames and Bindings

Frames are represented as lists of bindings, which are variable-value pairs:

@lisp
(define (make-binding variable value)
  (cons variable value))

(define (binding-variable binding)
  (car binding))

(define (binding-value binding)
  (cdr binding))

(define (binding-in-frame variable frame)
  (assoc variable frame))

(define (extend variable value frame)
  (cons (make-binding variable value) frame))
@end lisp

@quotation
@strong{@anchor{Exercise 4-71}Exercise 4.71:} Louis Reasoner wonders why the
@code{simple-query} and @code{disjoin} procedures (section @ref{4-4-4-2}) are
implemented using explicit @code{delay} operations, rather than being defined
as follows:

@lisp
(define (simple-query query-pattern frame-stream)
  (stream-flatmap
   (lambda (frame)
     (stream-append (find-assertions query-pattern frame)
                    (apply-rules query-pattern frame)))
   frame-stream))

(define (disjoin disjuncts frame-stream)
  (if (empty-disjunction? disjuncts)
      the-empty-stream
      (interleave
       (qeval (first-disjunct disjuncts) frame-stream)
       (disjoin (rest-disjuncts disjuncts) frame-stream))))
@end lisp

Can you give examples of queries where these simpler definitions would lead to
undesirable behavior?
@end quotation

@quotation
@strong{@anchor{Exercise 4-72}Exercise 4.72:} Why do @code{disjoin} and
@code{stream-flatmap} interleave the streams rather than simply append them?
Give examples that illustrate why interleaving works better.  (Hint: Why did we
use @code{interleave} in section @ref{3-5-3}?)
@end quotation

@quotation
@strong{@anchor{Exercise 4-73}Exercise 4.73:} Why does @code{flatten-stream}
use @code{delay} explicitly?  What would be wrong with defining it as follows:

@lisp
(define (flatten-stream stream)
  (if (stream-null? stream)
      the-empty-stream
      (interleave
       (stream-car stream)
       (flatten-stream (stream-cdr stream)))))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 4-74}Exercise 4.74:} Alyssa P. Hacker proposes to use
a simpler version of @code{stream-flatmap} in @code{negate}, @code{lisp-value},
and @code{find-assertions}.  She observes that the procedure that is mapped
over the frame stream in these cases always produces either the empty stream or
a singleton stream, so no interleaving is needed when combining these streams.

@enumerate a

@item
Fill in the missing expressions in Alyssa's program.

@lisp
(define (simple-stream-flatmap proc s)
  (simple-flatten (stream-map proc s)))

(define (simple-flatten stream)
  (stream-map <@var{??}>
              (stream-filter <@var{??}> stream)))
@end lisp

@item
Does the query system's behavior change if we change it in this way?

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 4-75}Exercise 4.75:} Implement for the query language
a new special form called @code{unique}.  @code{Unique} should succeed if there
is precisely one item in the data base satisfying a specified query.  For
example,

@lisp
(unique (job ?x (computer wizard)))
@end lisp

@noindent
should print the one-item stream

@lisp
(unique (job (Bitdiddle Ben) (computer wizard)))
@end lisp

@noindent
since Ben is the only computer wizard, and

@lisp
(unique (job ?x (computer programmer)))
@end lisp

@noindent
should print the empty stream, since there is more than one computer
programmer.  Moreover,

@lisp
(and (job ?x ?j) (unique (job ?anyone ?j)))
@end lisp

@noindent
should list all the jobs that are filled by only one person, and the people who
fill them.

There are two parts to implementing @code{unique}.  The first is to write a
procedure that handles this special form, and the second is to make
@code{qeval} dispatch to that procedure.  The second part is trivial, since
@code{qeval} does its dispatching in a data-directed way.  If your procedure is
called @code{uniquely-asserted}, all you need to do is

@lisp
(put 'unique 'qeval uniquely-asserted)
@end lisp

@noindent
and @code{qeval} will dispatch to this procedure for every query whose
@code{type} (@code{car}) is the symbol @code{unique}.

The real problem is to write the procedure @code{uniquely-asserted}.  This
should take as input the @code{contents} (@code{cdr}) of the @code{unique}
query, together with a stream of frames.  For each frame in the stream, it
should use @code{qeval} to find the stream of all extensions to the frame that
satisfy the given query.  Any stream that does not have exactly one item in it
should be eliminated.  The remaining streams should be passed back to be
accumulated into one big stream that is the result of the @code{unique} query.
This is similar to the implementation of the @code{not} special form.

Test your implementation by forming a query that lists all people who supervise
precisely one person.
@end quotation

@quotation
@strong{@anchor{Exercise 4-76}Exercise 4.76:} Our implementation of @code{and}
as a series combination of queries (@ref{Figure 4-5}) is elegant, but it is
inefficient because in processing the second query of the @code{and} we must
scan the data base for each frame produced by the first query.  If the data
base has @i{n} elements, and a typical query produces a number of output frames
proportional to @i{n} (say @i{n}/@i{k}), then scanning the data base for each
frame produced by the first query will require @i{n}^2/@i{k} calls to the
pattern matcher.  Another approach would be to process the two clauses of the
@code{and} separately, then look for all pairs of output frames that are
compatible.  If each query produces @i{n}/@i{k} output frames, then this means
that we must perform @i{n}^2/@i{k}^2 compatibility checks---a factor of @i{k}
fewer than the number of matches required in our current method.

Devise an implementation of @code{and} that uses this strategy.  You must
implement a procedure that takes two frames as inputs, checks whether the
bindings in the frames are compatible, and, if so, produces a frame that merges
the two sets of bindings.  This operation is similar to unification.
@end quotation

@quotation
@strong{@anchor{Exercise 4-77}Exercise 4.77:} In section @ref{4-4-3} we saw
that @code{not} and @code{lisp-value} can cause the query language to give
``wrong'' answers if these filtering operations are applied to frames in which
variables are unbound.  Devise a way to fix this shortcoming.  One idea is to
perform the filtering in a ``delayed'' manner by appending to the frame a
``promise'' to filter that is fulfilled only when enough variables have been
bound to make the operation possible.  We could wait to perform filtering until
all other operations have been performed.  However, for efficiency's sake, we
would like to perform filtering as soon as possible so as to cut down on the
number of intermediate frames generated.
@end quotation

@quotation
@strong{@anchor{Exercise 4-78}Exercise 4.78:} Redesign the query language as a
nondeterministic program to be implemented using the evaluator of section
@ref{4-3}, rather than as a stream process.  In this approach, each query will
produce a single answer (rather than the stream of all answers) and the user
can type @code{try-again} to see more answers.  You should find that much of
the mechanism we built in this section is subsumed by nondeterministic search
and backtracking.  You will probably also find, however, that your new query
language has subtle differences in behavior from the one implemented here.  Can
you find examples that illustrate this difference?
@end quotation

@quotation
@strong{@anchor{Exercise 4-79}Exercise 4.79:} When we implemented the Lisp
evaluator in section @ref{4-1}, we saw how to use local environments to avoid
name conflicts between the parameters of procedures.  For example, in
evaluating

@lisp
(define (square x)
  (* x x))

(define (sum-of-squares x y)
  (+ (square x) (square y)))

(sum-of-squares 3 4)
@end lisp

@noindent
there is no confusion between the @code{x} in @code{square} and the @code{x} in
@code{sum-of-squares}, because we evaluate the body of each procedure in an
environment that is specially constructed to contain bindings for the local
variables.  In the query system, we used a different strategy to avoid name
conflicts in applying rules.  Each time we apply a rule we rename the variables
with new names that are guaranteed to be unique.  The analogous strategy for
the Lisp evaluator would be to do away with local environments and simply
rename the variables in the body of a procedure each time we apply the
procedure.

Implement for the query language a rule-application method that uses
environments rather than renaming.  See if you can build on your environment
structure to create constructs in the query language for dealing with large
systems, such as the rule analog of block-structured procedures.  Can you
relate any of this to the problem of making deductions in a context (e.g., ``If
I supposed that @i{P} were true, then I would be able to deduce @i{A} and
@i{B}.'') as a method of problem solving?  (This problem is open-ended.  A good
answer is probably worth a Ph.D.)
@end quotation

@node    Chapter 5, References, Chapter 4, Top
