@chapter Modularity, Objects, and State

@quotation
[greek not included here]

(Even while it changes, it stands still.)

---Heraclitus
@end quotation

@quotation
Plus @,{c}a change, plus c'est la m@ringaccent{e}me chose.

---Alphonse Karr
@end quotation

The preceding chapters introduced the basic elements from which programs are
made.  We saw how primitive procedures and primitive data are combined to
construct compound entities, and we learned that abstraction is vital in
helping us to cope with the complexity of large systems.  But these tools are
not sufficient for designing programs.  Effective program synthesis also
requires organizational principles that can guide us in formulating the overall
design of a program.  In particular, we need strategies to help us structure
large systems so that they will be @newterm{modular}, that is, so that they can
be divided ``naturally'' into coherent parts that can be separately developed
and maintained.

One powerful design strategy, which is particularly appropriate to the
construction of programs for modeling physical systems, is to base the
structure of our programs on the structure of the system being modeled.  For
each object in the system, we construct a corresponding computational object.
For each system action, we define a symbolic operation in our computational
model.  Our hope in using this strategy is that extending the model to
accommodate new objects or new actions will require no strategic changes to the
program, only the addition of the new symbolic analogs of those objects or
actions.  If we have been successful in our system organization, then to add a
new feature or debug an old one we will have to work on only a localized part
of the system.

To a large extent, then, the way we organize a large program is dictated by our
perception of the system to be modeled.  In this chapter we will investigate
two prominent organizational strategies arising from two rather different
``world views'' of the structure of systems.  The first organizational strategy
concentrates on @newterm{objects}, viewing a large system as a collection of
distinct objects whose behaviors may change over time.  An alternative
organizational strategy concentrates on the @newterm{streams} of information
that flow in the system, much as an electrical engineer views a
signal-processing system.

Both the object-based approach and the stream-processing approach raise
significant linguistic issues in programming.  With objects, we must be
concerned with how a computational object can change and yet maintain its
identity.  This will force us to abandon our old substitution model of
computation (section @ref{1-1-5}) in favor of a more mechanistic but less
theoretically tractable @newterm{environment model} of computation.  The
difficulties of dealing with objects, change, and identity are a fundamental
consequence of the need to grapple with time in our computational models.
These difficulties become even greater when we allow the possibility of
concurrent execution of programs.  The stream approach can be most fully
exploited when we decouple simulated time in our model from the order of the
events that take place in the computer during evaluation.  We will accomplish
this using a technique known as @newterm{delayed evaluation}.

@menu
* 3-1::              Assignment and Local State
* 3-2::              The Environment Model of Evaluation
* 3-3::              Modeling with Mutable Data
* 3-4::              Concurrency: Time Is of the Essence
* 3-5::              Streams
@end menu

@node	3-1, 3-2, Chapter 3, Chapter 3
@section Assignment and Local State

We ordinarily view the world as populated by independent objects, each of which
has a state that changes over time.  An object is said to ``have state'' if its
behavior is influenced by its history.  A bank account, for example, has state
in that the answer to the question ``Can I withdraw $100?''  depends upon the
history of deposit and withdrawal transactions.  We can characterize an
object's state by one or more @newterm{state variables}, which among them
maintain enough information about history to determine the object's current
behavior.  In a simple banking system, we could characterize the state of an
account by a current balance rather than by remembering the entire history of
account transactions.

In a system composed of many objects, the objects are rarely completely
independent.  Each may influence the states of others through interactions,
which serve to couple the state variables of one object to those of other
objects.  Indeed, the view that a system is composed of separate objects is
most useful when the state variables of the system can be grouped into closely
coupled subsystems that are only loosely coupled to other subsystems.

This view of a system can be a powerful framework for organizing computational
models of the system.  For such a model to be modular, it should be decomposed
into computational objects that model the actual objects in the system.  Each
computational object must have its own @newterm{local state variables}
describing the actual object's state.  Since the states of objects in the
system being modeled change over time, the state variables of the corresponding
computational objects must also change.  If we choose to model the flow of time
in the system by the elapsed time in the computer, then we must have a way to
construct computational objects whose behaviors change as our programs run.  In
particular, if we wish to model state variables by ordinary symbolic names in
the programming language, then the language must provide an @newterm{assignment
operator} to enable us to change the value associated with a name.

@menu
* 3-1-1::            Local State Variables
* 3-1-2::            The Benefits of Introducing Assignment
* 3-1-3::            The Costs of Introducing Assignment
@end menu

@node	3-1-1, 3-1-2, 3-1, 3-1
@subsection Local State Variables

To illustrate what we mean by having a computational object with time-varying
state, let us model the situation of withdrawing money from a bank account.  We
will do this using a procedure @code{withdraw}, which takes as argument an
@code{amount} to be withdrawn.  If there is enough money in the account to
accommodate the withdrawal, then @code{withdraw} should return the balance
remaining after the withdrawal.  Otherwise, @code{withdraw} should return the
message @emph{Insufficient funds}. For example, if we begin with $100 in the
account, we should obtain the following sequence of responses using
@code{withdraw}:

@lisp
(withdraw 25)
@i{75}

(withdraw 25)
@i{50}

(withdraw 60)
@i{"Insufficient funds"}

(withdraw 15)
@i{35}
@end lisp

Observe that the expression @code{(withdraw 25)}, evaluated twice, yields
different values.  This is a new kind of behavior for a procedure.  Until now,
all our procedures could be viewed as specifications for computing mathematical
functions.  A call to a procedure computed the value of the function applied to
the given arguments, and two calls to the same procedure with the same
arguments always produced the same result.@footnote{Actually, this is not quite
true.  One exception was the random-number generator in section @ref{1-2-6}.
Another exception involved the operation/type tables we introduced in section
@ref{2-4-3}, where the values of two calls to @code{get} with the same
arguments depended on intervening calls to @code{put}.  On the other hand,
until we introduce assignment, we have no way to create such procedures
ourselves.}

To implement @code{withdraw}, we can use a variable @code{balance} to indicate
the balance of money in the account and define @code{withdraw} as a procedure
that accesses @code{balance}.  The @code{withdraw} procedure checks to see if
@code{balance} is at least as large as the requested @code{amount}.  If so,
@code{withdraw} decrements @code{balance} by @code{amount} and returns the new
value of @code{balance}.  Otherwise, @code{withdraw} returns the
@emph{Insufficient funds} message.  Here are the definitions of @code{balance}
and @code{withdraw}:

@lisp
(define balance 100)

(define (withdraw amount)
  (if (>= balance amount)
      (begin (set! balance (- balance amount))
             balance)
      "Insufficient funds"))
@end lisp

Decrementing @code{balance} is accomplished by the expression

@lisp
(set! balance (- balance amount))
@end lisp

This uses the @code{set!} special form, whose syntax is

@lisp
(set! <@var{name}> <@var{new-value}>)
@end lisp

Here <@var{name}> is a symbol and <@var{new-value}> is any expression.
@code{Set!} changes <@var{name}> so that its value is the result obtained by
evaluating <@var{new-value}>.  In the case at hand, we are changing
@code{balance} so that its new value will be the result of subtracting
@code{amount} from the previous value of @code{balance}.@footnote{The value of
a @code{set!} expression is implementation-dependent.  @code{Set!} should be
used only for its effect, not for its value.

The name @code{set!} reflects a naming convention used in Scheme: Operations
that change the values of variables (or that change data structures, as we will
see in section @ref{3-3}) are given names that end with an exclamation point.
This is similar to the convention of designating predicates by names that end
with a question mark.}

@code{Withdraw} also uses the @code{begin} special form to cause two
expressions to be evaluated in the case where the @code{if} test is true: first
decrementing @code{balance} and then returning the value of @code{balance}.  In
general, evaluating the expression

@lisp
(begin <@var{exp_1}> <@var{exp_2}> @dots{} <@var{exp_@i{k}}>)
@end lisp

@noindent
causes the expressions <@var{exp_1}> through <@var{exp_@i{k}}> to be evaluated
in sequence and the value of the final expression <@var{exp_@i{k}}> to be
returned as the value of the entire @code{begin} form.@footnote{We have already
used @code{begin} implicitly in our programs, because in Scheme the body of a
procedure can be a sequence of expressions.  Also, the <@var{consequent}> part
of each clause in a @code{cond} expression can be a sequence of expressions
rather than a single expression.}

Although @code{withdraw} works as desired, the variable @code{balance} presents
a problem.  As specified above, @code{balance} is a name defined in the global
environment and is freely accessible to be examined or modified by any
procedure.  It would be much better if we could somehow make @code{balance}
internal to @code{withdraw}, so that @code{withdraw} would be the only
procedure that could access @code{balance} directly and any other procedure
could access @code{balance} only indirectly (through calls to @code{withdraw}).
This would more accurately model the notion that @code{balance} is a local
state variable used by @code{withdraw} to keep track of the state of the
account.

We can make @code{balance} internal to @code{withdraw} by rewriting the
definition as follows:

@lisp
(define new-withdraw
  (let ((balance 100))
    (lambda (amount)
      (if (>= balance amount)
          (begin (set! balance (- balance amount))
                 balance)
          "Insufficient funds"))))
@end lisp

What we have done here is use @code{let} to establish an environment with a
local variable @code{balance}, bound to the initial value 100.  Within this
local environment, we use @code{lambda} to create a procedure that takes
@code{amount} as an argument and behaves like our previous @code{withdraw}
procedure.  This procedure---returned as the result of evaluating the
@code{let} expression---is @code{new-withdraw}, which behaves in precisely the
same way as @code{withdraw} but whose variable @code{balance} is not accessible
by any other procedure.@footnote{In programming-language jargon, the variable
@code{balance} is said to be @newterm{encapsulated} within the
@code{new-withdraw} procedure.  Encapsulation reflects the general
system-design principle known as the @newterm{hiding principle}: One can make a
system more modular and robust by protecting parts of the system from each
other; that is, by providing information access only to those parts of the
system that have a ``need to know.''}

Combining @code{set!} with local variables is the general programming technique
we will use for constructing computational objects with local state.
Unfortunately, using this technique raises a serious problem: When we first
introduced procedures, we also introduced the substitution model of evaluation
(section @ref{1-1-5}) to provide an interpretation of what procedure
application means.  We said that applying a procedure should be interpreted as
evaluating the body of the procedure with the formal parameters replaced by
their values.  The trouble is that, as soon as we introduce assignment into our
language, substitution is no longer an adequate model of procedure application.
(We will see why this is so in section @ref{3-1-3}.)  As a consequence, we
technically have at this point no way to understand why the @code{new-withdraw}
procedure behaves as claimed above.  In order to really understand a procedure
such as @code{new-withdraw}, we will need to develop a new model of procedure
application.  In section @ref{3-2} we will introduce such a model, together
with an explanation of @code{set!} and local variables.  First, however, we
examine some variations on the theme established by @code{new-withdraw}.

The following procedure, @code{make-withdraw}, creates ``withdrawal
processors.''  The formal parameter @code{balance} in @code{make-withdraw}
specifies the initial amount of money in the account.@footnote{In contrast with
@code{new-withdraw} above, we do not have to use @code{let} to make
@code{balance} a local variable, since formal parameters are already local.
This will be clearer after the discussion of the environment model of
evaluation in section @ref{3-2}.  (See also @ref{Exercise 3-10}.)}

@lisp
(define (make-withdraw balance)
  (lambda (amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds")))
@end lisp

@code{Make-withdraw} can be used as follows to create two objects @code{W1} and
@code{W2}:

@lisp
(define W1 (make-withdraw 100))
(define W2 (make-withdraw 100))

(W1 50)
@i{50}

(W2 70)
@i{30}

(W2 40)
@i{"Insufficient funds"}

(W1 40)
@i{10}
@end lisp

Observe that @code{W1} and @code{W2} are completely independent objects, each
with its own local state variable @code{balance}.  Withdrawals from one do not
affect the other.

We can also create objects that handle deposits as well as withdrawals, and
thus we can represent simple bank accounts.  Here is a procedure that returns a
``bank-account object'' with a specified initial balance:

@lisp
(define (make-account balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  (define (dispatch m)
    (cond ((eq? m 'withdraw) withdraw)
          ((eq? m 'deposit) deposit)
          (else (error "Unknown request -- MAKE-ACCOUNT"
                       m))))
  dispatch)
@end lisp

Each call to @code{make-account} sets up an environment with a local state
variable @code{balance}.  Within this environment, @code{make-account} defines
procedures @code{deposit} and @code{withdraw} that access @code{balance} and an
additional procedure @code{dispatch} that takes a ``message'' as input and
returns one of the two local procedures.  The @code{dispatch} procedure itself
is returned as the value that represents the bank-account object.  This is
precisely the @newterm{message-passing} style of programming that we saw in
section @ref{2-4-3}, although here we are using it in conjunction with the
ability to modify local variables.

@code{Make-account} can be used as follows:

@lisp
(define acc (make-account 100))

((acc 'withdraw) 50)
@i{50}

((acc 'withdraw) 60)
@i{"Insufficient funds"}

((acc 'deposit) 40)
@i{90}

((acc 'withdraw) 60)
@i{30}
@end lisp

Each call to @code{acc} returns the locally defined @code{deposit} or
@code{withdraw} procedure, which is then applied to the specified
@code{amount}.  As was the case with @code{make-withdraw}, another call to
@code{make-account}

@lisp
(define acc2 (make-account 100))
@end lisp

@noindent
will produce a completely separate account object, which maintains its own
local @code{balance}.

@quotation
@strong{@anchor{Exercise 3-1}Exercise 3.1:} An @newterm{accumulator} is a
procedure that is called repeatedly with a single numeric argument and
accumulates its arguments into a sum.  Each time it is called, it returns the
currently accumulated sum.  Write a procedure @code{make-accumulator} that
generates accumulators, each maintaining an independent sum.  The input to
@code{make-accumulator} should specify the initial value of the sum; for
example

@lisp
(define A (make-accumulator 5))

(A 10)
@i{15}

(A 10)
@i{25}
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-2}Exercise 3.2:} In software-testing applications,
it is useful to be able to count the number of times a given procedure is
called during the course of a computation.  Write a procedure
@code{make-monitored} that takes as input a procedure, @code{f}, that itself
takes one input.  The result returned by @code{make-monitored} is a third
procedure, say @code{mf}, that keeps track of the number of times it has been
called by maintaining an internal counter.  If the input to @code{mf} is the
special symbol @code{how-many-calls?}, then @code{mf} returns the value of the
counter.  If the input is the special symbol @code{reset-count}, then @code{mf}
resets the counter to zero.  For any other input, @code{mf} returns the result
of calling @code{f} on that input and increments the counter.  For instance, we
could make a monitored version of the @code{sqrt} procedure:

@lisp
(define s (make-monitored sqrt))

(s 100)
@i{10}

(s 'how-many-calls?)
@i{1}
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-3}Exercise 3.3:} Modify the @code{make-account}
procedure so that it creates password-protected accounts.  That is,
@code{make-account} should take a symbol as an additional argument, as in

@lisp
(define acc (make-account 100 'secret-password))
@end lisp

The resulting account object should process a request only if it is accompanied
by the password with which the account was created, and should otherwise return
a complaint:

@lisp
((acc 'secret-password 'withdraw) 40)
@i{60}

((acc 'some-other-password 'deposit) 50)
@i{"Incorrect password"}
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-4}Exercise 3.4:} Modify the @code{make-account}
procedure of @ref{Exercise 3-3} by adding another local state variable so that,
if an account is accessed more than seven consecutive times with an incorrect
password, it invokes the procedure @code{call-the-cops}.
@end quotation

@node	3-1-2, 3-1-3, 3-1-1, 3-1
@subsection The Benefits of Introducing Assignment

As we shall see, introducing assignment into our programming language leads us
into a thicket of difficult conceptual issues.  Nevertheless, viewing systems
as collections of objects with local state is a powerful technique for
maintaining a modular design.  As a simple example, consider the design of a
procedure @code{rand} that, whenever it is called, returns an integer chosen at
random.

It is not at all clear what is meant by ``chosen at random.''  What we
presumably want is for successive calls to @code{rand} to produce a sequence of
numbers that has statistical properties of uniform distribution.  We will not
discuss methods for generating suitable sequences here.  Rather, let us assume
that we have a procedure @code{rand-update} that has the property that if we
start with a given number @i{x}_1 and form

@lisp
@i{x}_2 = (rand-update @i{x}_1)
@i{x}_3 = (rand-update @i{x}_2)
@end lisp

@noindent
then the sequence of values @i{x}_1, @i{x}_2, @i{x}_3, @dots{}, will have the
desired statistical properties.@footnote{One common way to implement
@code{rand-update} is to use the rule that @i{x} is updated to @i{a}@i{x} +
@i{b} modulo @i{m}, where @i{a}, @i{b}, and @i{m} are appropriately chosen
integers.  Chapter 3 of Knuth 1981 includes an extensive discussion of
techniques for generating sequences of random numbers and establishing their
statistical properties.  Notice that the @code{rand-update} procedure computes
a mathematical function: Given the same input twice, it produces the same
output.  Therefore, the number sequence produced by @code{rand-update}
certainly is not ``random,'' if by ``random'' we insist that each number in the
sequence is unrelated to the preceding number.  The relation between ``real
randomness'' and so-called @newterm{pseudo-random} sequences, which are
produced by well-determined computations and yet have suitable statistical
properties, is a complex question involving difficult issues in mathematics and
philosophy.  Kolmogorov, Solomonoff, and Chaitin have made great progress in
clarifying these issues; a discussion can be found in Chaitin 1975.}

We can implement @code{rand} as a procedure with a local state variable
@code{x} that is initialized to some fixed value @code{random-init}.  Each call
to @code{rand} computes @code{rand-update} of the current value of @code{x},
returns this as the random number, and also stores this as the new value of
@code{x}.

@lisp
(define rand
  (let ((x random-init))
    (lambda ()
      (set! x (rand-update x))
      x)))
@end lisp

Of course, we could generate the same sequence of random numbers without using
assignment by simply calling @code{rand-update} directly.  However, this would
mean that any part of our program that used random numbers would have to
explicitly remember the current value of @code{x} to be passed as an argument
to @code{rand-update}.  To realize what an annoyance this would be, consider
using random numbers to implement a technique called @newterm{Monte Carlo
simulation}.

The Monte Carlo method consists of choosing sample experiments at random from a
large set and then making deductions on the basis of the probabilities
estimated from tabulating the results of those experiments.  For example, we
can approximate @i{[pi]} using the fact that 6/@i{[pi]}^2 is the probability
that two integers chosen at random will have no factors in common; that is,
that their greatest common divisor will be 1.@footnote{This theorem is due to
E. Ces@`aro.  See section 4.5.2 of Knuth 1981 for a discussion and a proof.} To
obtain the approximation to @i{[pi]}, we perform a large number of experiments.
In each experiment we choose two integers at random and perform a test to see
if their @acronym{GCD} is 1.  The fraction of times that the test is passed
gives us our estimate of 6/@i{[pi]}^2, and from this we obtain our
approximation to @i{[pi]}.

The heart of our program is a procedure @code{monte-carlo}, which takes as
arguments the number of times to try an experiment, together with the
experiment, represented as a no-argument procedure that will return either true
or false each time it is run.  @code{Monte-carlo} runs the experiment for the
designated number of trials and returns a number telling the fraction of the
trials in which the experiment was found to be true.

@lisp
(define (estimate-pi trials)
  (sqrt (/ 6 (monte-carlo trials cesaro-test))))

(define (cesaro-test)
   (= (gcd (rand) (rand)) 1))

(define (monte-carlo trials experiment)
  (define (iter trials-remaining trials-passed)
    (cond ((= trials-remaining 0)
           (/ trials-passed trials))
          ((experiment)
           (iter (- trials-remaining 1) (+ trials-passed 1)))
          (else
           (iter (- trials-remaining 1) trials-passed))))
  (iter trials 0))
@end lisp

Now let us try the same computation using @code{rand-update} directly rather
than @code{rand}, the way we would be forced to proceed if we did not use
assignment to model local state:

@lisp
(define (estimate-pi trials)
  (sqrt (/ 6 (random-gcd-test trials random-init))))

(define (random-gcd-test trials initial-x)
  (define (iter trials-remaining trials-passed x)
    (let ((x1 (rand-update x)))
      (let ((x2 (rand-update x1)))
        (cond ((= trials-remaining 0)
               (/ trials-passed trials))
              ((= (gcd x1 x2) 1)
               (iter (- trials-remaining 1)
                     (+ trials-passed 1)
                     x2))
              (else
               (iter (- trials-remaining 1)
                     trials-passed
                     x2))))))
  (iter trials 0 initial-x))
@end lisp

While the program is still simple, it betrays some painful breaches of
modularity.  In our first version of the program, using @code{rand}, we can
express the Monte Carlo method directly as a general @code{monte-carlo}
procedure that takes as an argument an arbitrary @code{experiment} procedure.
In our second version of the program, with no local state for the random-number
generator, @code{random-gcd-test} must explicitly manipulate the random numbers
@code{x1} and @code{x2} and recycle @code{x2} through the iterative loop as the
new input to @code{rand-update}.  This explicit handling of the random numbers
intertwines the structure of accumulating test results with the fact that our
particular experiment uses two random numbers, whereas other Monte Carlo
experiments might use one random number or three.  Even the top-level procedure
@code{estimate-pi} has to be concerned with supplying an initial random number.
The fact that the random-number generator's insides are leaking out into other
parts of the program makes it difficult for us to isolate the Monte Carlo idea
so that it can be applied to other tasks.  In the first version of the program,
assignment encapsulates the state of the random-number generator within the
@code{rand} procedure, so that the details of random-number generation remain
independent of the rest of the program.

The general phenomenon illustrated by the Monte Carlo example is this: From the
point of view of one part of a complex process, the other parts appear to
change with time.  They have hidden time-varying local state.  If we wish to
write computer programs whose structure reflects this decomposition, we make
computational objects (such as bank accounts and random-number generators)
whose behavior changes with time.  We model state with local state variables,
and we model the changes of state with assignments to those variables.

It is tempting to conclude this discussion by saying that, by introducing
assignment and the technique of hiding state in local variables, we are able to
structure systems in a more modular fashion than if all state had to be
manipulated explicitly, by passing additional parameters.  Unfortunately, as we
shall see, the story is not so simple.

@quotation
@strong{@anchor{Exercise 3-5}Exercise 3.5:} @newterm{Monte Carlo integration}
is a method of estimating definite integrals by means of Monte Carlo
simulation.  Consider computing the area of a region of space described by a
predicate @i{P}(@i{x}, @i{y}) that is true for points (@i{x}, @i{y}) in the
region and false for points not in the region.  For example, the region
contained within a circle of radius 3 centered at (5, 7) is described by the
predicate that tests whether (@i{x} - 5)^2 + (@i{y} - 7)^2 <= 3^2.  To estimate
the area of the region described by such a predicate, begin by choosing a
rectangle that contains the region.  For example, a rectangle with diagonally
opposite corners at (2, 4) and (8, 10) contains the circle above.  The desired
integral is the area of that portion of the rectangle that lies in the region.
We can estimate the integral by picking, at random, points (@i{x},@i{y}) that
lie in the rectangle, and testing @i{P}(@i{x}, @i{y}) for each point to
determine whether the point lies in the region.  If we try this with many
points, then the fraction of points that fall in the region should give an
estimate of the proportion of the rectangle that lies in the region.  Hence,
multiplying this fraction by the area of the entire rectangle should produce an
estimate of the integral.

Implement Monte Carlo integration as a procedure @code{estimate-integral} that
takes as arguments a predicate @code{P}, upper and lower bounds @code{x1},
@code{x2}, @code{y1}, and @code{y2} for the rectangle, and the number of trials
to perform in order to produce the estimate.  Your procedure should use the
same @code{monte-carlo} procedure that was used above to estimate @i{[pi]}.
Use your @code{estimate-integral} to produce an estimate of @i{[pi]} by
measuring the area of a unit circle.

You will find it useful to have a procedure that returns a number chosen at
random from a given range.  The following @code{random-in-range} procedure
implements this in terms of the @code{random} procedure used in section
@ref{1-2-6}, which returns a nonnegative number less than its
input.@footnote{@acronym{MIT} Scheme provides such a procedure.  If
@code{random} is given an exact integer (as in section @ref{1-2-6}) it returns
an exact integer, but if it is given a decimal value (as in this exercise) it
returns a decimal value.}

@lisp
(define (random-in-range low high)
  (let ((range (- high low)))
    (+ low (random range))))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-6}Exercise 3.6:} It is useful to be able to reset a
random-number generator to produce a sequence starting from a given value.
Design a new @code{rand} procedure that is called with an argument that is
either the symbol @code{generate} or the symbol @code{reset} and behaves as
follows: @code{(rand 'generate)} produces a new random number; @code{((rand
'reset) <@var{new-value}>)} resets the internal state variable to the
designated <@var{new-value}>.  Thus, by resetting the state, one can generate
repeatable sequences.  These are very handy to have when testing and debugging
programs that use random numbers.
@end quotation

@node	3-1-3,  , 3-1-2, 3-1
@subsection The Costs of Introducing Assignment

As we have seen, the @code{set!} operation enables us to model objects that
have local state.  However, this advantage comes at a price.  Our programming
language can no longer be interpreted in terms of the substitution model of
procedure application that we introduced in section @ref{1-1-5}.  Moreover, no
simple model with ``nice'' mathematical properties can be an adequate framework
for dealing with objects and assignment in programming languages.

So long as we do not use assignments, two evaluations of the same procedure
with the same arguments will produce the same result, so that procedures can be
viewed as computing mathematical functions.  Programming without any use of
assignments, as we did throughout the first two chapters of this book, is
accordingly known as @newterm{functional programming}.

To understand how assignment complicates matters, consider a simplified version
of the @code{make-withdraw} procedure of section @ref{3-1-1} that does not
bother to check for an insufficient amount:

@lisp
(define (make-simplified-withdraw balance)
  (lambda (amount)
    (set! balance (- balance amount))
    balance))

(define W (make-simplified-withdraw 25))

(W 20)
@i{5}

(W 10)
@i{ - 5}
@end lisp

Compare this procedure with the following @code{make-decrementer} procedure,
which does not use @code{set!}:

@lisp
(define (make-decrementer balance)
  (lambda (amount)
    (- balance amount)))
@end lisp

@code{Make-decrementer} returns a procedure that subtracts its input from a
designated amount @code{balance}, but there is no accumulated effect over
successive calls, as with @code{make-simplified-withdraw}:

@lisp
(define D (make-decrementer 25))

(D 20)
@i{5}

(D 10)
@i{15}
@end lisp

We can use the substitution model to explain how @code{make-decrementer} works.
For instance, let us analyze the evaluation of the expression

@lisp
((make-decrementer 25) 20)
@end lisp

We first simplify the operator of the combination by substituting 25 for
@code{balance} in the body of @code{make-decrementer}.  This reduces the
expression to

@lisp
((lambda (amount) (- 25 amount)) 20)
@end lisp

Now we apply the operator by substituting 20 for @code{amount} in the body of
the @code{lambda} expression:

@lisp
(- 25 20)
@end lisp

The final answer is 5.

Observe, however, what happens if we attempt a similar substitution analysis
with @code{make-simplified-withdraw}:

@lisp
((make-simplified-withdraw 25) 20)
@end lisp

We first simplify the operator by substituting 25 for @code{balance} in the
body of @code{make-simplified-withdraw}.  This reduces the expression
to@footnote{We don't substitute for the occurrence of @code{balance} in the
@code{set!} expression because the <@var{name}> in a @code{set!} is not
evaluated.  If we did substitute for it, we would get @code{(set! 25 (- 25
amount))}, which makes no sense.}

@lisp
((lambda (amount) (set! balance (- 25 amount)) 25) 20)
@end lisp

Now we apply the operator by substituting 20 for @code{amount} in the body of
the @code{lambda} expression:

@lisp
(set! balance (- 25 20)) 25
@end lisp

If we adhered to the substitution model, we would have to say that the meaning
of the procedure application is to first set @code{balance} to 5 and then
return 25 as the value of the expression.  This gets the wrong answer.  In
order to get the correct answer, we would have to somehow distinguish the first
occurrence of @code{balance} (before the effect of the @code{set!})  from the
second occurrence of @code{balance} (after the effect of the @code{set!}), and
the substitution model cannot do this.

The trouble here is that substitution is based ultimately on the notion that
the symbols in our language are essentially names for values.  But as soon as
we introduce @code{set!} and the idea that the value of a variable can change,
a variable can no longer be simply a name.  Now a variable somehow refers to a
place where a value can be stored, and the value stored at this place can
change.  In section @ref{3-2} we will see how environments play this role of
``place'' in our computational model.

@subsubheading Sameness and change

The issue surfacing here is more profound than the mere breakdown of a
particular model of computation.  As soon as we introduce change into our
computational models, many notions that were previously straightforward become
problematical.  Consider the concept of two things being ``the same.''

Suppose we call @code{make-decrementer} twice with the same argument to create
two procedures:

@lisp
(define D1 (make-decrementer 25))

(define D2 (make-decrementer 25))
@end lisp

Are @code{D1} and @code{D2} the same?  An acceptable answer is yes, because
@code{D1} and @code{D2} have the same computational behavior---each is a
procedure that subtracts its input from 25.  In fact, @code{D1} could be
substituted for @code{D2} in any computation without changing the result.

Contrast this with making two calls to @code{make-simplified-withdraw}:

@lisp
(define W1 (make-simplified-withdraw 25))

(define W2 (make-simplified-withdraw 25))
@end lisp

Are @code{W1} and @code{W2} the same?  Surely not, because calls to @code{W1}
and @code{W2} have distinct effects, as shown by the following sequence of
interactions:

@lisp
(W1 20)
@i{5}

(W1 20)
@i{ - 15}

(W2 20)
@i{5}
@end lisp

Even though @code{W1} and @code{W2} are ``equal'' in the sense that they are
both created by evaluating the same expression, @code{(make-simplified-withdraw
25)}, it is not true that @code{W1} could be substituted for @code{W2} in any
expression without changing the result of evaluating the expression.

A language that supports the concept that ``equals can be substituted for
equals'' in an expresssion without changing the value of the expression is said
to be @newterm{referentially transparent}.  Referential transparency is
violated when we include @code{set!} in our computer language.  This makes it
tricky to determine when we can simplify expressions by substituting equivalent
expressions.  Consequently, reasoning about programs that use assignment
becomes drastically more difficult.

Once we forgo referential transparency, the notion of what it means for
computational objects to be ``the same'' becomes difficult to capture in a
formal way.  Indeed, the meaning of ``same'' in the real world that our
programs model is hardly clear in itself.  In general, we can determine that
two apparently identical objects are indeed ``the same one'' only by modifying
one object and then observing whether the other object has changed in the same
way.  But how can we tell if an object has ``changed'' other than by observing
the ``same'' object twice and seeing whether some property of the object
differs from one observation to the next?  Thus, we cannot determine ``change''
without some @emph{a priori} notion of ``sameness,'' and we cannot determine
sameness without observing the effects of change.

As an example of how this issue arises in programming, consider the situation
where Peter and Paul have a bank account with $100 in it.  There is a
substantial difference between modeling this as

@lisp
(define peter-acc (make-account 100))
(define paul-acc (make-account 100))
@end lisp

@noindent
and modeling it as

@lisp
(define peter-acc (make-account 100))
(define paul-acc peter-acc)
@end lisp

In the first situation, the two bank accounts are distinct.  Transactions made
by Peter will not affect Paul's account, and vice versa.  In the second
situation, however, we have defined @code{paul-acc} to be @emph{the same thing}
as @code{peter-acc}.  In effect, Peter and Paul now have a joint bank account,
and if Peter makes a withdrawal from @code{peter-acc} Paul will observe less
money in @code{paul-acc}.  These two similar but distinct situations can cause
confusion in building computational models.  With the shared account, in
particular, it can be especially confusing that there is one object (the bank
account) that has two different names (@code{peter-acc} and @code{paul-acc});
if we are searching for all the places in our program where @code{paul-acc} can
be changed, we must remember to look also at things that change
@code{peter-acc}.@footnote{The phenomenon of a single computational object
being accessed by more than one name is known as @newterm{aliasing}.  The joint
bank account situation illustrates a very simple example of an alias.  In
section @ref{3-3} we will see much more complex examples, such as ``distinct''
compound data structures that share parts.  Bugs can occur in our programs if
we forget that a change to an object may also, as a ``side effect,'' change a
``different'' object because the two ``different'' objects are actually a
single object appearing under different aliases.  These so-called
@newterm{side-effect bugs} are so difficult to locate and to analyze that some
people have proposed that programming languages be designed in such a way as to
not allow side effects or aliasing (Lampson et al. 1981; Morris, Schmidt, and
Wadler 1980).}

With reference to the above remarks on ``sameness'' and ``change,'' observe
that if Peter and Paul could only examine their bank balances, and could not
perform operations that changed the balance, then the issue of whether the two
accounts are distinct would be moot.  In general, so long as we never modify
data objects, we can regard a compound data object to be precisely the totality
of its pieces.  For example, a rational number is determined by giving its
numerator and its denominator.  But this view is no longer valid in the
presence of change, where a compound data object has an ``identity'' that is
something different from the pieces of which it is composed.  A bank account is
still ``the same'' bank account even if we change the balance by making a
withdrawal; conversely, we could have two different bank accounts with the same
state information.  This complication is a consequence, not of our programming
language, but of our perception of a bank account as an object.  We do not, for
example, ordinarily regard a rational number as a changeable object with
identity, such that we could change the numerator and still have ``the same''
rational number.

@subsubheading Pitfalls of imperative programming

In contrast to functional programming, programming that makes extensive use of
assignment is known as @newterm{imperative programming}.  In addition to
raising complications about computational models, programs written in
imperative style are susceptible to bugs that cannot occur in functional
programs.  For example, recall the iterative factorial program from section
@ref{1-2-1}:

@lisp
(define (factorial n)
  (define (iter product counter)
    (if (> counter n)
        product
        (iter (* counter product)
              (+ counter 1))))
  (iter 1 1))
@end lisp

Instead of passing arguments in the internal iterative loop, we could adopt a
more imperative style by using explicit assignment to update the values of the
variables @code{product} and @code{counter}:

@lisp
(define (factorial n)
  (let ((product 1)
        (counter 1))
    (define (iter)
      (if (> counter n)
          product
          (begin (set! product (* counter product))
                 (set! counter (+ counter 1))
                 (iter))))
    (iter)))
@end lisp

This does not change the results produced by the program, but it does introduce
a subtle trap.  How do we decide the order of the assignments?  As it happens,
the program is correct as written.  But writing the assignments in the opposite
order

@lisp
(set! counter (+ counter 1))
(set! product (* counter product))
@end lisp

@noindent
would have produced a different, incorrect result.  In general, programming
with assignment forces us to carefully consider the relative orders of the
assignments to make sure that each statement is using the correct version of
the variables that have been changed.  This issue simply does not arise in
functional programs.@footnote{In view of this, it is ironic that introductory
programming is most often taught in a highly imperative style.  This may be a
vestige of a belief, common throughout the 1960s and 1970s, that programs that
call procedures must inherently be less efficient than programs that perform
assignments.  (Steele (1977) debunks this argument.)  Alternatively it may
reflect a view that step-by-step assignment is easier for beginners to
visualize than procedure call.  Whatever the reason, it often saddles beginning
programmers with ``should I set this variable before or after that one''
concerns that can complicate programming and obscure the important ideas.}

The complexity of imperative programs becomes even worse if we consider
applications in which several processes execute concurrently.  We will return
to this in section @ref{3-4}.  First, however, we will address the issue of
providing a computational model for expressions that involve assignment, and
explore the uses of objects with local state in designing simulations.

@quotation
@strong{@anchor{Exercise 3-7}Exercise 3.7:} Consider the bank account objects
created by @code{make-account}, with the password modification described in
@ref{Exercise 3-3}.  Suppose that our banking system requires the ability to
make joint accounts.  Define a procedure @code{make-joint} that accomplishes
this.  @code{Make-joint} should take three arguments.  The first is a
password-protected account.  The second argument must match the password with
which the account was defined in order for the @code{make-joint} operation to
proceed.  The third argument is a new password.  @code{Make-joint} is to create
an additional access to the original account using the new password.  For
example, if @code{peter-acc} is a bank account with password
@code{open-sesame}, then

@lisp
(define paul-acc
  (make-joint peter-acc 'open-sesame 'rosebud))
@end lisp

@noindent
will allow one to make transactions on @code{peter-acc} using the name
@code{paul-acc} and the password @code{rosebud}.  You may wish to modify your
solution to @ref{Exercise 3-3} to accommodate this new feature
@end quotation

@quotation
@strong{@anchor{Exercise 3-8}Exercise 3.8:} When we defined the evaluation
model in section @ref{1-1-3}, we said that the first step in evaluating an
expression is to evaluate its subexpressions.  But we never specified the order
in which the subexpressions should be evaluated (e.g., left to right or right
to left).  When we introduce assignment, the order in which the arguments to a
procedure are evaluated can make a difference to the result.  Define a simple
procedure @code{f} such that evaluating @code{(+ (f 0) (f 1))} will return 0 if
the arguments to @code{+} are evaluated from left to right but will return 1 if
the arguments are evaluated from right to left.
@end quotation

@node	3-2, 3-3, 3-1, Chapter 3
@section The Environment Model of Evaluation

When we introduced compound procedures in @ref{Chapter 1}, we used the
substitution model of evaluation (section @ref{1-1-5}) to define what is meant
by applying a procedure to arguments:

@itemize @bullet

@item
To apply a compound procedure to arguments, evaluate the body of the procedure
with each formal parameter replaced by the corresponding argument.

@end itemize

Once we admit assignment into our programming language, such a definition is no
longer adequate.  In particular, section @ref{3-1-3} argued that, in the
presence of assignment, a variable can no longer be considered to be merely a
name for a value.  Rather, a variable must somehow designate a ``place'' in
which values can be stored.  In our new model of evaluation, these places will
be maintained in structures called @newterm{environments}.

An environment is a sequence of @newterm{frames}.  Each frame is a table
(possibly empty) of @newterm{bindings}, which associate variable names with
their corresponding values.  (A single frame may contain at most one binding
for any variable.)  Each frame also has a pointer to its @newterm{enclosing
environment}, unless, for the purposes of discussion, the frame is considered
to be @newterm{global}.  The @newterm{value of a variable} with respect to an
environment is the value given by the binding of the variable in the first
frame in the environment that contains a binding for that variable.  If no
frame in the sequence specifies a binding for the variable, then the variable
is said to be @newterm{unbound} in the environment.

@quotation
@strong{@anchor{Figure 3-1}Figure 3.1:} A simple environment structure.

@example
           +--------+
           |      I |
           | x: 3   |
           | y: 5   |
           +--------+
              ^  ^
              |  |
            C |  | D
+---------+   |  |   +----------+
|      II |   |  |   |      III |
| z: 6    +---+  +---+ m: 1     |
| x: 7    |          | y: 2     |
+---------+          +----------+
@end example
@end quotation

@ref{Figure 3-1} shows a simple environment structure consisting of three
frames, labeled I, II, and III.  In the diagram, A, B, C, and D are pointers to
environments.  C and D point to the same environment.  The variables @code{z}
and @code{x} are bound in frame II, while @code{y} and @code{x} are bound in
frame I.  The value of @code{x} in environment D is 3.  The value of @code{x}
with respect to environment B is also 3.  This is determined as follows: We
examine the first frame in the sequence (frame III) and do not find a binding
for @code{x}, so we proceed to the enclosing environment D and find the binding
in frame I.  On the other hand, the value of @code{x} in environment A is 7,
because the first frame in the sequence (frame II) contains a binding of
@code{x} to 7.  With respect to environment A, the binding of @code{x} to 7 in
frame II is said to @newterm{shadow} the binding of @code{x} to 3 in frame I.

The environment is crucial to the evaluation process, because it determines the
context in which an expression should be evaluated.  Indeed, one could say that
expressions in a programming language do not, in themselves, have any meaning.
Rather, an expression acquires a meaning only with respect to some environment
in which it is evaluated.  Even the interpretation of an expression as
straightforward as @code{(+ 1 1)} depends on an understanding that one is
operating in a context in which @code{+} is the symbol for addition.  Thus, in
our model of evaluation we will always speak of evaluating an expression with
respect to some environment.  To describe interactions with the interpreter, we
will suppose that there is a global environment, consisting of a single frame
(with no enclosing environment) that includes values for the symbols associated
with the primitive procedures.  For example, the idea that @code{+} is the
symbol for addition is captured by saying that the symbol @code{+} is bound in
the global environment to the primitive addition procedure.

@menu
* 3-2-1::            The Rules for Evaluation
* 3-2-2::            Applying Simple Procedures
* 3-2-3::            Frames as the Repository of Local State
* 3-2-4::            Internal Definitions
@end menu

@node	3-2-1, 3-2-2, 3-2, 3-2
@subsection The Rules for Evaluation

The overall specification of how the interpreter evaluates a combination
remains the same as when we first introduced it in section @ref{1-1-3}:

@itemize @bullet

@item
To evaluate a combination:

@end itemize

@enumerate 1

@item
Evaluate the subexpressions of the combination.@footnote{ssignment introduces a
subtlety into step 1 of the evaluation rule.  As shown in @ref{Exercise 3-8},
the presence of assignment allows us to write expressions that will produce
different values depending on the order in which the subexpressions in a
combination are evaluated.  Thus, to be precise, we should specify an
evaluation order in step 1 (e.g., left to right or right to left).  However,
this order should always be considered to be an implementation detail, and one
should never write programs that depend on some particular order.  For
instance, a sophisticated compiler might optimize a program by varying the
order in which subexpressions are evaluated.}

@item
Apply the value of the operator subexpression to the values of the operand
subexpressions.

@end enumerate

The environment model of evaluation replaces the substitution model in
specifying what it means to apply a compound procedure to arguments.

In the environment model of evaluation, a procedure is always a pair consisting
of some code and a pointer to an environment.  Procedures are created in one
way only: by evaluating a @code{lambda} expression.  This produces a procedure
whose code is obtained from the text of the @code{lambda} expression and whose
environment is the environment in which the @code{lambda} expression was
evaluated to produce the procedure.  For example, consider the procedure
definition

@lisp
(define (square x)
  (* x x))
@end lisp

@noindent
evaluated in the global environment.  The procedure definition syntax is just
syntactic sugar for an underlying implicit @code{lambda} expression.  It would
have been equivalent to have used

@lisp
(define square
  (lambda (x) (* x x)))
@end lisp

@noindent
which evaluates @code{(lambda (x) (* x x))} and binds @code{square} to the
resulting value, all in the global environment.

@ref{Figure 3-2} shows the result of evaluating this @code{define} expression.
The procedure object is a pair whose code specifies that the procedure has one
formal parameter, namely @code{x}, and a procedure body @code{(* x x)}.  The
environment part of the procedure is a pointer to the global environment, since
that is the environment in which the @code{lambda} expression was evaluated to
produce the procedure. A new binding, which associates the procedure object
with the symbol @code{square}, has been added to the global frame.  In general,
@code{define} creates definitions by adding bindings to frames.

@quotation
@strong{@anchor{Figure 3-2}Figure 3.2:} Environment structure produced by
evaluating @code{(define (square x) (* x x))} in the global environment.

@example
           +----------------------+
           | other variables      |
global --->|                      |
env        | square: --+          |
           +-----------|----------+
                       |       ^
(define (square x)     |       |
  (* x x))             V       |
                   .---.---.   |
                   | O | O-+---+
                   `-|-^---'
                     |
                     V
                   parameters: x
                   body: (* x x)
@end example
@end quotation

Now that we have seen how procedures are created, we can describe how
procedures are applied.  The environment model specifies: To apply a procedure
to arguments, create a new environment containing a frame that binds the
parameters to the values of the arguments.  The enclosing environment of this
frame is the environment specified by the procedure.  Now, within this new
environment, evaluate the procedure body.

To show how this rule is followed, @ref{Figure 3-3} illustrates the environment
structure created by evaluating the expression @code{(square 5)} in the global
environment, where @code{square} is the procedure generated in @ref{Figure
3-2}.  Applying the procedure results in the creation of a new environment,
labeled E1 in the figure, that begins with a frame in which @code{x}, the
formal parameter for the procedure, is bound to the argument 5.  The pointer
leading upward from this frame shows that the frame's enclosing environment is
the global environment.  The global environment is chosen here, because this is
the environment that is indicated as part of the @code{square} procedure
object.  Within E1, we evaluate the body of the procedure, @code{(* x x)}.
Since the value of @code{x} in E1 is 5, the result is @code{(* 5 5)}, or 25.

@quotation
@strong{@anchor{Figure 3-3}Figure 3.3:} Environment created by evaluating
@code{(square 5)} in the global environment.

@example
          +------------------------------------+
          | other variables                    |
global -->|                                    |
env       | square: --+                        |
          +-----------|---------------------+--+
                      |       ^             ^
(square 5)            |       |             |
                      V       |             |
                  .---.---.   |         +---+--+
                  | O | O-+---+   E1 -->| x: 5 |
                  `-|-^---'             +------+
                    |
                    V
                  parameters: x
                  body: (* x x)
@end example
@end quotation

The environment model of procedure application can be summarized by two rules:

@itemize @bullet

@item
A procedure object is applied to a set of arguments by constructing a frame,
binding the formal parameters of the procedure to the arguments of the call,
and then evaluating the body of the procedure in the context of the new
environment constructed.  The new frame has as its enclosing environment the
environment part of the procedure object being applied.

@item
A procedure is created by evaluating a @code{lambda} expression relative to a
given environment.  The resulting procedure object is a pair consisting of the
text of the @code{lambda} expression and a pointer to the environment in which
the procedure was created.

@end itemize

We also specify that defining a symbol using @code{define} creates a binding in
the current environment frame and assigns to the symbol the indicated
value.@footnote{If there is already a binding for the variable in the current
frame, then the binding is changed.  This is convenient because it allows
redefinition of symbols; however, it also means that @code{define} can be used
to change values, and this brings up the issues of assignment without
explicitly using @code{set!}.  Because of this, some people prefer
redefinitions of existing symbols to signal errors or warnings.} Finally, we
specify the behavior of @code{set!}, the operation that forced us to introduce
the environment model in the first place.  Evaluating the expression
@code{(set! <@var{variable}> <@var{value}>)} in some environment locates the
binding of the variable in the environment and changes that binding to indicate
the new value.  That is, one finds the first frame in the environment that
contains a binding for the variable and modifies that frame.  If the variable
is unbound in the environment, then @code{set!} signals an error.

These evaluation rules, though considerably more complex than the substitution
model, are still reasonably straightforward.  Moreover, the evaluation model,
though abstract, provides a correct description of how the interpreter
evaluates expressions.  In @ref{Chapter 4} we shall see how this model can
serve as a blueprint for implementing a working interpreter.  The following
sections elaborate the details of the model by analyzing some illustrative
programs.

@node	3-2-2, 3-2-3, 3-2-1, 3-2
@subsection Applying Simple Procedures

When we introduced the substitution model in section @ref{1-1-5} we showed how
the combination @code{(f 5)} evaluates to 136, given the following procedure
definitions:

@lisp
(define (square x)
  (* x x))

(define (sum-of-squares x y)
  (+ (square x) (square y)))

(define (f a)
  (sum-of-squares (+ a 1) (* a 2)))
@end lisp

We can analyze the same example using the environment model.  @ref{Figure 3-4}
shows the three procedure objects created by evaluating the definitions of
@code{f}, @code{square}, and @code{sum-of-squares} in the global environment.
Each procedure object consists of some code, together with a pointer to the
global environment.

@quotation
@strong{@anchor{Figure 3-4}Figure 3.4:} Procedure objects in the global frame.

@example
          +--------------------------------------------+
          | sum-of-squares:                            |
global -->| square:                                    |
env       | f: --+                                     |
          +------|--------------+--------------+-------+
                 |     ^        |     ^        |     ^
                 |     |        |     |        |     |
                 V     |        V     |        V     |
             .---.---. |    .---.---. |    .---.---. |
             | O | O-+-+    | O | O-+-+    | O | O-+-+
             `-|-^---'      `-|-^---'      `-|-^---'
               |              |              |
               V              V              V
   parameters: a          parameters: x  parameters: x, y
   body: (sum-of-squares  body: (* x x)  body: (+ (square x)
           (+ a 1)                                (square y))
           (* a 2))
@end example
@end quotation

In @ref{Figure 3-5} we see the environment structure created by evaluating the
expression @code{(f 5)}.  The call to @code{f} creates a new environment E1
beginning with a frame in which @code{a}, the formal parameter of @code{f}, is
bound to the argument 5.  In E1, we evaluate the body of @code{f}:

@lisp
(sum-of-squares (+ a 1) (* a 2))
@end lisp

@quotation
@strong{@anchor{Figure 3-5}Figure 3.5:} Environments created by evaluating
@code{(f 5)} using the procedures in @ref{Figure 3-4}.

@example
          +-----------------------------------------------------+
global -->|                                                     |
env       +-----------------------------------------------------+
            ^              ^                ^               ^
(f 5)       |              |                |               |
        +------+       +-------+        +------+        +-------+
  E1 -->| a: 5 |  E2 ->| x: 6  |  E3 -->| x: 6 |  E4 -->| x: 10 |
        |      |       | y: 10 |        |      |        |       |
        +------+       +-------+        +------+        +-------+
   (sum-of-squares   (+ (square x)       (* x x)         (* x x)
     (+ a 1)            (square u))
     (+ a 2))
@end example
@end quotation

To evaluate this combination, we first evaluate the subexpressions.  The first
subexpression, @code{sum-of-squares}, has a value that is a procedure object.
(Notice how this value is found: We first look in the first frame of E1, which
contains no binding for @code{sum-of-squares}.  Then we proceed to the
enclosing environment, i.e. the global environment, and find the binding shown
in @ref{Figure 3-4}.)  The other two subexpressions are evaluated by applying
the primitive operations @code{+} and @code{*} to evaluate the two combinations
@code{(+ a 1)} and @code{(* a 2)} to obtain 6 and 10, respectively.

Now we apply the procedure object @code{sum-of-squares} to the arguments 6 and
10.  This results in a new environment E2 in which the formal parameters
@code{x} and @code{y} are bound to the arguments.  Within E2 we evaluate the
combination @code{(+ (square x) (square y))}.  This leads us to evaluate
@code{(square x)}, where @code{square} is found in the global frame and
@code{x} is 6.  Once again, we set up a new environment, E3, in which @code{x}
is bound to 6, and within this we evaluate the body of @code{square}, which is
@code{(* x x)}.  Also as part of applying @code{sum-of-squares}, we must
evaluate the subexpression @code{(square y)}, where @code{y} is 10.  This
second call to @code{square} creates another environment, E4, in which
@code{x}, the formal parameter of @code{square}, is bound to 10.  And within E4
we must evaluate @code{(* x x)}.

The important point to observe is that each call to @code{square} creates a new
environment containing a binding for @code{x}.  We can see here how the
different frames serve to keep separate the different local variables all named
@code{x}.  Notice that each frame created by @code{square} points to the global
environment, since this is the environment indicated by the @code{square}
procedure object.

After the subexpressions are evaluated, the results are returned.  The values
generated by the two calls to @code{square} are added by @code{sum-of-squares},
and this result is returned by @code{f}.  Since our focus here is on the
environment structures, we will not dwell on how these returned values are
passed from call to call; however, this is also an important aspect of the
evaluation process, and we will return to it in detail in @ref{Chapter 5}.

@quotation
@strong{@anchor{Exercise 3-9}Exercise 3.9:} In section @ref{1-2-1} we used the
substitution model to analyze two procedures for computing factorials, a
recursive version

@lisp
(define (factorial n)
  (if (= n 1)
      1
      (* n (factorial (- n 1)))))
@end lisp

@noindent
and an iterative version

@lisp
(define (factorial n)
  (fact-iter 1 1 n))

(define (fact-iter product counter max-count)
  (if (> counter max-count)
      product
      (fact-iter (* counter product)
                 (+ counter 1)
                 max-count)))
@end lisp

Show the environment structures created by evaluating @code{(factorial 6)}
using each version of the @code{factorial} procedure.@footnote{The environment
model will not clarify our claim in section @ref{1-2-1} that the interpreter
can execute a procedure such as @code{fact-iter} in a constant amount of space
using tail recursion.  We will discuss tail recursion when we deal with the
control structure of the interpreter in section @ref{5-4}.}
@end quotation

@node	3-2-3, 3-2-4, 3-2-2, 3-2
@subsection Frames as the Repository of Local State

We can turn to the environment model to see how procedures and assignment can
be used to represent objects with local state.  As an example, consider the
``withdrawal processor'' from section @ref{3-1-1} created by calling the
procedure

@lisp
(define (make-withdraw balance)
  (lambda (amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds")))
@end lisp

Let us describe the evaluation of

@lisp
(define W1 (make-withdraw 100))
@end lisp

@noindent
followed by

@lisp
(W1 50)
@i{50}
@end lisp

@ref{Figure 3-6} shows the result of defining the @code{make-withdraw}
procedure in the global environment.  This produces a procedure object that
contains a pointer to the global environment.  So far, this is no different
from the examples we have already seen, except that the body of the procedure
is itself a @code{lambda} expression.

@quotation
@strong{@anchor{Figure 3-6}Figure 3.6:} Result of defining @code{make-withdraw}
in the global environment.

@example
          +---------------------------+
global -->| make-withdraw: --+        |
env       +------------------|--------+
                             |      ^
                             V      |
                         .---.---.  |
                         | O | O-+--+
                         `-|-^---'
                           |
                           V
         parameters: balance
         body: (lambda (amount)
                 (if (>= balance amount)
                     (begin (set! balance
                                  (- balance amount))
                            balance)
                     "Insufficient funds"))
@end example
@end quotation

The interesting part of the computation happens when we apply the procedure
@code{make-withdraw} to an argument:

@lisp
(define W1 (make-withdraw 100))
@end lisp

We begin, as usual, by setting up an environment E1 in which the formal
parameter @code{balance} is bound to the argument 100.  Within this
environment, we evaluate the body of @code{make-withdraw}, namely the
@code{lambda} expression.  This constructs a new procedure object, whose code
is as specified by the @code{lambda} and whose environment is E1, the
environment in which the @code{lambda} was evaluated to produce the procedure.
The resulting procedure object is the value returned by the call to
@code{make-withdraw}.  This is bound to @code{W1} in the global environment,
since the @code{define} itself is being evaluated in the global environment.
@ref{Figure 3-7} shows the resulting environment structure.

@quotation
@strong{@anchor{Figure 3-7}Figure 3.7:} Result of evaluating @code{(define W1
(make-withdraw 100))}.

@example
          +-----------------------------------------------+
          | make-withdraw: -----------------------+       |
global -->|                                       |       |
          | W1: --+                               |       |
          +-------|-------------------------------|-------+
                  |                ^              |     ^
                  |                |              V     |
                  |        +-------+------+   .---.---. |
                  |  E1 -->| balance: 100 |   | O | O-+-+
                  |        +--------------+   `-|-^---'
                  V                ^            |
              .---.---.            |            V
            +-+-O | O-+------------+    parameters: balance
            | `---^---'                 body: ...
            V
    parameters: amount
    body: (if (>= balance amount)
              (begin (set! balance (- balance amount))
                     balance)
              "Insufficient funds")
@end example
@end quotation

Now we can analyze what happens when @code{W1} is applied to an argument:

@lisp
(W1 50)
@i{50}
@end lisp

We begin by constructing a frame in which @code{amount}, the formal parameter
of @code{W1}, is bound to the argument 50.  The crucial point to observe is
that this frame has as its enclosing environment not the global environment,
but rather the environment E1, because this is the environment that is
specified by the @code{W1} procedure object.  Within this new environment, we
evaluate the body of the procedure:

@lisp
(if (>= balance amount)
    (begin (set! balance (- balance amount))
           balance)
    "Insufficient funds")
@end lisp

The resulting environment structure is shown in @ref{Figure 3-8}.  The
expression being evaluated references both @code{amount} and @code{balance}.
@code{Amount} will be found in the first frame in the environment, while
@code{balance} will be found by following the enclosing-environment pointer to
E1.

@quotation
@strong{@anchor{Figure 3-8}Figure 3.8:} Environments created by applying the
procedure object @code{W1}.

@example
          +---------------------------------------------------+
          | make-withdraw: ...                                |
global -->|                                                   |
env       | W1: --+                                           |
          +-------|-------------------------------------------+
                  |               ^
                  |               |
                  |       +-------+------+ Here is the balance
                  | E1 -->| balance: 100 | that will be changed
                  |       +--------------+ by the set!.
                  V               ^   ^
              .---.---.           |   +----+
              | O | O-+-----------+        |
              `-|-^---'             +------+-----+
                |                   | amount: 50 |
                V                   +------------+
      parameters: amount   (if (>= balance amount)
      body: ...                (begin (set! balance
                                            (- balance amount))
                                      balance)
                               "Insufficient funds")
@end example
@end quotation

When the @code{set!} is executed, the binding of @code{balance} in E1 is
changed.  At the completion of the call to @code{W1}, @code{balance} is 50, and
the frame that contains @code{balance} is still pointed to by the procedure
object @code{W1}.  The frame that binds @code{amount} (in which we executed the
code that changed @code{balance}) is no longer relevant, since the procedure
call that constructed it has terminated, and there are no pointers to that
frame from other parts of the environment.  The next time @code{W1} is called,
this will build a new frame that binds @code{amount} and whose enclosing
environment is E1.  We see that E1 serves as the ``place'' that holds the local
state variable for the procedure object @code{W1}.  @ref{Figure 3-9} shows the
situation after the call to @code{W1}.

@quotation
@strong{@anchor{Figure 3-9}Figure 3.9:} Environments after the call to
@code{W1}.

@example
           +------------------------------------+
           | make-withdraw: ...                 |
global --->|                                    |
env        | W1: --+                            |
           +-------|----------------------------+
                   |                   ^
                   |                   |
                   |            +------+------+
                   |     E1 --->| balance: 50 |
                   |            +-------------+
                   V                   ^
               .---.---.               |
               | O | O-+---------------+
               `-|-^---'
                 |
                 V
          parameters: amount
          body: ...
@end example
@end quotation

Observe what happens when we create a second ``withdraw'' object by making
another call to @code{make-withdraw}:

@lisp
(define W2 (make-withdraw 100))
@end lisp

This produces the environment structure of @ref{Figure 3-10}, which shows that
@code{W2} is a procedure object, that is, a pair with some code and an
environment.  The environment E2 for @code{W2} was created by the call to
@code{make-withdraw}.  It contains a frame with its own local binding for
@code{balance}.  On the other hand, @code{W1} and @code{W2} have the same code:
the code specified by the @code{lambda} expression in the body of
@code{make-withdraw}.@footnote{Whether @code{W1} and @code{W2} share the same
physical code stored in the computer, or whether they each keep a copy of the
code, is a detail of the implementation.  For the interpreter we implement in
@ref{Chapter 4}, the code is in fact shared.} We see here why @code{W1} and
@code{W2} behave as independent objects.  Calls to @code{W1} reference the
state variable @code{balance} stored in E1, whereas calls to @code{W2}
reference the @code{balance} stored in E2. Thus, changes to the local state of
one object do not affect the other object.

@quotation
@strong{@anchor{Figure 3-10}Figure 3.10:} Using @code{(define W2 (make-withdraw
100))} to create a second object.

@example
         +-------------------------------------------------+
         | make-withdraw: ...                              |
global ->| W2: ---------------------------+                |
env      | W1: --+                        |                |
         +-------|------------------------|----------------+
                 |              ^         |              ^
                 |              |         |              |
                 |       +------+------+  |       +------+-------+
                 |  E1 ->| balance: 50 |  |  E2 ->| balance: 100 |
                 |       +-------------+  |       +--------------+
                 V              ^         V              ^
             .---.---.          |     .---.---.          |
             | O | O-+----------+     | O | O-+----------+
             `-|-^---'                `-|-^---'
               | +----------------------+
               V V
        parameters: amount
        body: ...
@end example
@end quotation

@quotation
@strong{@anchor{Exercise 3-10}Exercise 3.10:} In the @code{make-withdraw}
procedure, the local variable @code{balance} is created as a parameter of
@code{make-withdraw}.  We could also create the local state variable
explicitly, using @code{let}, as follows:

@lisp
(define (make-withdraw initial-amount)
  (let ((balance initial-amount))
    (lambda (amount)
      (if (>= balance amount)
          (begin (set! balance (- balance amount))
                 balance)
          "Insufficient funds"))))
@end lisp

Recall from section @ref{1-3-2} that @code{let} is simply syntactic sugar for a
procedure call:

@lisp
(let ((<@var{var}> <@var{exp}>)) <@var{body}>)
@end lisp

@noindent
is interpreted as an alternate syntax for

@lisp
((lambda (<@var{var}>) <@var{body}>) <@var{exp}>)
@end lisp

Use the environment model to analyze this alternate version of
@code{make-withdraw}, drawing figures like the ones above to illustrate the
interactions

@lisp
(define W1 (make-withdraw 100))

(W1 50)

(define W2 (make-withdraw 100))
@end lisp

Show that the two versions of @code{make-withdraw} create objects with the same
behavior.  How do the environment structures differ for the two versions?
@end quotation

@node	3-2-4,  , 3-2-3, 3-2
@subsection Internal Definitions

Section @ref{1-1-8} introduced the idea that procedures can have internal
definitions, thus leading to a block structure as in the following procedure to
compute square roots:

@lisp
(define (sqrt x)
  (define (good-enough? guess)
    (< (abs (- (square guess) x)) 0.001))
  (define (improve guess)
    (average guess (/ x guess)))
  (define (sqrt-iter guess)
    (if (good-enough? guess)
        guess
        (sqrt-iter (improve guess))))
  (sqrt-iter 1.0))
@end lisp

Now we can use the environment model to see why these internal definitions
behave as desired.  @ref{Figure 3-11} shows the point in the evaluation of the
expression @code{(sqrt 2)} where the internal procedure @code{good-enough?} has
been called for the first time with @code{guess} equal to 1.

@quotation
@strong{@anchor{Figure 3-11}Figure 3.11:} @code{Sqrt} procedure with internal
definitions.

@example
          +--------------------------------------------------+
global -->| sqrt: --+                                        |
env       |         |                                        |
          +---------|----------------------------------------+
                    V       ^                   ^
                .---.---.   |                   |
     +----------+-O | O-+---+        +----------+------------+
     |          `---^---'            | x: 2                  |
     V                         E1 -->| good-enough?: -+      |
parameters: x                        | improve: ...   |      |
body: (define good-enough? ...)      | sqrt-iter: ... |      |
      (define improve ...)           +----------------|------+
      (define sqrt-iter ...)          ^  ^            |     ^
      (sqrt-iter 1.0)                 |  |            V     |
                            +---------++ |        .---.---. |
                      E2 -->| guess: 1 | |        | O | O-+-+
                            +----------+ |        `-|-^---'
                      call to sqrt-iter  |          |
                                         |          V
                               +---------++    parameters: guess
                         E3 -->| guess: 1 |    body: (< (abs ...)
                               +----------+             ...)
                         call to good-enough?
@end example
@end quotation

Observe the structure of the environment.  @code{Sqrt} is a symbol in the
global environment that is bound to a procedure object whose associated
environment is the global environment.  When @code{sqrt} was called, a new
environment E1 was formed, subordinate to the global environment, in which the
parameter @code{x} is bound to 2.  The body of @code{sqrt} was then evaluated
in E1.  Since the first expression in the body of @code{sqrt} is

@lisp
(define (good-enough? guess)
  (< (abs (- (square guess) x)) 0.001))
@end lisp

@noindent
evaluating this expression defined the procedure @code{good-enough?}  in the
environment E1.  To be more precise, the symbol @code{good-enough?} was added
to the first frame of E1, bound to a procedure object whose associated
environment is E1.  Similarly, @code{improve} and @code{sqrt-iter} were defined
as procedures in E1.  For conciseness, @ref{Figure 3-11} shows only the
procedure object for @code{good-enough?}.

After the local procedures were defined, the expression @code{(sqrt-iter 1.0)}
was evaluated, still in environment E1.  So the procedure object bound to
@code{sqrt-iter} in E1 was called with 1 as an argument.  This created an
environment E2 in which @code{guess}, the parameter of @code{sqrt-iter}, is
bound to 1.  @code{Sqrt-iter} in turn called @code{good-enough?} with the value
of @code{guess} (from E2) as the argument for @code{good-enough?}.  This set up
another environment, E3, in which @code{guess} (the parameter of
@code{good-enough?}) is bound to 1.  Although @code{sqrt-iter} and
@code{good-enough?} both have a parameter named @code{guess}, these are two
distinct local variables located in different frames.  Also, E2 and E3 both
have E1 as their enclosing environment, because the @code{sqrt-iter} and
@code{good-enough?} procedures both have E1 as their environment part.  One
consequence of this is that the symbol @code{x} that appears in the body of
@code{good-enough?} will reference the binding of @code{x} that appears in E1,
namely the value of @code{x} with which the original @code{sqrt} procedure was
called.

The environment model thus explains the two key properties that make local
procedure definitions a useful technique for modularizing programs:

@itemize @bullet

@item
The names of the local procedures do not interfere with names external to the
enclosing procedure, because the local procedure names will be bound in the
frame that the procedure creates when it is run, rather than being bound in the
global environment.

@item
The local procedures can access the arguments of the enclosing procedure,
simply by using parameter names as free variables.  This is because the body of
the local procedure is evaluated in an environment that is subordinate to the
evaluation environment for the enclosing procedure.

@end itemize

@quotation
@strong{@anchor{Exercise 3-11}Exercise 3.11:} In section @ref{3-2-3} we saw how
the environment model described the behavior of procedures with local state.
Now we have seen how internal definitions work.  A typical message-passing
procedure contains both of these aspects.  Consider the bank account procedure
of section @ref{3-1-1}:

@lisp
(define (make-account balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  (define (dispatch m)
    (cond ((eq? m 'withdraw) withdraw)
          ((eq? m 'deposit) deposit)
          (else (error "Unknown request -- MAKE-ACCOUNT"
                       m))))
  dispatch)
@end lisp

Show the environment structure generated by the sequence of interactions

@lisp
(define acc (make-account 50))

((acc 'deposit) 40)
@i{90}

((acc 'withdraw) 60)
@i{30}
@end lisp

Where is the local state for @code{acc} kept?  Suppose we define another
account

@lisp
(define acc2 (make-account 100))
@end lisp

How are the local states for the two accounts kept distinct?  Which parts of
the environment structure are shared between @code{acc} and @code{acc2}?
@end quotation

@node	3-3, 3-4, 3-2, Chapter 3
@section Modeling with Mutable Data

Chapter 2 dealt with compound data as a means for constructing computational
objects that have several parts, in order to model real-world objects that have
several aspects.  In that chapter we introduced the discipline of data
abstraction, according to which data structures are specified in terms of
constructors, which create data objects, and selectors, which access the parts
of compound data objects.  But we now know that there is another aspect of data
that @ref{Chapter 2} did not address.  The desire to model systems composed of
objects that have changing state leads us to the need to modify compound data
objects, as well as to construct and select from them.  In order to model
compound objects with changing state, we will design data abstractions to
include, in addition to selectors and constructors, operations called
@newterm{mutators}, which modify data objects.  For instance, modeling a
banking system requires us to change account balances.  Thus, a data structure
for representing bank accounts might admit an operation

@lisp
(set-balance! <@var{account}> <@var{new-value}>)
@end lisp

@noindent
that changes the balance of the designated account to the designated new value.
Data objects for which mutators are defined are known as @newterm{mutable data
objects}.

@ref{Chapter 2} introduced pairs as a general-purpose ``glue'' for synthesizing
compound data.  We begin this section by defining basic mutators for pairs, so
that pairs can serve as building blocks for constructing mutable data objects.
These mutators greatly enhance the representational power of pairs, enabling us
to build data structures other than the sequences and trees that we worked with
in section @ref{2-2}.  We also present some examples of simulations in which
complex systems are modeled as collections of objects with local state.

@menu
* 3-3-1::            Mutable List Structure
* 3-3-2::            Representing Queues
* 3-3-3::            Representing Tables
* 3-3-4::            A Simulator for Digital Circuits
* 3-3-5::            Propagation of Constraints
@end menu

@node	3-3-1, 3-3-2, 3-3, 3-3
@subsection Mutable List Structure

The basic operations on pairs---@code{cons}, @code{car}, and @code{cdr}---can
be used to construct list structure and to select parts from list structure,
but they are incapable of modifying list structure.  The same is true of the
list operations we have used so far, such as @code{append} and @code{list},
since these can be defined in terms of @code{cons}, @code{car}, and @code{cdr}.
To modify list structures we need new operations.

@quotation
@strong{@anchor{Figure 3-12}Figure 3.12:} Lists @code{x}: @code{((a b) c d)}
and @code{y}: @code{(e f)}.

@example
     +---+---+     +---+---+     +---+---+
x -->| * | *-+---->| * | *-+---->| * | / |
     +-|-+---+     +-|-+---+     +-|-+---+
       |             V             V
       |           +---+         +---+
       |           | c |         | d |
       |           +---+         +---+
       |           +---+---+     +---+---+
       +---------->| * | *-+---->| * | / |
                   +-|-+---+     +-|-+---+
                     V             V
                   +---+         +---+
                   | a |         | b |
                   +---+         +---+
                   +---+---+     +---+---+
              y -->| * | *-+---->| * | / |
                   +-|-+---+     +-|-+---+
                     V             V
                   +---+         +---+
                   | e |         | f |
                   +---+         +---+
@end example
@end quotation

@quotation
@strong{@anchor{Figure 3-13}Figure 3.13:} Effect of @code{(set-car! x y)} on
the lists in @ref{Figure 3-12}.

@example
     +---+---+     +---+---+     +---+---+
x -->| * | *-+---->| * | *-+---->| * | / |
     +-|-+---+     +-|-+---+     +-|-+---+
       |             V             V
       |           +---+         +---+
       |           | c |         | d |
       |           +---+         +---+
       |           +---+---+     +---+---+
       |           | * | *-+---->| * | / |
       |           +-|-+---+     +-|-+---+
       |             V             V
       |           +---+         +---+
       |           | a |         | b |
       |           +---+         +---+
       +---------->+---+---+     +---+---+
                   | * | *-+---->| * | / |
              y -->+-|-+---+     +-|-+---+
                     V             V
                   +---+         +---+
                   | e |         | f |
                   +---+         +---+
@end example
@end quotation

@quotation
@strong{@anchor{Figure 3-14}Figure 3.14:} Effect of @code{(define z (cons y
(cdr x)))} on the lists in @ref{Figure 3-12}.

@example
     +---+---+     +---+---+     +---+---+
x -->| * | *-+---->| * | *-+---->| * | / |
     +-|-+---+ +-->+-|-+---+     +-|-+---+
       |       |     V             V
       |       |   +---+         +---+
       |       |   | c |         | d |
       |       |   +---+         +---+
       |       |   +---+---+     +---+---+
       +-------+-->| * | *-+---->| * | / |
               |   +-|-+---+     +-|-+---+
     +---+---+ |     V             V
z -->| * | *-+-+   +---+         +---+
     +-|-+---+     | a |         | b |
       |           +---+         +---+
       +---------->+---+---+     +---+---+
                   | * | *-+---->| * | / |
              y -->+-|-+---+     +-|-+---+
                     V             V
                   +---+         +---+
                   | e |         | f |
                   +---+         +---+
@end example
@end quotation

@quotation
@strong{@anchor{Figure 3-15}Figure 3.15:} Effect of @code{(set-cdr! x y)} on
the lists in @ref{Figure 3-12}.

@example
     +---+---+     +---+---+     +---+---+
x -->| * | * |     | * | *-+---->| * | / |
     +-|-+-|-+     +-|-+---+     +-|-+---+
       |   |         V             V
       |   |       +---+         +---+
       |   |       | c |         | d |
       |   |       +---+         +---+
       |   |       +---+---+     +---+---+
       +---+------>| * | *-+---->| * | / |
           |       +-|-+---+     +-|-+---+
           |         V             V
           |       +---+         +---+
           |       | a |         | b |
           |       +---+         +---+
           +------>+---+---+     +---+---+
                   | * | *-+---->| * | / |
              y -->+-|-+---+     +-|-+---+
                     V             V
                   +---+         +---+
                   | e |         | f |
                   +---+         +---+
@end example
@end quotation

The primitive mutators for pairs are @code{set-car!} and
@code{set-cdr!}. @code{Set-car!} takes two arguments, the first of which must
be a pair.  It modifies this pair, replacing the @code{car} pointer by a
pointer to the second argument of @code{set-car!}.@footnote{@code{Set-car!} and
@code{set-cdr!} return implementation-dependent values.  Like @code{set!}, they
should be used only for their effect.}

As an example, suppose that @code{x} is bound to the list @code{((a b) c d)}
and @code{y} to the list @code{(e f)} as illustrated in @ref{Figure 3-12}.
Evaluating the expression @code{ (set-car!  x y)} modifies the pair to which
@code{x} is bound, replacing its @code{car} by the value of @code{y}.  The
result of the operation is shown in @ref{Figure 3-13}.  The structure @code{x}
has been modified and would now be printed as @code{((e f) c d)}.  The pairs
representing the list @code{(a b)}, identified by the pointer that was
replaced, are now detached from the original structure.@footnote{We see from
this that mutation operations on lists can create ``garbage'' that is not part
of any accessible structure.  We will see in section @ref{5-3-2} that Lisp
memory-management systems include a @newterm{garbage collector}, which
identifies and recycles the memory space used by unneeded pairs.}

Compare @ref{Figure 3-13} with @ref{Figure 3-14}, which illustrates the result
of executing @code{(define z (cons y (cdr x)))} with @code{x} and @code{y}
bound to the original lists of @ref{Figure 3-12}.  The variable @code{z} is now
bound to a new pair created by the @code{cons} operation; the list to which
@code{x} is bound is unchanged.

The @code{set-cdr!} operation is similar to @code{set-car!}.  The only
difference is that the @code{cdr} pointer of the pair, rather than the
@code{car} pointer, is replaced.  The effect of executing @code{(set-cdr! x y)}
on the lists of @ref{Figure 3-12} is shown in @ref{Figure 3-15}.  Here the
@code{cdr} pointer of @code{x} has been replaced by the pointer to @code{(e
f)}.  Also, the list @code{(c d)}, which used to be the @code{cdr} of @code{x},
is now detached from the structure.

@code{Cons} builds new list structure by creating new pairs, while
@code{set-car!} and @code{set-cdr!} modify existing pairs.  Indeed, we could
implement @code{cons} in terms of the two mutators, together with a procedure
@code{get-new-pair}, which returns a new pair that is not part of any existing
list structure.  We obtain the new pair, set its @code{car} and @code{cdr}
pointers to the designated objects, and return the new pair as the result of
the @code{cons}.@footnote{@code{Get-new-pair} is one of the operations that
must be implemented as part of the memory management required by a Lisp
implementation.  We will discuss this in section @ref{5-3-1}.}

@lisp
(define (cons x y)
  (let ((new (get-new-pair)))
    (set-car! new x)
    (set-cdr! new y)
    new))
@end lisp

@quotation
@strong{@anchor{Exercise 3-12}Exercise 3.12:} The following procedure for
appending lists was introduced in section @ref{2-2-1}:

@lisp
(define (append x y)
  (if (null? x)
      y
      (cons (car x) (append (cdr x) y))))
@end lisp

@code{Append} forms a new list by successively @code{cons}ing the elements of
@code{x} onto @code{y}.  The procedure @code{append!} is similar to
@code{append}, but it is a mutator rather than a constructor.  It appends the
lists by splicing them together, modifying the final pair of @code{x} so that
its @code{cdr} is now @code{y}.  (It is an error to call @code{append!} with an
empty @code{x}.)

@lisp
(define (append! x y)
  (set-cdr! (last-pair x) y)
  x)
@end lisp

Here @code{last-pair} is a procedure that returns the last pair in its
argument:

@lisp
(define (last-pair x)
  (if (null? (cdr x))
      x
      (last-pair (cdr x))))
@end lisp

Consider the interaction

@lisp
(define x (list 'a 'b))

(define y (list 'c 'd))

(define z (append x y))

z
@i{(a b c d)}

(cdr x)
<@var{response}>

(define w (append! x y))

w
@i{(a b c d)}

(cdr x)
<@var{response}>
@end lisp

What are the missing <@var{response}>s?  Draw box-and-pointer diagrams to
explain your answer.
@end quotation

@quotation
@strong{@anchor{Exercise 3-13}Exercise 3.13:} Consider the following
@code{make-cycle} procedure, which uses the @code{last-pair} procedure defined
in @ref{Exercise 3-12}:

@lisp
(define (make-cycle x)
  (set-cdr! (last-pair x) x)
  x)
@end lisp

Draw a box-and-pointer diagram that shows the structure @code{z} created by

@lisp
(define z (make-cycle (list 'a 'b 'c)))
@end lisp

What happens if we try to compute @code{(last-pair z)}?
@end quotation

@quotation
@strong{@anchor{Exercise 3-14}Exercise 3.14:} The following procedure is quite
useful, although obscure:

@lisp
(define (mystery x)
  (define (loop x y)
    (if (null? x)
        y
        (let ((temp (cdr x)))
          (set-cdr! x y)
          (loop temp x))))
  (loop x '()))
@end lisp

@code{Loop} uses the ``temporary'' variable @code{temp} to hold the old value
of the @code{cdr} of @code{x}, since the @code{set-cdr!}  on the next line
destroys the @code{cdr}.  Explain what @code{mystery} does in general.  Suppose
@code{v} is defined by @code{(define v (list 'a 'b 'c 'd))}. Draw the
box-and-pointer diagram that represents the list to which @code{v} is bound.
Suppose that we now evaluate @code{(define w (mystery v))}. Draw
box-and-pointer diagrams that show the structures @code{v} and @code{w} after
evaluating this expression.  What would be printed as the values of @code{v}
and @code{w}?
@end quotation

@subsubheading Sharing and identity

We mentioned in section @ref{3-1-3} the theoretical issues of ``sameness'' and
``change'' raised by the introduction of assignment.  These issues arise in
practice when individual pairs are @newterm{shared} among different data
objects.  For example, consider the structure formed by

@lisp
(define x (list 'a 'b))
(define z1 (cons x x))
@end lisp

As shown in @ref{Figure 3-16}, @code{z1} is a pair whose @code{car} and
@code{cdr} both point to the same pair @code{x}.  This sharing of @code{x} by
the @code{car} and @code{cdr} of @code{z1} is a consequence of the
straightforward way in which @code{cons} is implemented.  In general, using
@code{cons} to construct lists will result in an interlinked structure of pairs
in which many individual pairs are shared by many different structures.

@quotation
@strong{@anchor{Figure 3-16}Figure 3.16:} The list @code{z1} formed by
@code{(cons x x)}.

@example
      +---+---+
z1 -->| * | * |
      +-|-+-|-+
        V   V
      +---+---+     +---+---+
 x -->| * | *-+---->| * | / |
      +-|-+---+     +-|-+---+
        V             V
      +---+         +---+
      | a |         | b |
      +---+         +---+
@end example
@end quotation

@quotation
@strong{@anchor{Figure 3-17}Figure 3.17:} The list @code{z2} formed by
@code{(cons (list 'a 'b) (list 'a 'b))}.

@example
      +---+---+     +---+---+     +---+---+
z2 -->| * | *-+---->| * | *-+---->| * | / |
      +-|-+---+     +-|-+---+     +-|-+---+
        |             V             V
        |           +---+         +---+
        |           | a |         | b |
        |           +---+         +---+
        |             ^             ^
        |             |             |
        |           +-|-+---+     +-|-+---+
        +---------->| * | *-+---->| * | / |
                    +---+---+     +---+---+
@end example
@end quotation

In contrast to @ref{Figure 3-16}, @ref{Figure 3-17} shows the structure created
by

@lisp
(define z2 (cons (list 'a 'b) (list 'a 'b)))
@end lisp

In this structure, the pairs in the two @code{(a b)} lists are distinct,
although the actual symbols are shared.@footnote{The two pairs are distinct
because each call to @code{cons} returns a new pair.  The symbols are shared;
in Scheme there is a unique symbol with any given name.  Since Scheme provides
no way to mutate a symbol, this sharing is undetectable.  Note also that the
sharing is what enables us to compare symbols using @code{eq?}, which simply
checks equality of pointers.}

When thought of as a list, @code{z1} and @code{z2} both represent ``the same''
list, @code{((a b) a b)}.  In general, sharing is completely undetectable if we
operate on lists using only @code{cons}, @code{car}, and @code{cdr}.  However,
if we allow mutators on list structure, sharing becomes significant.  As an
example of the difference that sharing can make, consider the following
procedure, which modifies the @code{car} of the structure to which it is
applied:

@lisp
(define (set-to-wow! x)
  (set-car! (car x) 'wow)
  x)
@end lisp

Even though @code{z1} and @code{z2} are ``the same'' structure, applying
@code{set-to-wow!} to them yields different results.  With @code{z1}, altering
the @code{car} also changes the @code{cdr}, because in @code{z1} the @code{car}
and the @code{cdr} are the same pair.  With @code{z2}, the @code{car} and
@code{cdr} are distinct, so @code{set-to-wow!} modifies only the @code{car}:

@lisp
z1
@i{((a b) a b)}

(set-to-wow! z1)
@i{((wow b) wow b)}

z2
@i{((a b) a b)}

(set-to-wow! z2)
@i{((wow b) a b)}
@end lisp

One way to detect sharing in list structures is to use the predicate
@code{eq?}, which we introduced in section @ref{2-3-1} as a way to test whether
two symbols are equal.  More generally, @code{(eq?  x y)} tests whether
@code{x} and @code{y} are the same object (that is, whether @code{x} and
@code{y} are equal as pointers).  Thus, with @code{z1} and @code{z2} as defined
in figures @ref{Figure 3-16} and @ref{Figure 3-17}, @code{(eq?  (car z1) (cdr
z1))} is true and @code{(eq? (car z2) (cdr z2))} is false.

As will be seen in the following sections, we can exploit sharing to greatly
extend the repertoire of data structures that can be represented by pairs.  On
the other hand, sharing can also be dangerous, since modifications made to
structures will also affect other structures that happen to share the modified
parts.  The mutation operations @code{set-car!} and @code{set-cdr!} should be
used with care; unless we have a good understanding of how our data objects are
shared, mutation can have unanticipated results.@footnote{The subtleties of
dealing with sharing of mutable data objects reflect the underlying issues of
``sameness'' and ``change'' that were raised in section @ref{3-1-3}.  We
mentioned there that admitting change to our language requires that a compound
object must have an ``identity'' that is something different from the pieces
from which it is composed.  In Lisp, we consider this ``identity'' to be the
quality that is tested by @code{eq?}, i.e., by equality of pointers.  Since in
most Lisp implementations a pointer is essentially a memory address, we are
``solving the problem'' of defining the identity of objects by stipulating that
a data object ``itself'' is the information stored in some particular set of
memory locations in the computer.  This suffices for simple Lisp programs, but
is hardly a general way to resolve the issue of ``sameness'' in computational
models.}

@quotation
@strong{@anchor{Exercise 3-15}Exercise 3.15:} Draw box-and-pointer diagrams to
explain the effect of @code{set-to-wow!} on the structures @code{z1} and
@code{z2} above.
@end quotation

@quotation
@strong{@anchor{Exercise 3-16}Exercise 3.16:} Ben Bitdiddle decides to write a
procedure to count the number of pairs in any list structure.  ``It's easy,''
he reasons.  ``The number of pairs in any structure is the number in the
@code{car} plus the number in the @code{cdr} plus one more to count the current
pair.''  So Ben writes the following procedure:

@lisp
(define (count-pairs x)
  (if (not (pair? x))
      0
      (+ (count-pairs (car x))
         (count-pairs (cdr x))
         1)))
@end lisp

Show that this procedure is not correct.  In particular, draw box-and-pointer
diagrams representing list structures made up of exactly three pairs for which
Ben's procedure would return 3; return 4; return 7; never return at all.
@end quotation

@quotation
@strong{@anchor{Exercise 3-17}Exercise 3.17:} Devise a correct version of the
@code{count-pairs} procedure of @ref{Exercise 3-16} that returns the number of
distinct pairs in any structure.  (Hint: Traverse the structure, maintaining an
auxiliary data structure that is used to keep track of which pairs have already
been counted.)
@end quotation

@quotation
@strong{@anchor{Exercise 3-18}Exercise 3.18:} Write a procedure that examines a
list and determines whether it contains a cycle, that is, whether a program
that tried to find the end of the list by taking successive @code{cdr}s would
go into an infinite loop.  @ref{Exercise 3-13} constructed such lists.
@end quotation

@quotation
@strong{@anchor{Exercise 3-19}Exercise 3.19:} Redo @ref{Exercise 3-18} using an
algorithm that takes only a constant amount of space.  (This requires a very
clever idea.)
@end quotation

@subsubheading Mutation is just assignment

When we introduced compound data, we observed in section @ref{2-1-3} that pairs
can be represented purely in terms of procedures:

@lisp
(define (cons x y)
  (define (dispatch m)
    (cond ((eq? m 'car) x)
          ((eq? m 'cdr) y)
          (else (error "Undefined operation -- CONS" m))))
  dispatch)

(define (car z) (z 'car))

(define (cdr z) (z 'cdr))
@end lisp

The same observation is true for mutable data.  We can implement mutable data
objects as procedures using assignment and local state.  For instance, we can
extend the above pair implementation to handle @code{set-car!} and
@code{set-cdr!} in a manner analogous to the way we implemented bank accounts
using @code{make-account} in section @ref{3-1-1}:

@lisp
(define (cons x y)
  (define (set-x! v) (set! x v))
  (define (set-y! v) (set! y v))
  (define (dispatch m)
    (cond ((eq? m 'car) x)
          ((eq? m 'cdr) y)
          ((eq? m 'set-car!) set-x!)
          ((eq? m 'set-cdr!) set-y!)
          (else (error "Undefined operation -- CONS" m))))
  dispatch)

(define (car z) (z 'car))

(define (cdr z) (z 'cdr))

(define (set-car! z new-value)
  ((z 'set-car!) new-value)
  z)

(define (set-cdr! z new-value)
  ((z 'set-cdr!) new-value)
  z)
@end lisp

Assignment is all that is needed, theoretically, to account for the behavior of
mutable data.  As soon as we admit @code{set!} to our language, we raise all
the issues, not only of assignment, but of mutable data in general.@footnote{On
the other hand, from the viewpoint of implementation, assignment requires us to
modify the environment, which is itself a mutable data structure.  Thus,
assignment and mutation are equipotent: Each can be implemented in terms of the
other.}

@quotation
@strong{@anchor{Exercise 3-20}Exercise 3.20:} Draw environment diagrams to
illustrate the evaluation of the sequence of expressions

@lisp
(define x (cons 1 2))
(define z (cons x x))
(set-car! (cdr z) 17)

(car x)
@i{17}
@end lisp

@noindent
using the procedural implementation of pairs given above.  (Compare
@ref{Exercise 3-11}.)
@end quotation

@node	3-3-2, 3-3-3, 3-3-1, 3-3
@subsection Representing Queues

The mutators @code{set-car!} and @code{set-cdr!} enable us to use pairs to
construct data structures that cannot be built with @code{cons}, @code{car},
and @code{cdr} alone.  This section shows how to use pairs to represent a data
structure called a queue.  Section @ref{3-3-3} will show how to represent data
structures called tables.

A @newterm{queue} is a sequence in which items are inserted at one end (called
the @newterm{rear} of the queue) and deleted from the other end (the
@newterm{front}).  @ref{Figure 3-18} shows an initially empty queue in which
the items @code{a} and @code{b} are inserted.  Then @code{a} is removed,
@code{c} and @code{d} are inserted, and @code{b} is removed.  Because items are
always removed in the order in which they are inserted, a queue is sometimes
called a @newterm{FIFO} (first in, first out) buffer.

@quotation
@strong{@anchor{Figure 3-18}Figure 3.18:} Queue operations.

@example
Operation                Resulting Queue
(define q (make-queue))
(insert-queue! q 'a)     a
(insert-queue! q 'b)     a b
(delete-queue! q)        b
(insert-queue! q 'c)     b c
(insert-queue! q 'd)     b c d
(delete-queue! q)        c d
@end example
@end quotation

In terms of data abstraction, we can regard a queue as defined by the following
set of operations:

@itemize @bullet

@item
a constructor: @code{(make-queue)} returns an empty queue (a queue containing
no items).

@item
two selectors:

@lisp
(empty-queue? <@var{queue}>)
@end lisp

@noindent
tests if the queue is empty.

@lisp
(front-queue <@var{queue}>)
@end lisp

@noindent
returns the object at the front of the queue, signaling an error if the queue
is empty; it does not modify the queue.

@item
two mutators:

@lisp
(insert-queue! <@var{queue}> <@var{item}>)
@end lisp

@noindent
inserts the item at the rear of the queue and returns the modified queue as its
value.

@lisp
(delete-queue! <@var{queue}>)
@end lisp

@noindent
removes the item at the front of the queue and returns the modified queue as
its value, signaling an error if the queue is empty before the deletion.

@end itemize

Because a queue is a sequence of items, we could certainly represent it as an
ordinary list; the front of the queue would be the @code{car} of the list,
inserting an item in the queue would amount to appending a new element at the
end of the list, and deleting an item from the queue would just be taking the
@code{cdr} of the list.  However, this representation is inefficient, because
in order to insert an item we must scan the list until we reach the end.  Since
the only method we have for scanning a list is by successive @code{cdr}
operations, this scanning requires @i{[theta]}(@i{n}) steps for a list of @i{n}
items.  A simple modification to the list representation overcomes this
disadvantage by allowing the queue operations to be implemented so that they
require @i{[theta]}(1) steps; that is, so that the number of steps needed is
independent of the length of the queue.

The difficulty with the list representation arises from the need to scan to
find the end of the list.  The reason we need to scan is that, although the
standard way of representing a list as a chain of pairs readily provides us
with a pointer to the beginning of the list, it gives us no easily accessible
pointer to the end.  The modification that avoids the drawback is to represent
the queue as a list, together with an additional pointer that indicates the
final pair in the list.  That way, when we go to insert an item, we can consult
the rear pointer and so avoid scanning the list.

A queue is represented, then, as a pair of pointers, @code{front-ptr} and
@code{rear-ptr}, which indicate, respectively, the first and last pairs in an
ordinary list.  Since we would like the queue to be an identifiable object, we
can use @code{cons} to combine the two pointers.  Thus, the queue itself will
be the @code{cons} of the two pointers.  @ref{Figure 3-19} illustrates this
representation.

@quotation
@strong{@anchor{Figure 3-19}Figure 3.19:} Implementation of a queue as a list
with front and rear pointers.

@example
       +---+---+
  q -->| * | *-+-------------------+
       +-|-+---+                   |
         |                         |
         | front-ptr               | rear-ptr
         V                         V
     +---+---+    +---+---+    +---+---+
     | * | *-+--->| * | *-+--->| * | / |
     +-|-+---+    +-|-+---+    +-|-+---+
       V            V            V
     +---+        +---+        +---+
     | a |        | b |        | c |
     +---+        +---+        +---+
@end example
@end quotation

To define the queue operations we use the following procedures, which enable us
to select and to modify the front and rear pointers of a queue:

@lisp
(define (front-ptr queue) (car queue))

(define (rear-ptr queue) (cdr queue))

(define (set-front-ptr! queue item) (set-car! queue item))

(define (set-rear-ptr! queue item) (set-cdr! queue item))
@end lisp

Now we can implement the actual queue operations.  We will consider a queue to
be empty if its front pointer is the empty list:

@lisp
(define (empty-queue? queue) (null? (front-ptr queue)))
@end lisp

The @code{make-queue} constructor returns, as an initially empty queue, a pair
whose @code{car} and @code{cdr} are both the empty list:

@lisp
(define (make-queue) (cons '() '()))
@end lisp

To select the item at the front of the queue, we return the @code{car} of the
pair indicated by the front pointer:

@lisp
(define (front-queue queue)
  (if (empty-queue? queue)
      (error "FRONT called with an empty queue" queue)
      (car (front-ptr queue))))
@end lisp

To insert an item in a queue, we follow the method whose result is indicated in
@ref{Figure 3-20}.  We first create a new pair whose @code{car} is the item to
be inserted and whose @code{cdr} is the empty list.  If the queue was initially
empty, we set the front and rear pointers of the queue to this new pair.
Otherwise, we modify the final pair in the queue to point to the new pair, and
also set the rear pointer to the new pair.

@quotation
@strong{@anchor{Figure 3-20}Figure 3.20:} Result of using @code{(insert-queue!
q 'd)} on the queue of @ref{Figure 3-19}.

@example
       +---+---+
  q -->| * | *-+--------------------------------+
       +-|-+---+                                |
         |                                      |
         | front-ptr                            | rear-ptr
         V                                      V
     +---+---+    +---+---+    +---+---+    +---+---+
     | * | *-+--->| * | *-+--->| * | *-+--->| * | / |
     +-|-+---+    +-|-+---+    +-|-+---+    +-|-+---+
       V            V            V            V
     +---+        +---+        +---+        +---+
     | a |        | b |        | c |        | d |
     +---+        +---+        +---+        +---+
@end example
@end quotation

@lisp
(define (insert-queue! queue item)
  (let ((new-pair (cons item '())))
    (cond ((empty-queue? queue)
           (set-front-ptr! queue new-pair)
           (set-rear-ptr! queue new-pair)
           queue)
          (else
           (set-cdr! (rear-ptr queue) new-pair)
           (set-rear-ptr! queue new-pair)
           queue))))
@end lisp

To delete the item at the front of the queue, we merely modify the front
pointer so that it now points at the second item in the queue, which can be
found by following the @code{cdr} pointer of the first item (see @ref{Figure
3-21}):@footnote{If the first item is the final item in the queue, the front
pointer will be the empty list after the deletion, which will mark the queue as
empty; we needn't worry about updating the rear pointer, which will still point
to the deleted item, because @code{empty-queue?} looks only at the front
pointer.}

@quotation
@strong{@anchor{Figure 3-21}Figure 3.21:} Result of using @code{(delete-queue!
q)} on the queue of @ref{Figure 3-20}.

@example
       +---+---+
  q -->| * | *-+--------------------------------+
       +-|-+---+                                |
         +------------+                         |
            front-ptr |                         | rear-ptr
                      V                         V
     +---+---+    +---+---+    +---+---+    +---+---+
     | * | *-+--->| * | *-+--->| * | *-+--->| * | / |
     +-|-+---+    +-|-+---+    +-|-+---+    +-|-+---+
       V            V            V            V
     +---+        +---+        +---+        +---+
     | a |        | b |        | c |        | d |
     +---+        +---+        +---+        +---+
@end example
@end quotation

@lisp
(define (delete-queue! queue)
  (cond ((empty-queue? queue)
         (error "DELETE! called with an empty queue" queue))
        (else
         (set-front-ptr! queue (cdr (front-ptr queue)))
         queue)))
@end lisp

@quotation
@strong{@anchor{Exercise 3-21}Exercise 3.21:} Ben Bitdiddle decides to test the
queue implementation described above.  He types in the procedures to the Lisp
interpreter and proceeds to try them out:

@lisp
(define q1 (make-queue))

(insert-queue! q1 'a)
@i{((a) a)}

(insert-queue! q1 'b)
@i{((a b) b)}

(delete-queue! q1)
@i{((b) b)}

(delete-queue! q1)
@i{(() b)}
@end lisp

``It's all wrong!'' he complains.  ``The interpreter's response shows that the
last item is inserted into the queue twice.  And when I delete both items, the
second @code{b} is still there, so the queue isn't empty, even though it's
supposed to be.''  Eva Lu Ator suggests that Ben has misunderstood what is
happening.  ``It's not that the items are going into the queue twice,'' she
explains.  ``It's just that the standard Lisp printer doesn't know how to make
sense of the queue representation.  If you want to see the queue printed
correctly, you'll have to define your own print procedure for queues.'' Explain
what Eva Lu is talking about.  In particular, show why Ben's examples produce
the printed results that they do.  Define a procedure @code{print-queue} that
takes a queue as input and prints the sequence of items in the queue.
@end quotation

@quotation
@strong{@anchor{Exercise 3-22}Exercise 3.22:} Instead of representing a queue
as a pair of pointers, we can build a queue as a procedure with local state.
The local state will consist of pointers to the beginning and the end of an
ordinary list.  Thus, the @code{make-queue} procedure will have the form

@lisp
(define (make-queue)
  (let ((front-ptr @dots{} )
        (rear-ptr @dots{} ))
    <@var{definitions of internal procedures}>
    (define (dispatch m) @dots{})
    dispatch))
@end lisp

Complete the definition of @code{make-queue} and provide implementations of the
queue operations using this representation.
@end quotation

@quotation
@strong{@anchor{Exercise 3-23}Exercise 3.23:} A @newterm{deque} (``double-ended
queue'') is a sequence in which items can be inserted and deleted at either the
front or the rear.  Operations on deques are the constructor @code{make-deque},
the predicate @code{empty-deque?}, selectors @code{front-deque} and
@code{rear-deque}, and mutators @code{front-insert-deque!},
@code{rear-insert-deque!}, @code{front-delete-deque!}, and
@code{rear-delete-deque!}.  Show how to represent deques using pairs, and give
implementations of the operations.@footnote{Be careful not to make the
interpreter try to print a structure that contains cycles.  (See @ref{Exercise
3-13}.)}  All operations should be accomplished in @i{[theta]}(1) steps.
@end quotation

@node	3-3-3, 3-3-4, 3-3-2, 3-3
@subsection Representing Tables

When we studied various ways of representing sets in @ref{Chapter 2}, we
mentioned in section @ref{2-3-3} the task of maintaining a table of records
indexed by identifying keys.  In the implementation of data-directed
programming in section @ref{2-4-3}, we made extensive use of two-dimensional
tables, in which information is stored and retrieved using two keys.  Here we
see how to build tables as mutable list structures.

We first consider a one-dimensional table, in which each value is stored under
a single key.  We implement the table as a list of records, each of which is
implemented as a pair consisting of a key and the associated value. The records
are glued together to form a list by pairs whose @code{car}s point to
successive records.  These gluing pairs are called the @newterm{backbone} of
the table.  In order to have a place that we can change when we add a new
record to the table, we build the table as a @newterm{headed list}.  A headed
list has a special backbone pair at the beginning, which holds a dummy
``record''---in this case the arbitrarily chosen symbol @code{*table*}.
@ref{Figure 3-22} shows the box-and-pointer diagram for the table

@lisp
a:  1
b:  2
c:  3
@end lisp

@quotation
@strong{@anchor{Figure 3-22}Figure 3.22:} A table represented as a headed list.

@example
 +---+---+    +---+---+    +---+---+    +---+---+
 | * | *-+--->| * | *-+--->| * | *-+--->| * | / |
 +-|-+---+    +-|-+---+    +-|-+---+    +-|-+---+
   |            |            |            |
   V            V            V            V
+---------+   +---+---+   +---+---+   +---+---+
| *table* |   | * | * |   | * | * |   | * | * |
+---------+   +-|-+-|-+   +-|-+-|-+   +-|-+-|-+
                |   |       |   |       |   |
                V   V       V   V       V   V
             +---+ +---+ +---+ +---+ +---+ +---+
             | a | | 1 | | b | | 2 | | c | | 3 |
             +---+ +---+ +---+ +---+ +---+ +---+
@end example
@end quotation

To extract information from a table we use the @code{lookup} procedure, which
takes a key as argument and returns the associated value (or false if there is
no value stored under that key).  @code{Lookup} is defined in terms of the
@code{assoc} operation, which expects a key and a list of records as arguments.
Note that @code{assoc} never sees the dummy record.  @code{Assoc} returns the
record that has the given key as its @code{car}.@footnote{Because @code{assoc}
uses @code{equal?}, it can recognize keys that are symbols, numbers, or list
structure.}  @code{Lookup} then checks to see that the resulting record
returned by @code{assoc} is not false, and returns the value (the @code{cdr})
of the record.

@lisp
(define (lookup key table)
  (let ((record (assoc key (cdr table))))
    (if record
        (cdr record)
        false)))

(define (assoc key records)
  (cond ((null? records) false)
        ((equal? key (caar records)) (car records))
        (else (assoc key (cdr records)))))
@end lisp

To insert a value in a table under a specified key, we first use @code{assoc}
to see if there is already a record in the table with this key.  If not, we
form a new record by @code{cons}ing the key with the value, and insert this at
the head of the table's list of records, after the dummy record.  If there
already is a record with this key, we set the @code{cdr} of this record to the
designated new value.  The header of the table provides us with a fixed
location to modify in order to insert the new record.@footnote{Thus, the first
backbone pair is the object that represents the table ``itself''; that is, a
pointer to the table is a pointer to this pair.  This same backbone pair always
starts the table.  If we did not arrange things in this way, @code{insert!}
would have to return a new value for the start of the table when it added a new
record.}

@lisp
(define (insert! key value table)
  (let ((record (assoc key (cdr table))))
    (if record
        (set-cdr! record value)
        (set-cdr! table
                  (cons (cons key value) (cdr table)))))
  'ok)
@end lisp

To construct a new table, we simply create a list containing the symbol
@code{*table*}:

@lisp
(define (make-table)
  (list '*table*))
@end lisp

@subsubheading Two-dimensional tables

In a two-dimensional table, each value is indexed by two keys.  We can
construct such a table as a one-dimensional table in which each key identifies
a subtable.  @ref{Figure 3-23} shows the box-and-pointer diagram for the table

@example
math:
    +:  43
    -:  45
    *:  42
letters:
    a:  97
    b:  98
@end example

@noindent
which has two subtables.  (The subtables don't need a special header symbol,
since the key that identifies the subtable serves this purpose.)

@quotation
@strong{@anchor{Figure 3-23}Figure 3.23:} A two-dimensional table.

@example
table
  |
  V
+---+---+   +---+---+   +---+---+
| * | *-+-->| * | *-+-->| * | / |
+-|-+---+   +-|-+---+   +-|-+---+
  V           |           V
+-------+     |         +---+---+   +---+---+   +---+---+
|*table*|     |         | * | *-+-->| * | *-+-->| * | / |
+-------+     |         +-|-+---+   +-|-+---+   +-|-+---+
              |           V           V           V
              |       +-------+     +---+---+   +---+---+
              |       |letters|     | * | * |   | * | * |
              |       +-------+     +-|-+-|-+   +-|-+-|-+
              |                       V   V       V   V
              |                    +---+ +---+ +---+ +---+
              |                    | a | | 97| | b | | 98|
              |                    +---+ +---+ +---+ +---+
              V
            +---+---+   +---+---+   +---+---+   +---+---+
            | * | *-+-->| * | *-+-->| * | *-+-->| * | / |
            +-|-+---+   +-|-+---+   +-|-+---+   +-|-+---+
              V           V           V           V
          +------+      +---+---+   +---+---+   +---+---+
          | math |      | * | * |   | * | * |   | * | * |
          +------+      +-|-+-|-+   +-|-+-|-+   +-|-+-|-+
                          V   V       V   V       V   V
                       +---+ +---+ +---+ +---+ +---+ +---+
                       | + | | 43| | - | | 45| | * | | 42|
                       +---+ +---+ +---+ +---+ +---+ +---+
@end example
@end quotation

When we look up an item, we use the first key to identify the correct subtable.
Then we use the second key to identify the record within the subtable.

@lisp
(define (lookup key-1 key-2 table)
  (let ((subtable (assoc key-1 (cdr table))))
    (if subtable
        (let ((record (assoc key-2 (cdr subtable))))
          (if record
              (cdr record)
              false))
        false)))
@end lisp

To insert a new item under a pair of keys, we use @code{assoc} to see if there
is a subtable stored under the first key.  If not, we build a new subtable
containing the single record (@code{key-2}, @code{value}) and insert it into
the table under the first key.  If a subtable already exists for the first key,
we insert the new record into this subtable, using the insertion method for
one-dimensional tables described above:

@lisp
(define (insert! key-1 key-2 value table)
  (let ((subtable (assoc key-1 (cdr table))))
    (if subtable
        (let ((record (assoc key-2 (cdr subtable))))
          (if record
              (set-cdr! record value)
              (set-cdr! subtable
                        (cons (cons key-2 value)
                              (cdr subtable)))))
        (set-cdr! table
                  (cons (list key-1
                              (cons key-2 value))
                        (cdr table)))))
  'ok)
@end lisp

@subsubheading Creating local tables

The @code{lookup} and @code{insert!} operations defined above take the table as
an argument.  This enables us to use programs that access more than one table.
Another way to deal with multiple tables is to have separate @code{lookup} and
@code{insert!} procedures for each table.  We can do this by representing a
table procedurally, as an object that maintains an internal table as part of
its local state.  When sent an appropriate message, this ``table object''
supplies the procedure with which to operate on the internal table.  Here is a
generator for two-dimensional tables represented in this fashion:

@lisp
(define (make-table)
  (let ((local-table (list '*table*)))
    (define (lookup key-1 key-2)
      (let ((subtable (assoc key-1 (cdr local-table))))
        (if subtable
            (let ((record (assoc key-2 (cdr subtable))))
              (if record
                  (cdr record)
                  false))
            false)))
    (define (insert! key-1 key-2 value)
      (let ((subtable (assoc key-1 (cdr local-table))))
        (if subtable
            (let ((record (assoc key-2 (cdr subtable))))
              (if record
                  (set-cdr! record value)
                  (set-cdr! subtable
                            (cons (cons key-2 value)
                                  (cdr subtable)))))
            (set-cdr! local-table
                      (cons (list key-1
                                  (cons key-2 value))
                            (cdr local-table)))))
      'ok)
    (define (dispatch m)
      (cond ((eq? m 'lookup-proc) lookup)
            ((eq? m 'insert-proc!) insert!)
            (else (error "Unknown operation -- TABLE" m))))
    dispatch))
@end lisp

Using @code{make-table}, we could implement the @code{get} and @code{put}
operations used in section @ref{2-4-3} for data-directed programming, as
follows:

@lisp
(define operation-table (make-table))
(define get (operation-table 'lookup-proc))
(define put (operation-table 'insert-proc!))
@end lisp

@code{Get} takes as arguments two keys, and @code{put} takes as arguments two
keys and a value.  Both operations access the same local table, which is
encapsulated within the object created by the call to @code{make-table}.

@quotation
@strong{@anchor{Exercise 3-24}Exercise 3.24:} In the table implementations
above, the keys are tested for equality using @code{equal?} (called by
@code{assoc}).  This is not always the appropriate test.  For instance, we
might have a table with numeric keys in which we don't need an exact match to
the number we're looking up, but only a number within some tolerance of it.
Design a table constructor @code{make-table} that takes as an argument a
@code{same-key?} procedure that will be used to test ``equality'' of keys.
@code{Make-table} should return a @code{dispatch} procedure that can be used to
access appropriate @code{lookup} and @code{insert!} procedures for a local
table.
@end quotation

@quotation
@strong{@anchor{Exercise 3-25}Exercise 3.25:} Generalizing one- and
two-dimensional tables, show how to implement a table in which values are
stored under an arbitrary number of keys and different values may be stored
under different numbers of keys.  The @code{lookup} and @code{insert!}
procedures should take as input a list of keys used to access the table.
@end quotation

@quotation
@strong{@anchor{Exercise 3-26}Exercise 3.26:} To search a table as implemented
above, one needs to scan through the list of records.  This is basically the
unordered list representation of section @ref{2-3-3}.  For large tables, it may
be more efficient to structure the table in a different manner.  Describe a
table implementation where the (key, value) records are organized using a
binary tree, assuming that keys can be ordered in some way (e.g., numerically
or alphabetically).  (Compare @ref{Exercise 2-66} of @ref{Chapter 2}.)
@end quotation

@quotation
@strong{@anchor{Exercise 3-27}Exercise 3.27:} @newterm{Memoization} (also
called @newterm{tabulation}) is a technique that enables a procedure to record,
in a local table, values that have previously been computed.  This technique
can make a vast difference in the performance of a program.  A memoized
procedure maintains a table in which values of previous calls are stored using
as keys the arguments that produced the values.  When the memoized procedure is
asked to compute a value, it first checks the table to see if the value is
already there and, if so, just returns that value.  Otherwise, it computes the
new value in the ordinary way and stores this in the table.  As an example of
memoization, recall from section @ref{1-2-2} the exponential process for
computing Fibonacci numbers:

@lisp
(define (fib n)
  (cond ((= n 0) 0)
        ((= n 1) 1)
        (else (+ (fib (- n 1))
                 (fib (- n 2))))))
@end lisp

The memoized version of the same procedure is

@lisp
(define memo-fib
  (memoize (lambda (n)
             (cond ((= n 0) 0)
                   ((= n 1) 1)
                   (else (+ (memo-fib (- n 1))
                            (memo-fib (- n 2))))))))
@end lisp

@noindent
where the memoizer is defined as

@lisp
(define (memoize f)
  (let ((table (make-table)))
    (lambda (x)
      (let ((previously-computed-result (lookup x table)))
        (or previously-computed-result
            (let ((result (f x)))
              (insert! x result table)
              result))))))
@end lisp

Draw an environment diagram to analyze the computation of @code{(memo-fib 3)}.
Explain why @code{memo-fib} computes the @i{n}th Fibonacci number in a number
of steps proportional to @i{n}.  Would the scheme still work if we had simply
defined @code{memo-fib} to be @code{(memoize fib)}?
@end quotation

@node	3-3-4, 3-3-5, 3-3-3, 3-3
@subsection A Simulator for Digital Circuits

Designing complex digital systems, such as computers, is an important
engineering activity.  Digital systems are constructed by interconnecting
simple elements.  Although the behavior of these individual elements is simple,
networks of them can have very complex behavior.  Computer simulation of
proposed circuit designs is an important tool used by digital systems
engineers.  In this section we design a system for performing digital logic
simulations.  This system typifies a kind of program called an
@newterm{event-driven simulation}, in which actions (``events'') trigger
further events that happen at a later time, which in turn trigger more events,
and so so.

Our computational model of a circuit will be composed of objects that
correspond to the elementary components from which the circuit is constructed.
There are @newterm{wires}, which carry @newterm{digital signals}.  A digital
signal may at any moment have only one of two possible values, 0 and 1.  There
are also various types of digital @newterm{function boxes}, which connect wires
carrying input signals to other output wires.  Such boxes produce output
signals computed from their input signals.  The output signal is delayed by a
time that depends on the type of the function box.  For example, an
@newterm{inverter} is a primitive function box that inverts its input.  If the
input signal to an inverter changes to 0, then one inverter-delay later the
inverter will change its output signal to 1.  If the input signal to an
inverter changes to 1, then one inverter-delay later the inverter will change
its output signal to 0.  We draw an inverter symbolically as in @ref{Figure
3-24}.  An @newterm{and-gate}, also shown in @ref{Figure 3-24}, is a primitive
function box with two inputs and one output.  It drives its output signal to a
value that is the @newterm{logical and} of the inputs.  That is, if both of its
input signals become 1, then one and-gate-delay time later the and-gate will
force its output signal to be 1; otherwise the output will be 0.  An
@newterm{or-gate} is a similar two-input primitive function box that drives its
output signal to a value that is the @newterm{logical or} of the inputs.  That
is, the output will become 1 if at least one of the input signals is 1;
otherwise the output will become 0.

@quotation
@strong{@anchor{Figure 3-24}Figure 3.24:} Primitive functions in the digital
logic simulator.

@example
               __          ___
  |\        --|  \       --\  \
--| >o--      |   )--       )  >--
  |/        --|__/       --/__/

Inverter    And-gate     Or-gate
@end example
@end quotation

We can connect primitive functions together to construct more complex
functions.  To accomplish this we wire the outputs of some function boxes to
the inputs of other function boxes.  For example, the @newterm{half-adder}
circuit shown in @ref{Figure 3-25} consists of an or-gate, two and-gates, and
an inverter.  It takes two input signals, A and B, and has two output signals,
S and C.  S will become 1 whenever precisely one of A and B is 1, and C will
become 1 whenever A and B are both 1.  We can see from the figure that, because
of the delays involved, the outputs may be generated at different times.  Many
of the difficulties in the design of digital circuits arise from this fact.

@quotation
@strong{@anchor{Figure 3-25}Figure 3.25:} A half-adder circuit.

@example
    +--------------------------------------+
    |         ____                         |
A --------*---\   \ D               ___    |
    |     |    >   >---------------|   \   |
    |  +--|---/___/                |    )----- S
    |  |  |              |\  E  +--|___/   |
    |  |  |           +--| >o---+          |
    |  |  |    ___    |  |/                |
    |  |  +---|   \   |                    |
    |  |      |    )--*----------------------- C
B -----*------|___/                        |
    |                                      |
    +--------------------------------------+
@end example
@end quotation

We will now build a program for modeling the digital logic circuits we wish to
study.  The program will construct computational objects modeling the wires,
which will ``hold'' the signals.  Function boxes will be modeled by procedures
that enforce the correct relationships among the signals.

One basic element of our simulation will be a procedure @code{make-wire}, which
constructs wires.  For example, we can construct six wires as follows:

@lisp
(define a (make-wire))
(define b (make-wire))
(define c (make-wire))

(define d (make-wire))
(define e (make-wire))
(define s (make-wire))
@end lisp

We attach a function box to a set of wires by calling a procedure that
constructs that kind of box.  The arguments to the constructor procedure are
the wires to be attached to the box.  For example, given that we can construct
and-gates, or-gates, and inverters, we can wire together the half-adder shown
in @ref{Figure 3-25}:

@lisp
(or-gate a b d)
@i{ok}

(and-gate a b c)
@i{ok}

(inverter c e)
@i{ok}

(and-gate d e s)
@i{ok}
@end lisp

Better yet, we can explicitly name this operation by defining a procedure
@code{half-adder} that constructs this circuit, given the four external wires
to be attached to the half-adder:

@lisp
(define (half-adder a b s c)
  (let ((d (make-wire)) (e (make-wire)))
    (or-gate a b d)
    (and-gate a b c)
    (inverter c e)
    (and-gate d e s)
    'ok))
@end lisp

The advantage of making this definition is that we can use @code{half-adder}
itself as a building block in creating more complex circuits.  @ref{Figure
3-26}, for example, shows a @newterm{full-adder} composed of two half-adders
and an or-gate.@footnote{A full-adder is a basic circuit element used in adding
two binary numbers.  Here A and B are the bits at corresponding positions in
the two numbers to be added, and C_(@i{i}@i{n}) is the carry bit from the
addition one place to the right.  The circuit generates SUM, which is the sum
bit in the corresponding position, and C_(@i{o}@i{u}@i{t}), which is the carry
bit to be propagated to the left.} We can construct a full-adder as follows:

@lisp
(define (full-adder a b c-in sum c-out)
  (let ((s (make-wire))
        (c1 (make-wire))
        (c2 (make-wire)))
    (half-adder b c-in s c1)
    (half-adder a s sum c2)
    (or-gate c1 c2 c-out)
    'ok))
@end lisp

Having defined @code{full-adder} as a procedure, we can now use it as a
building block for creating still more complex circuits.  (For example, see
@ref{Exercise 3-30}.)

@quotation
@strong{@anchor{Figure 3-26}Figure 3.26:} A full-adder circuit.

@example
    +----------------------------------+
    |              +-------+           |
A -----------------+ full  +-------------- SUM
    |  +-------+   | adder |   ____    |
B -----+ half  +---+       +---\   \   |
    |  | adder |   +-------+    >or >----- Cout
C -----+       +---------------/___/   |
    |  +-------+                       |
    +----------------------------------+
@end example
@end quotation

In essence, our simulator provides us with the tools to construct a language of
circuits.  If we adopt the general perspective on languages with which we
approached the study of Lisp in section @ref{1-1}, we can say that the
primitive function boxes form the primitive elements of the language, that
wiring boxes together provides a means of combination, and that specifying
wiring patterns as procedures serves as a means of abstraction.

@subsubheading Primitive function boxes

The primitive function boxes implement the ``forces'' by which a change in the
signal on one wire influences the signals on other wires.  To build function
boxes, we use the following operations on wires:

@itemize @bullet

@item

@lisp
(get-signal <@var{wire}>)
@end lisp

@noindent
returns the current value of the signal on the wire.

@item

@lisp
(set-signal! <@var{wire}> <@var{new value}>)
@end lisp

@noindent
changes the value of the signal on the wire to the new value.

@item

@lisp
(add-action! <@var{wire}> <@var{procedure of no arguments}>)
@end lisp

@noindent
asserts that the designated procedure should be run whenever the signal on the
wire changes value.  Such procedures are the vehicles by which changes in the
signal value on the wire are communicated to other wires.

@end itemize

In addition, we will make use of a procedure @code{after-delay} that takes a
time delay and a procedure to be run and executes the given procedure after the
given delay.

Using these procedures, we can define the primitive digital logic functions.
To connect an input to an output through an inverter, we use @code{add-action!}
to associate with the input wire a procedure that will be run whenever the
signal on the input wire changes value.  The procedure computes the
@code{logical-not} of the input signal, and then, after one
@code{inverter-delay}, sets the output signal to be this new value:

@lisp
(define (inverter input output)
  (define (invert-input)
    (let ((new-value (logical-not (get-signal input))))
      (after-delay inverter-delay
                   (lambda ()
                     (set-signal! output new-value)))))
  (add-action! input invert-input)
  'ok)

(define (logical-not s)
  (cond ((= s 0) 1)
        ((= s 1) 0)
        (else (error "Invalid signal" s))))
@end lisp

An and-gate is a little more complex.  The action procedure must be run if
either of the inputs to the gate changes.  It computes the @code{logical-and}
(using a procedure analogous to @code{logical-not}) of the values of the
signals on the input wires and sets up a change to the new value to occur on
the output wire after one @code{and-gate-delay}.

@lisp
(define (and-gate a1 a2 output)
  (define (and-action-procedure)
    (let ((new-value
           (logical-and (get-signal a1) (get-signal a2))))
      (after-delay and-gate-delay
                   (lambda ()
                     (set-signal! output new-value)))))
  (add-action! a1 and-action-procedure)
  (add-action! a2 and-action-procedure)
  'ok)
@end lisp

@quotation
@strong{@anchor{Exercise 3-28}Exercise 3.28:} Define an or-gate as a primitive
function box.  Your @code{or-gate} constructor should be similar to
@code{and-gate}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-29}Exercise 3.29:} Another way to construct an
or-gate is as a compound digital logic device, built from and-gates and
inverters.  Define a procedure @code{or-gate} that accomplishes this.  What is
the delay time of the or-gate in terms of @code{and-gate-delay} and
@code{inverter-delay}?
@end quotation

@quotation
@strong{@anchor{Exercise 3-30}Exercise 3.30:} @ref{Figure 3-27} shows a
@newterm{ripple-carry adder} formed by stringing together @i{n} full-adders.
This is the simplest form of parallel adder for adding two @i{n}-bit binary
numbers.  The inputs A_1, A_2, A_3, @dots{}, A_@i{n} and B_1, B_2, B_3,
@dots{}, B_@i{n} are the two binary numbers to be added (each A_@i{k} and
B_@i{k} is a 0 or a 1).  The circuit generates S_1, S_2, S_3, @dots{}, S_@i{n},
the @i{n} bits of the sum, and C, the carry from the addition.  Write a
procedure @code{ripple-carry-adder} that generates this circuit.  The procedure
should take as arguments three lists of @i{n} wires each---the A_@i{k}, the
B_@i{k}, and the S_@i{k}---and also another wire C.  The major drawback of the
ripple-carry adder is the need to wait for the carry signals to propagate.
What is the delay needed to obtain the complete output from an @i{n}-bit
ripple-carry adder, expressed in terms of the delays for and-gates, or-gates,
and inverters?
@end quotation

@quotation
@strong{@anchor{Figure 3-27}Figure 3.27:} A ripple-carry adder for @i{n}-bit
numbers.

@example
   :                                              :   :
   : A_1 B_1   C_1   A_2 B_2   C_2   A_3 B_3   C_3:   : A_n B_n C_n=0
   :  |   |   +---+   |   |   +---+   |   |   +-----  :  |   |   +-
   |  |   |   |   |   |   |   |   |   |   |   |   :   :  |   |   | 
   : ++---+---++  |  ++---+---++  |  ++---+---++  :   : ++---+---++
   : |   FA    |  |  |   FA    |  |  |   FA    |  :   : |   FA    |
   : +--+---+--+  |  +--+---+--+  |  +--+---+--+  :   : +--+---+--+
   :    |   |     |     |   |     |     |   |     :   :    |   |   
C ------+   |     +-----+   |     +-----+   |     :  ------+   |   
   :        |       C_1     |       C_2     |     :   :C_(n-1) |   
   :        |               |               |     :   :        |   
           S_1             S_2             S_3                S_n
@end example
@end quotation

@subsubheading Representing wires

A wire in our simulation will be a computational object with two local state
variables: a @code{signal-value} (initially taken to be 0) and a collection of
@code{action-procedures} to be run when the signal changes value.  We implement
the wire, using message-passing style, as a collection of local procedures
together with a @code{dispatch} procedure that selects the appropriate local
operation, just as we did with the simple bank-account object in section
@ref{3-1-1}:

@lisp
(define (make-wire)
  (let ((signal-value 0) (action-procedures '()))
    (define (set-my-signal! new-value)
      (if (not (= signal-value new-value))
          (begin (set! signal-value new-value)
                 (call-each action-procedures))
          'done))

    (define (accept-action-procedure! proc)
      (set! action-procedures (cons proc action-procedures))
      (proc))

    (define (dispatch m)
      (cond ((eq? m 'get-signal) signal-value)
            ((eq? m 'set-signal!) set-my-signal!)
            ((eq? m 'add-action!) accept-action-procedure!)
            (else (error "Unknown operation -- WIRE" m))))
    dispatch))
@end lisp

The local procedure @code{set-my-signal!} tests whether the new signal value
changes the signal on the wire.  If so, it runs each of the action procedures,
using the following procedure @code{call-each}, which calls each of the items
in a list of no-argument procedures:

@lisp
(define (call-each procedures)
  (if (null? procedures)
      'done
      (begin
        ((car procedures))
        (call-each (cdr procedures)))))
@end lisp

The local procedure @code{accept-action-procedure!} adds the given procedure to
the list of procedures to be run, and then runs the new procedure once.  (See
@ref{Exercise 3-31}.)

With the local @code{dispatch} procedure set up as specified, we can provide
the following procedures to access the local operations on
wires:@footnote{[Footnote 27]@anchor{Footnote 27} These procedures are simply
syntactic sugar that allow us to use ordinary procedural syntax to access the
local procedures of objects.  It is striking that we can interchange the role
of ``procedures'' and ``data'' in such a simple way.  For example, if we write
@code{(wire 'get-signal)} we think of @code{wire} as a procedure that is called
with the message @code{get-signal} as input.  Alternatively, writing
@code{(get-signal wire)} encourages us to think of @code{wire} as a data object
that is the input to a procedure @code{get-signal}.  The truth of the matter is
that, in a language in which we can deal with procedures as objects, there is
no fundamental difference between ``procedures'' and ``data,'' and we can
choose our syntactic sugar to allow us to program in whatever style we choose.}

@lisp
(define (get-signal wire)
  (wire 'get-signal))

(define (set-signal! wire new-value)
  ((wire 'set-signal!) new-value))

(define (add-action! wire action-procedure)
  ((wire 'add-action!) action-procedure))
@end lisp

Wires, which have time-varying signals and may be incrementally attached to
devices, are typical of mutable objects.  We have modeled them as procedures
with local state variables that are modified by assignment.  When a new wire is
created, a new set of state variables is allocated (by the @code{let}
expression in @code{make-wire}) and a new @code{dispatch} procedure is
constructed and returned, capturing the environment with the new state
variables.

The wires are shared among the various devices that have been connected to
them.  Thus, a change made by an interaction with one device will affect all
the other devices attached to the wire.  The wire communicates the change to
its neighbors by calling the action procedures provided to it when the
connections were established.

@subsubheading The agenda

The only thing needed to complete the simulator is @code{after-delay}.  The
idea here is that we maintain a data structure, called an @newterm{agenda},
that contains a schedule of things to do.  The following operations are defined
for agendas:

@itemize @bullet

@item
@code{(make-agenda)} returns a new empty agenda.

@item
@code{(empty-agenda? <@var{agenda}>)} is true if the specified agenda is
empty.

@item
@code{(first-agenda-item <@var{agenda}>)} returns the first item on the
agenda.

@item
@code{(remove-first-agenda-item! <@var{agenda}>)} modifies the agenda by
removing the first item.

@item
@code{(add-to-agenda! <@var{time}> <@var{action}>
<@var{agenda}>)} modifies the agenda by adding the given action procedure
to be run at the specified time.

@item
@code{(current-time <@var{agenda}>)} returns the current simulation time.

@end itemize

The particular agenda that we use is denoted by @code{the-agenda}.  The
procedure @code{after-delay} adds new elements to @code{the-agenda}:

@lisp
(define (after-delay delay action)
  (add-to-agenda! (+ delay (current-time the-agenda))
                  action
                  the-agenda))
@end lisp

The simulation is driven by the procedure @code{propagate}, which operates on
@code{the-agenda}, executing each procedure on the agenda in sequence.  In
general, as the simulation runs, new items will be added to the agenda, and
@code{propagate} will continue the simulation as long as there are items on the
agenda:

@lisp
(define (propagate)
  (if (empty-agenda? the-agenda)
      'done
      (let ((first-item (first-agenda-item the-agenda)))
        (first-item)
        (remove-first-agenda-item! the-agenda)
        (propagate))))
@end lisp

@subsubheading A sample simulation

The following procedure, which places a ``probe'' on a wire, shows the
simulator in action.  The probe tells the wire that, whenever its signal
changes value, it should print the new signal value, together with the current
time and a name that identifies the wire:

@lisp
(define (probe name wire)
  (add-action! wire
               (lambda ()
                 (newline)
                 (display name)
                 (display " ")
                 (display (current-time the-agenda))
                 (display "  New-value = ")
                 (display (get-signal wire)))))
@end lisp

We begin by initializing the agenda and specifying delays for the primitive
function boxes:

@lisp
(define the-agenda (make-agenda))
(define inverter-delay 2)
(define and-gate-delay 3)
(define or-gate-delay 5)
@end lisp

Now we define four wires, placing probes on two of them:

@lisp
(define input-1 (make-wire))
(define input-2 (make-wire))
(define sum (make-wire))
(define carry (make-wire))

(probe 'sum sum)
@i{sum 0  New-value = 0}

(probe 'carry carry)
@i{carry 0  New-value = 0}
@end lisp

Next we connect the wires in a half-adder circuit (as in @ref{Figure 3-25}),
set the signal on @code{input-1} to 1, and run the simulation:

@lisp
(half-adder input-1 input-2 sum carry)
@i{ok}

(set-signal! input-1 1)
@i{done}

(propagate)
@i{sum 8  New-value = 1}
@i{done}
@end lisp

The @code{sum} signal changes to 1 at time 8.  We are now eight time units from
the beginning of the simulation.  At this point, we can set the signal on
@code{input-2} to 1 and allow the values to propagate:

@lisp
(set-signal! input-2 1)
@i{done}

(propagate)
@i{carry 11  New-value = 1}
@i{sum 16  New-value = 0}
@i{done}
@end lisp

The @code{carry} changes to 1 at time 11 and the @code{sum} changes to 0 at
time 16.

@quotation
@strong{@anchor{Exercise 3-31}Exercise 3.31:} The internal procedure
@code{accept-action-procedure!} defined in @code{make-wire} specifies that when
a new action procedure is added to a wire, the procedure is immediately run.
Explain why this initialization is necessary.  In particular, trace through the
half-adder example in the paragraphs above and say how the system's response
would differ if we had defined @code{accept-action-procedure!} as

@lisp
(define (accept-action-procedure! proc)
  (set! action-procedures (cons proc action-procedures)))
@end lisp
@end quotation

@subsubheading Implementing the agenda

Finally, we give details of the agenda data structure, which holds the
procedures that are scheduled for future execution.

The agenda is made up of @newterm{time segments}.  Each time segment is a pair
consisting of a number (the time) and a queue (see @ref{Exercise 3-32}) that
holds the procedures that are scheduled to be run during that time segment.

@lisp
(define (make-time-segment time queue)
  (cons time queue))

(define (segment-time s) (car s))

(define (segment-queue s) (cdr s))
@end lisp

We will operate on the time-segment queues using the queue operations described
in section @ref{3-3-2}.

The agenda itself is a one-dimensional table of time segments.  It differs from
the tables described in section @ref{3-3-3} in that the segments will be sorted
in order of increasing time.  In addition, we store the @newterm{current time}
(i.e., the time of the last action that was processed) at the head of the
agenda.  A newly constructed agenda has no time segments and has a current time
of 0:@footnote{The agenda is a headed list, like the tables in section
@ref{3-3-3}, but since the list is headed by the time, we do not need an
additional dummy header (such as the @code{*table*} symbol used with tables).}

@lisp
(define (make-agenda) (list 0))

(define (current-time agenda) (car agenda))

(define (set-current-time! agenda time)
  (set-car! agenda time))

(define (segments agenda) (cdr agenda))

(define (set-segments! agenda segments)
  (set-cdr! agenda segments))

(define (first-segment agenda) (car (segments agenda)))

(define (rest-segments agenda) (cdr (segments agenda)))
@end lisp

An agenda is empty if it has no time segments:

@lisp
(define (empty-agenda? agenda)
  (null? (segments agenda)))
@end lisp

To add an action to an agenda, we first check if the agenda is empty.  If so,
we create a time segment for the action and install this in the agenda.
Otherwise, we scan the agenda, examining the time of each segment.  If we find
a segment for our appointed time, we add the action to the associated queue.
If we reach a time later than the one to which we are appointed, we insert a
new time segment into the agenda just before it.  If we reach the end of the
agenda, we must create a new time segment at the end.

@lisp
(define (add-to-agenda! time action agenda)
  (define (belongs-before? segments)
    (or (null? segments)
        (< time (segment-time (car segments)))))
  (define (make-new-time-segment time action)
    (let ((q (make-queue)))
      (insert-queue! q action)
      (make-time-segment time q)))
  (define (add-to-segments! segments)
    (if (= (segment-time (car segments)) time)
        (insert-queue! (segment-queue (car segments))
                       action)
        (let ((rest (cdr segments)))
          (if (belongs-before? rest)
              (set-cdr!
               segments
               (cons (make-new-time-segment time action)
                     (cdr segments)))
              (add-to-segments! rest)))))
  (let ((segments (segments agenda)))
    (if (belongs-before? segments)
        (set-segments!
         agenda
         (cons (make-new-time-segment time action)
               segments))
        (add-to-segments! segments))))
@end lisp

The procedure that removes the first item from the agenda deletes the item at
the front of the queue in the first time segment.  If this deletion makes the
time segment empty, we remove it from the list of segments:@footnote{Observe
that the @code{if} expression in this procedure has no <@var{alternative}>
expression.  Such a ``one-armed @code{if} statement'' is used to decide whether
to do something, rather than to select between two expressions.  An @code{if}
expression returns an unspecified value if the predicate is false and there is
no <@var{alternative}>.}

@lisp
(define (remove-first-agenda-item! agenda)
  (let ((q (segment-queue (first-segment agenda))))
    (delete-queue! q)
    (if (empty-queue? q)
        (set-segments! agenda (rest-segments agenda)))))
@end lisp

The first agenda item is found at the head of the queue in the first time
segment.  Whenever we extract an item, we also update the current
time:@footnote{In this way, the current time will always be the time of the
action most recently processed.  Storing this time at the head of the agenda
ensures that it will still be available even if the associated time segment has
been deleted.}

@lisp
(define (first-agenda-item agenda)
  (if (empty-agenda? agenda)
      (error "Agenda is empty -- FIRST-AGENDA-ITEM")
      (let ((first-seg (first-segment agenda)))
        (set-current-time! agenda (segment-time first-seg))
        (front-queue (segment-queue first-seg)))))
@end lisp

@quotation
@strong{@anchor{Exercise 3-32}Exercise 3.32:} The procedures to be run during
each time segment of the agenda are kept in a queue.  Thus, the procedures for
each segment are called in the order in which they were added to the agenda
(first in, first out).  Explain why this order must be used.  In particular,
trace the behavior of an and-gate whose inputs change from 0,1 to 1,0 in the
same segment and say how the behavior would differ if we stored a segment's
procedures in an ordinary list, adding and removing procedures only at the
front (last in, first out).
@end quotation


@node	3-3-5,  , 3-3-4, 3-3
@subsection Propagation of Constraints

Computer programs are traditionally organized as one-directional computations,
which perform operations on prespecified arguments to produce desired outputs.
On the other hand, we often model systems in terms of relations among
quantities.  For example, a mathematical model of a mechanical structure might
include the information that the deflection @i{d} of a metal rod is related to
the force @i{f} on the rod, the length @i{L} of the rod, the cross-sectional
area @i{A}, and the elastic modulus @i{E} via the equation

@example
dAE = FL
@end example

Such an equation is not one-directional.  Given any four of the quantities, we
can use it to compute the fifth.  Yet translating the equation into a
traditional computer language would force us to choose one of the quantities to
be computed in terms of the other four.  Thus, a procedure for computing the
area @i{A} could not be used to compute the deflection @i{d}, even though the
computations of @i{A} and @i{d} arise from the same
equation.@footnote{Constraint propagation first appeared in the incredibly
forward-looking @acronym{SKETCHPAD} system of Ivan Sutherland (1963).  A
beautiful constraint-propagation system based on the Smalltalk language was
developed by Alan Borning (1977) at Xerox Palo Alto Research Center.  Sussman,
Stallman, and Steele applied constraint propagation to electrical circuit
analysis (Sussman and Stallman 1975; Sussman and Steele 1980). TK!Solver
(Konopasek and Jayaraman 1984) is an extensive modeling environment based on
constraints.}

In this section, we sketch the design of a language that enables us to work in
terms of relations themselves.  The primitive elements of the language are
@newterm{primitive constraints}, which state that certain relations hold
between quantities.  For example, @code{(adder a b c)} specifies that the
quantities @i{a}, @i{b}, and @i{c} must be related by the equation @i{a} +
@i{b} = @i{c}, @code{(multiplier x y z)} expresses the constraint @i{x}@i{y} =
@i{z}, and @code{(constant 3.14 x)} says that the value of @i{x} must be 3.14.

Our language provides a means of combining primitive constraints in order to
express more complex relations.  We combine constraints by constructing
@newterm{constraint networks}, in which constraints are joined by
@newterm{connectors}.  A connector is an object that ``holds'' a value that may
participate in one or more constraints.  For example, we know that the
relationship between Fahrenheit and Celsius temperatures is

@example
9C = 5(F - 32)
@end example

Such a constraint can be thought of as a network consisting of primitive adder,
multiplier, and constant constraints (@ref{Figure 3-28}).  In the figure, we
see on the left a multiplier box with three terminals, labeled @i{m}1, @i{m}2,
and @i{p}.  These connect the multiplier to the rest of the network as follows:
The @i{m}1 terminal is linked to a connector @i{C}, which will hold the Celsius
temperature.  The @i{m}2 terminal is linked to a connector @i{w}, which is also
linked to a constant box that holds 9.  The @i{p} terminal, which the
multiplier box constrains to be the product of @i{m}1 and @i{m}2, is linked to
the @i{p} terminal of another multiplier box, whose @i{m}2 is connected to a
constant 5 and whose @i{m}1 is connected to one of the terms in a sum.

@quotation
@strong{@anchor{Figure 3-28}Figure 3.28:} The relation 9@i{C} = 5(@i{F} - 32)
expressed as a constraint network.

@example
       +---------+     +---------+   v   +---------+
C -----+ m1      |  u  |      m1 +-------+ a1      |
       |    *  p +-----+ p  *    |       |    *  s +---- F
    +--+ m2      |     |      m2 +--+ +--+ a2      |
    |  +---------+     +---------+  | |  +---------+
  w |                              x| |y
    |    +-----+        +-----+     | |     +-----+
    +----+  9  |        |  5  +-----+ +-----+  32 |
         +-----+        +-----+             +-----+
@end example
@end quotation

Computation by such a network proceeds as follows: When a connector is given a
value (by the user or by a constraint box to which it is linked), it awakens
all of its associated constraints (except for the constraint that just awakened
it) to inform them that it has a value.  Each awakened constraint box then
polls its connectors to see if there is enough information to determine a value
for a connector.  If so, the box sets that connector, which then awakens all of
its associated constraints, and so on.  For instance, in conversion between
Celsius and Fahrenheit, @i{w}, @i{x}, and @i{y} are immediately set by the
constant boxes to 9, 5, and 32, respectively.  The connectors awaken the
multipliers and the adder, which determine that there is not enough information
to proceed.  If the user (or some other part of the network) sets @i{C} to a
value (say 25), the leftmost multiplier will be awakened, and it will set @i{u}
to 25*9 = 225.  Then @i{u} awakens the second multiplier, which sets @i{v} to
45, and @i{v} awakens the adder, which sets @i{f} to 77.

@subsubheading Using the constraint system

To use the constraint system to carry out the temperature computation outlined
above, we first create two connectors, @code{C} and @code{F}, by calling the
constructor @code{make-connector}, and link @code{C} and @code{F} in an
appropriate network:

@lisp
(define C (make-connector))
(define F (make-connector))
(celsius-fahrenheit-converter C F)
@i{ok}
@end lisp

The procedure that creates the network is defined as follows:

@lisp
(define (celsius-fahrenheit-converter c f)
  (let ((u (make-connector))
        (v (make-connector))
        (w (make-connector))
        (x (make-connector))
        (y (make-connector)))
    (multiplier c w u)
    (multiplier v x u)
    (adder v y f)
    (constant 9 w)
    (constant 5 x)
    (constant 32 y)
    'ok))
@end lisp

This procedure creates the internal connectors @code{u}, @code{v}, @code{w},
@code{x}, and @code{y}, and links them as shown in @ref{Figure 3-28} using the
primitive constraint constructors @code{adder}, @code{multiplier}, and
@code{constant}.  Just as with the digital-circuit simulator of section
@ref{3-3-4}, expressing these combinations of primitive elements in terms of
procedures automatically provides our language with a means of abstraction for
compound objects.

To watch the network in action, we can place probes on the connectors @code{C}
and @code{F}, using a @code{probe} procedure similar to the one we used to
monitor wires in section @ref{3-3-4}.  Placing a probe on a connector will
cause a message to be printed whenever the connector is given a value:

@lisp
(probe "Celsius temp" C)
(probe "Fahrenheit temp" F)
@end lisp

Next we set the value of @code{C} to 25.  (The third argument to
@code{set-value!} tells @code{C} that this directive comes from the
@code{user}.)

@lisp
(set-value! C 25 'user)
@i{Probe: Celsius temp = 25}
@i{Probe: Fahrenheit temp = 77}
@i{done}
@end lisp

The probe on @code{C} awakens and reports the value.  @code{C} also propagates
its value through the network as described above.  This sets @code{F} to 77,
which is reported by the probe on @code{F}.

Now we can try to set @code{F} to a new value, say 212:

@lisp
(set-value! F 212 'user)
@i{Error! Contradiction (77 212)}
@end lisp

The connector complains that it has sensed a contradiction: Its value is 77,
and someone is trying to set it to 212.  If we really want to reuse the network
with new values, we can tell @code{C} to forget its old value:

@lisp
(forget-value! C 'user)
@i{Probe: Celsius temp = ?}
@i{Probe: Fahrenheit temp = ?}
@i{done}
@end lisp

@code{C} finds that the @code{user}, who set its value originally, is now
retracting that value, so @code{C} agrees to lose its value, as shown by the
probe, and informs the rest of the network of this fact.  This information
eventually propagates to @code{F}, which now finds that it has no reason for
continuing to believe that its own value is 77.  Thus, @code{F} also gives up
its value, as shown by the probe.

Now that @code{F} has no value, we are free to set it to 212:

@lisp
(set-value! F 212 'user)
@i{Probe: Fahrenheit temp = 212}
@i{Probe: Celsius temp = 100}
@i{done}
@end lisp

This new value, when propagated through the network, forces @code{C} to have a
value of 100, and this is registered by the probe on @code{C}.  Notice that the
very same network is being used to compute @code{C} given @code{F} and to
compute @code{F} given @code{C}.  This nondirectionality of computation is the
distinguishing feature of constraint-based systems.

@subsubheading Implementing the constraint system

The constraint system is implemented via procedural objects with local state,
in a manner very similar to the digital-circuit simulator of section
@ref{3-3-4}.  Although the primitive objects of the constraint system are
somewhat more complex, the overall system is simpler, since there is no concern
about agendas and logic delays.

The basic operations on connectors are the following:

@itemize @bullet

@item
@code{(has-value? <@var{connector}>)} tells whether the connector has a value.

@item
@code{(get-value <@var{connector}>)} returns the connector's current value.

@item
@code{(set-value! <@var{connector}> <@var{new-value}> <@var{informant}>)}
indicates that the informant is requesting the connector to set its value to
the new value.

@item
@code{(forget-value! <@var{connector}> <@var{retractor}>)} tells the connector
that the retractor is requesting it to forget its value.

@item
@code{(connect <@var{connector}> <@var{new-constraint}>)} tells the connector
to participate in the new constraint.

@end itemize

The connectors communicate with the constraints by means of the procedures
@code{inform-about-value}, which tells the given constraint that the connector
has a value, and @code{inform-about-no-value}, which tells the constraint that
the connector has lost its value.

@code{Adder} constructs an adder constraint among summand connectors @code{a1}
and @code{a2} and a @code{sum} connector.  An adder is implemented as a
procedure with local state (the procedure @code{me} below):

@lisp
(define (adder a1 a2 sum)
  (define (process-new-value)
    (cond ((and (has-value? a1) (has-value? a2))
           (set-value! sum
                       (+ (get-value a1) (get-value a2))
                       me))
          ((and (has-value? a1) (has-value? sum))
           (set-value! a2
                       (- (get-value sum) (get-value a1))
                       me))
          ((and (has-value? a2) (has-value? sum))
           (set-value! a1
                       (- (get-value sum) (get-value a2))
                       me))))
  (define (process-forget-value)
    (forget-value! sum me)
    (forget-value! a1 me)
    (forget-value! a2 me)
    (process-new-value))
  (define (me request)
    (cond ((eq? request 'I-have-a-value)
           (process-new-value))
          ((eq? request 'I-lost-my-value)
           (process-forget-value))
          (else
           (error "Unknown request -- ADDER" request))))
  (connect a1 me)
  (connect a2 me)
  (connect sum me)
  me)
@end lisp

@code{Adder} connects the new adder to the designated connectors and returns it
as its value.  The procedure @code{me}, which represents the adder, acts as a
dispatch to the local procedures.  The following ``syntax interfaces'' (see
footnote @ref{Footnote 27} in section @ref{3-3-4}) are used in conjunction with
the dispatch:

@lisp
(define (inform-about-value constraint)
  (constraint 'I-have-a-value))

(define (inform-about-no-value constraint)
  (constraint 'I-lost-my-value))
@end lisp

The adder's local procedure @code{process-new-value} is called when the adder
is informed that one of its connectors has a value. The adder first checks to
see if both @code{a1} and @code{a2} have values. If so, it tells @code{sum} to
set its value to the sum of the two addends.  The @code{informant} argument to
@code{set-value!} is @code{me}, which is the adder object itself.  If @code{a1}
and @code{a2} do not both have values, then the adder checks to see if perhaps
@code{a1} and @code{sum} have values.  If so, it sets @code{a2} to the
difference of these two.  Finally, if @code{a2} and @code{sum} have values,
this gives the adder enough information to set @code{a1}.  If the adder is told
that one of its connectors has lost a value, it requests that all of its
connectors now lose their values.  (Only those values that were set by this
adder are actually lost.)  Then it runs @code{process-new-value}.  The reason
for this last step is that one or more connectors may still have a value (that
is, a connector may have had a value that was not originally set by the adder),
and these values may need to be propagated back through the adder.

A multiplier is very similar to an adder. It will set its @code{product} to 0
if either of the factors is 0, even if the other factor is not known.

@lisp
(define (multiplier m1 m2 product)
  (define (process-new-value)
    (cond ((or (and (has-value? m1) (= (get-value m1) 0))
               (and (has-value? m2) (= (get-value m2) 0)))
           (set-value! product 0 me))
          ((and (has-value? m1) (has-value? m2))
           (set-value! product
                       (* (get-value m1) (get-value m2))
                       me))
          ((and (has-value? product) (has-value? m1))
           (set-value! m2
                       (/ (get-value product) (get-value m1))
                       me))
          ((and (has-value? product) (has-value? m2))
           (set-value! m1
                       (/ (get-value product) (get-value m2))
                       me))))
  (define (process-forget-value)
    (forget-value! product me)
    (forget-value! m1 me)
    (forget-value! m2 me)
    (process-new-value))
  (define (me request)
    (cond ((eq? request 'I-have-a-value)
           (process-new-value))
          ((eq? request 'I-lost-my-value)
           (process-forget-value))
          (else
           (error "Unknown request -- MULTIPLIER" request))))
  (connect m1 me)
  (connect m2 me)
  (connect product me)
  me)
@end lisp

A @code{constant} constructor simply sets the value of the designated
connector.  Any @code{I-have-a-value} or @code{I-lost-my-value} message sent to
the constant box will produce an error.

@lisp
(define (constant value connector)
  (define (me request)
    (error "Unknown request -- CONSTANT" request))
  (connect connector me)
  (set-value! connector value me)
  me)
@end lisp

Finally, a probe prints a message about the setting or unsetting of
the designated connector:

@lisp
(define (probe name connector)
  (define (print-probe value)
    (newline)
    (display "Probe: ")
    (display name)
    (display " = ")
    (display value))
  (define (process-new-value)
    (print-probe (get-value connector)))
  (define (process-forget-value)
    (print-probe "?"))
  (define (me request)
    (cond ((eq? request 'I-have-a-value)
           (process-new-value))
          ((eq? request 'I-lost-my-value)
           (process-forget-value))
          (else
           (error "Unknown request -- PROBE" request))))
  (connect connector me)
  me)
@end lisp

@subsubheading Representing connectors

A connector is represented as a procedural object with local state variables
@code{value}, the current value of the connector; @code{informant}, the object
that set the connector's value; and @code{constraints}, a list of the
constraints in which the connector participates.

@lisp
(define (make-connector)
  (let ((value false) (informant false) (constraints '()))
    (define (set-my-value newval setter)
      (cond ((not (has-value? me))
             (set! value newval)
             (set! informant setter)
             (for-each-except setter
                              inform-about-value
                              constraints))
            ((not (= value newval))
             (error "Contradiction" (list value newval)))
            (else 'ignored)))
    (define (forget-my-value retractor)
      (if (eq? retractor informant)
          (begin (set! informant false)
                 (for-each-except retractor
                                  inform-about-no-value
                                  constraints))
          'ignored))
    (define (connect new-constraint)
      (if (not (memq new-constraint constraints))
          (set! constraints
                (cons new-constraint constraints)))
      (if (has-value? me)
          (inform-about-value new-constraint))
      'done)
    (define (me request)
      (cond ((eq? request 'has-value?)
             (if informant true false))
            ((eq? request 'value) value)
            ((eq? request 'set-value!) set-my-value)
            ((eq? request 'forget) forget-my-value)
            ((eq? request 'connect) connect)
            (else (error "Unknown operation -- CONNECTOR"
                         request))))
    me))
@end lisp

The connector's local procedure @code{set-my-value} is called when there is a
request to set the connector's value.  If the connector does not currently have
a value, it will set its value and remember as @code{informant} the constraint
that requested the value to be set.@footnote{The @code{setter} might not be a
constraint.  In our temperature example, we used @code{user} as the
@code{setter}.}  Then the connector will notify all of its participating
constraints except the constraint that requested the value to be set.  This is
accomplished using the following iterator, which applies a designated procedure
to all items in a list except a given one:

@lisp
(define (for-each-except exception procedure list)
  (define (loop items)
    (cond ((null? items) 'done)
          ((eq? (car items) exception) (loop (cdr items)))
          (else (procedure (car items))
                (loop (cdr items)))))
  (loop list))
@end lisp

If a connector is asked to forget its value, it runs the local procedure
@code{forget-my-value}, which first checks to make sure that the request is
coming from the same object that set the value originally.  If so, the
connector informs its associated constraints about the loss of the value.

The local procedure @code{connect} adds the designated new constraint to the
list of constraints if it is not already in that list.  Then, if the connector
has a value, it informs the new constraint of this fact.

The connector's procedure @code{me} serves as a dispatch to the other internal
procedures and also represents the connector as an object.  The following
procedures provide a syntax interface for the dispatch:

@lisp
(define (has-value? connector)
  (connector 'has-value?))

(define (get-value connector)
  (connector 'value))

(define (set-value! connector new-value informant)
  ((connector 'set-value!) new-value informant))

(define (forget-value! connector retractor)
  ((connector 'forget) retractor))

(define (connect connector new-constraint)
  ((connector 'connect) new-constraint))
@end lisp

@quotation
@strong{@anchor{Exercise 3-33}Exercise 3.33:} Using primitive multiplier,
adder, and constant constraints, define a procedure @code{averager} that takes
three connectors @code{a}, @code{b}, and @code{c} as inputs and establishes the
constraint that the value of @code{c} is the average of the values of @code{a}
and @code{b}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-34}Exercise 3.34:} Louis Reasoner wants to build a
squarer, a constraint device with two terminals such that the value of
connector @code{b} on the second terminal will always be the square of the
value @code{a} on the first terminal.  He proposes the following simple device
made from a multiplier:

@lisp
(define (squarer a b)
  (multiplier a a b))
@end lisp

There is a serious flaw in this idea.  Explain.
@end quotation

@quotation
@strong{@anchor{Exercise 3-35}Exercise 3.35:} Ben Bitdiddle tells Louis that
one way to avoid the trouble in @ref{Exercise 3-34} is to define a squarer as a
new primitive constraint.  Fill in the missing portions in Ben's outline for a
procedure to implement such a constraint:

@lisp
(define (squarer a b)
  (define (process-new-value)
    (if (has-value? b)
        (if (< (get-value b) 0)
            (error "square less than 0 -- SQUARER" (get-value b))
            <@var{alternative1}>)
        <@var{alternative2}>))
  (define (process-forget-value) <@var{body1}>)
  (define (me request) <@var{body2}>)
  <@var{rest of definition}>
  me)
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-36}Exercise 3.36:} Suppose we evaluate the following
sequence of expressions in the global environment:

@lisp
(define a (make-connector))
(define b (make-connector))
(set-value! a 10 'user)
@end lisp

At some time during evaluation of the @code{set-value!}, the following
expression from the connector's local procedure is evaluated:

@lisp
(for-each-except setter inform-about-value constraints)
@end lisp

Draw an environment diagram showing the environment in which the above
expression is evaluated.
@end quotation

@quotation
@strong{@anchor{Exercise 3-37}Exercise 3.37:} The
@code{celsius-fahrenheit-converter} procedure is cumbersome when compared with
a more expression-oriented style of definition, such as

@lisp
(define (celsius-fahrenheit-converter x)
  (c+ (c* (c/ (cv 9) (cv 5))
          x)
      (cv 32)))

(define C (make-connector))
(define F (celsius-fahrenheit-converter C))
@end lisp

Here @code{c+}, @code{c*}, etc. are the ``constraint'' versions of the
arithmetic operations.  For example, @code{c+} takes two connectors as
arguments and returns a connector that is related to these by an adder
constraint:

@lisp
(define (c+ x y)
  (let ((z (make-connector)))
    (adder x y z)
    z))
@end lisp

Define analogous procedures @code{c-}, @code{c*}, @code{c/}, and @code{cv}
(constant value) that enable us to define compound constraints as in the
converter example above.@footnote{The expression-oriented format is convenient
because it avoids the need to name the intermediate expressions in a
computation.  Our original formulation of the constraint language is cumbersome
in the same way that many languages are cumbersome when dealing with operations
on compound data.  For example, if we wanted to compute the product (@i{a} +
@i{b}) * (@i{c} + @i{d}), where the variables represent vectors, we could work
in ``imperative style,'' using procedures that set the values of designated
vector arguments but do not themselves return vectors as values:

@lisp
(v-sum a b temp1)
(v-sum c d temp2)
(v-prod temp1 temp2 answer)
@end lisp

Alternatively, we could deal with expressions, using procedures that return
vectors as values, and thus avoid explicitly mentioning @code{temp1} and
@code{temp2}:

@lisp
(define answer (v-prod (v-sum a b) (v-sum c d)))
@end lisp

Since Lisp allows us to return compound objects as values of procedures, we can
transform our imperative-style constraint language into an expression-oriented
style as shown in this exercise.  In languages that are impoverished in
handling compound objects, such as Algol, Basic, and Pascal (unless one
explicitly uses Pascal pointer variables), one is usually stuck with the
imperative style when manipulating compound objects.  Given the advantage of
the expression-oriented format, one might ask if there is any reason to have
implemented the system in imperative style, as we did in this section.  One
reason is that the non-expression-oriented constraint language provides a
handle on constraint objects (e.g., the value of the @code{adder} procedure) as
well as on connector objects.  This is useful if we wish to extend the system
with new operations that communicate with constraints directly rather than only
indirectly via operations on connectors.  Although it is easy to implement the
expression-oriented style in terms of the imperative implementation, it is very
difficult to do the converse.}
@end quotation

@node	3-4, 3-5, 3-3, Chapter 3
@section Concurrency: Time Is of the Essence

We've seen the power of computational objects with local state as tools for
modeling.  Yet, as section @ref{3-1-3} warned, this power extracts a price: the
loss of referential transparency, giving rise to a thicket of questions about
sameness and change, and the need to abandon the substitution model of
evaluation in favor of the more intricate environment model.

The central issue lurking beneath the complexity of state, sameness, and change
is that by introducing assignment we are forced to admit @newterm{time} into
our computational models.  Before we introduced assignment, all our programs
were timeless, in the sense that any expression that has a value always has the
same value.  In contrast, recall the example of modeling withdrawals from a
bank account and returning the resulting balance, introduced at the beginning
of section @ref{3-1-1}:

@lisp
(withdraw 25)
@i{75}

(withdraw 25)
@i{50}
@end lisp

Here successive evaluations of the same expression yield different values.
This behavior arises from the fact that the execution of assignment statements
(in this case, assignments to the variable @code{balance}) delineates
@newterm{moments in time} when values change.  The result of evaluating an
expression depends not only on the expression itself, but also on whether the
evaluation occurs before or after these moments.  Building models in terms of
computational objects with local state forces us to confront time as an
essential concept in programming.

We can go further in structuring computational models to match our perception
of the physical world.  Objects in the world do not change one at a time in
sequence.  Rather we perceive them as acting @newterm{concurrently}---all at
once.  So it is often natural to model systems as collections of computational
processes that execute concurrently.  Just as we can make our programs modular
by organizing models in terms of objects with separate local state, it is often
appropriate to divide computational models into parts that evolve separately
and concurrently.  Even if the programs are to be executed on a sequential
computer, the practice of writing programs as if they were to be executed
concurrently forces the programmer to avoid inessential timing constraints and
thus makes programs more modular.

In addition to making programs more modular, concurrent computation can provide
a speed advantage over sequential computation.  Sequential computers execute
only one operation at a time, so the amount of time it takes to perform a task
is proportional to the total number of operations performed.@footnote{Most real
processors actually execute a few operations at a time, following a strategy
called @newterm{pipelining}.  Although this technique greatly improves the
effective utilization of the hardware, it is used only to speed up the
execution of a sequential instruction stream, while retaining the behavior of
the sequential program.}  However, if it is possible to decompose a problem
into pieces that are relatively independent and need to communicate only
rarely, it may be possible to allocate pieces to separate computing processors,
producing a speed advantage proportional to the number of processors available.

Unfortunately, the complexities introduced by assignment become even more
problematic in the presence of concurrency.  The fact of concurrent execution,
either because the world operates in parallel or because our computers do,
entails additional complexity in our understanding of time.

@menu
* 3-4-1::            The Nature of Time in Concurrent Systems
* 3-4-2::            Mechanisms for Controlling Concurrency
@end menu

@node	3-4-1, 3-4-2, 3-4, 3-4
@subsection The Nature of Time in Concurrent Systems

On the surface, time seems straightforward.  It is an ordering imposed on
events.@footnote{To quote some graffiti seen on a Cambridge building wall:
``Time is a device that was invented to keep everything from happening at
once.''}  For any events @i{A} and @i{B}, either @i{A} occurs before @i{B},
@i{A} and @i{B} are simultaneous, or @i{A} occurs after @i{B}.  For instance,
returning to the bank account example, suppose that Peter withdraws $10 and
Paul withdraws $25 from a joint account that initially contains $100, leaving
$65 in the account.  Depending on the order of the two withdrawals, the
sequence of balances in the account is either $100 -> $90 -> $65 or $100 -> $75
-> $65.  In a computer implementation of the banking system, this changing
sequence of balances could be modeled by successive assignments to a variable
@code{balance}.

In complex situations, however, such a view can be problematic.  Suppose that
Peter and Paul, and other people besides, are accessing the same bank account
through a network of banking machines distributed all over the world.  The
actual sequence of balances in the account will depend critically on the
detailed timing of the accesses and the details of the communication among the
machines.

This indeterminacy in the order of events can pose serious problems in the
design of concurrent systems.  For instance, suppose that the withdrawals made
by Peter and Paul are implemented as two separate processes sharing a common
variable @code{balance}, each process specified by the procedure given in
section @ref{3-1-1}:

@lisp
(define (withdraw amount)
  (if (>= balance amount)
      (begin (set! balance (- balance amount))
             balance)
      "Insufficient funds"))
@end lisp

If the two processes operate independently, then Peter might test the
balance and attempt to withdraw a legitimate amount.  However, Paul
might withdraw some funds in between the time that Peter checks the
balance and the time Peter completes the withdrawal, thus invalidating
Peter's test.

Things can be worse still.  Consider the expression

@lisp
(set! balance (- balance amount))
@end lisp

@noindent
executed as part of each withdrawal process.  This consists of three steps: (1)
accessing the value of the @code{balance} variable; (2) computing the new
balance; (3) setting @code{balance} to this new value.  If Peter and Paul's
withdrawals execute this statement concurrently, then the two withdrawals might
interleave the order in which they access @code{balance} and set it to the new
value.

The timing diagram in @ref{Figure 3-29} depicts an order of events where
@code{balance} starts at 100, Peter withdraws 10, Paul withdraws 25, and yet
the final value of @code{balance} is 75.  As shown in the diagram, the reason
for this anomaly is that Paul's assignment of 75 to @code{balance} is made
under the assumption that the value of @code{balance} to be decremented is 100.
That assumption, however, became invalid when Peter changed @code{balance} to
90.  This is a catastrophic failure for the banking system, because the total
amount of money in the system is not conserved.  Before the transactions, the
total amount of money was $100.  Afterwards, Peter has $10, Paul has $25, and
the bank has $75.@footnote{An even worse failure for this system could occur if
the two @code{set!} operations attempt to change the balance simultaneously, in
which case the actual data appearing in memory might end up being a random
combination of the information being written by the two processes.  Most
computers have interlocks on the primitive memory-write operations, which
protect against such simultaneous access.  Even this seemingly simple kind of
protection, however, raises implementation challenges in the design of
multiprocessing computers, where elaborate @newterm{cache-coherence} protocols
are required to ensure that the various processors will maintain a consistent
view of memory contents, despite the fact that data may be replicated
(``cached'') among the different processors to increase the speed of memory
access.}

The general phenomenon illustrated here is that several processes may share a
common state variable.  What makes this complicated is that more than one
process may be trying to manipulate the shared state at the same time.  For the
bank account example, during each transaction, each customer should be able to
act as if the other customers did not exist.  When a customer changes the
balance in a way that depends on the balance, he must be able to assume that,
just before the moment of change, the balance is still what he thought it was.

@subsubheading Correct behavior of concurrent programs

The above example typifies the subtle bugs that can creep into concurrent
programs.  The root of this complexity lies in the assignments to variables
that are shared among the different processes.  We already know that we must be
careful in writing programs that use @code{set!}, because the results of a
computation depend on the order in which the assignments occur.@footnote{The
factorial program in section @ref{3-1-3} illustrates this for a single
sequential process.}  With concurrent processes we must be especially careful
about assignments, because we may not be able to control the order of the
assignments made by the different processes.  If several such changes might be
made concurrently (as with two depositors accessing a joint account) we need
some way to ensure that our system behaves correctly.  For example, in the case
of withdrawals from a joint bank account, we must ensure that money is
conserved.  To make concurrent programs behave correctly, we may have to place
some restrictions on concurrent execution.

@quotation
@strong{@anchor{Figure 3-29}Figure 3.29:} Timing diagram showing how
interleaving the order of events in two banking withdrawals can lead to an
incorrect final balance.

@example
 |           Peter              Bank              Paul
 |                              ____
 |                             /    \
 |             .--------------| $100 |-------------.
 |             |               \____/              |
 |             V                                   V
 |  .----------------------.            .----------------------.           
 |  | Access balance: $100 |            | Access balance: $100 |
 |  `----------+-----------'            `----------+-----------'
 |             V                                   V
 |  .----------------------.            .----------------------.           
 |  | new value: 100-10=90 |            | new value: 100-25=75 |
 |  `----------+-----------'            `----------+-----------'
 |             V                                   |
 |  .----------------------.                       |
 |  | set! balance to $90  |                       |
 |  `----------+-----------'    ____               |
 |             |               /    \              |
 |             `------------->| $ 90 |             V
 |                             \____/   .----------------------.
 |                                      | new value: 100-25=75 |
 |                              ____    `----------+-----------'
 |                             /    \              |
 |                            | $ 90 |<------------'
 V                             \____/
time
@end example
@end quotation

One possible restriction on concurrency would stipulate that no two operations
that change any shared state variables can occur at the same time.  This is an
extremely stringent requirement.  For distributed banking, it would require the
system designer to ensure that only one transaction could proceed at a time.
This would be both inefficient and overly conservative.  @ref{Figure 3-30}
shows Peter and Paul sharing a bank account, where Paul has a private account
as well.  The diagram illustrates two withdrawals from the shared account (one
by Peter and one by Paul) and a deposit to Paul's private account.@footnote{The
columns show the contents of Peter's wallet, the joint account (in Bank1),
Paul's wallet, and Paul's private account (in Bank2), before and after each
withdrawal (W) and deposit (D).  Peter withdraws $10 from Bank1; Paul deposits
$5 in Bank2, then withdraws $25 from Bank1.}  The two withdrawals from the
shared account must not be concurrent (since both access and update the same
account), and Paul's deposit and withdrawal must not be concurrent (since both
access and update the amount in Paul's wallet).  But there should be no problem
permitting Paul's deposit to his private account to proceed concurrently with
Peter's withdrawal from the shared account.

@quotation
@strong{@anchor{Figure 3-30}Figure 3.30:} Concurrent deposits and withdrawals
from a joint account in Bank1 and a private account in Bank2.

@example
 |    Peter          Bank1          Paul           Bank2
 |    ____           ____           ____           ____  
 |   /    \         /    \         /    \         /    \ 
 |  |  $7  |--. .--| $100 |       |  $5  |--. .--| $300 |
 |   \____/   V V   \____/         \____/   V V   \____/ 
 |           +---+                         +---+         
 |           | W |                         | D |         
 |    ____   ++-++   ____           ____   ++-++   ____  
 |   /    \   | |   /    \         /    \   | |   /    \ 
 |  | $17  |<-' `->| $90  |--. .--|  $0  |<-' `->| $305 |
 |   \____/         \____/   V V   \____/         \____/ 
 |                          +---+
 |                          | W |
 |    ____           ____   ++-++   ____           ____  
 |   /    \         /    \   | |   /    \         /    \ 
 |  | $17  |       | $65  |<-' `->| $25  |       | $305 |
 |   \____/         \____/         \____/         \____/ 
 V
time
@end example
@end quotation

A less stringent restriction on concurrency would ensure that a concurrent
system produces the same result as if the processes had run sequentially in
some order.  There are two important aspects to this requirement.  First, it
does not require the processes to actually run sequentially, but only to
produce results that are the same @emph{as if} they had run sequentially.  For
the example in @ref{Figure 3-30}, the designer of the bank account system can
safely allow Paul's deposit and Peter's withdrawal to happen concurrently,
because the net result will be the same as if the two operations had happened
sequentially.  Second, there may be more than one possible ``correct'' result
produced by a concurrent program, because we require only that the result be
the same as for @emph{some} sequential order.  For example, suppose that Peter
and Paul's joint account starts out with $100, and Peter deposits $40 while
Paul concurrently withdraws half the money in the account.  Then sequential
execution could result in the account balance being either $70 or $90 (see
@ref{Exercise 3-38}).@footnote{[Footnote 39]@anchor{Footnote 39} A more formal
way to express this idea is to say that concurrent programs are inherently
@newterm{nondeterministic}. That is, they are described not by single-valued
functions, but by functions whose results are sets of possible values.  In
section @ref{4-3} we will study a language for expressing nondeterministic
computations.}

There are still weaker requirements for correct execution of concurrent
programs.  A program for simulating diffusion (say, the flow of heat in an
object) might consist of a large number of processes, each one representing a
small volume of space, that update their values concurrently.  Each process
repeatedly changes its value to the average of its own value and its neighbors'
values.  This algorithm converges to the right answer independent of the order
in which the operations are done; there is no need for any restrictions on
concurrent use of the shared values.

@quotation
@strong{@anchor{Exercise 3-38}Exercise 3.38:} Suppose that Peter, Paul, and
Mary share a joint bank account that initially contains $100.  Concurrently,
Peter deposits $10, Paul withdraws $20, and Mary withdraws half the money in
the account, by executing the following commands:

@example
Peter: (set! balance (+ balance 10))
Paul:  (set! balance (- balance 20))
Mary:  (set! balance (- balance (/ balance 2)))
@end example

@enumerate a

@item
List all the different possible values for @code{balance} after these three
transactions have been completed, assuming that the banking system forces the
three processes to run sequentially in some order.

@item
What are some other values that could be produced if the system allows the
processes to be interleaved?  Draw timing diagrams like the one in @ref{Figure
3-29} to explain how these values can occur.

@end enumerate
@end quotation

@node	3-4-2,  , 3-4-1, 3-4
@subsection Mechanisms for Controlling Concurrency

We've seen that the difficulty in dealing with concurrent processes is rooted
in the need to consider the interleaving of the order of events in the
different processes.  For example, suppose we have two processes, one with
three ordered events (@i{a},@i{b},@i{c}) and one with three ordered events
(@i{x},@i{y},@i{z}).  If the two processes run concurrently, with no
constraints on how their execution is interleaved, then there are 20 different
possible orderings for the events that are consistent with the individual
orderings for the two processes:

@example
(a,b,c,x,y,z)  (a,x,b,y,c,z)  (x,a,b,c,y,z)  (x,a,y,z,b,c)
(a,b,x,c,y,z)  (a,x,b,y,z,c)  (x,a,b,y,c,z)  (x,y,a,b,c,z)
(a,b,x,y,c,z)  (a,x,y,b,c,z)  (x,a,b,y,z,c)  (x,y,a,b,z,c)
(a,b,x,y,z,c)  (a,x,y,b,z,c)  (x,a,y,b,c,z)  (x,y,a,z,b,c)
(a,x,b,c,y,z)  (a,x,y,z,b,c)  (x,a,y,b,z,c)  (x,y,z,a,b,c)
@end example

As programmers designing this system, we would have to consider the effects of
each of these 20 orderings and check that each behavior is acceptable.  Such an
approach rapidly becomes unwieldy as the numbers of processes and events
increase.

A more practical approach to the design of concurrent systems is to devise
general mechanisms that allow us to constrain the interleaving of concurrent
processes so that we can be sure that the program behavior is correct.  Many
mechanisms have been developed for this purpose.  In this section, we describe
one of them, the @newterm{serializer}.

@subsubheading Serializing access to shared state

Serialization implements the following idea: Processes will execute
concurrently, but there will be certain collections of procedures that cannot
be executed concurrently.  More precisely, serialization creates distinguished
sets of procedures such that only one execution of a procedure in each
serialized set is permitted to happen at a time.  If some procedure in the set
is being executed, then a process that attempts to execute any procedure in the
set will be forced to wait until the first execution has finished.

We can use serialization to control access to shared variables.  For example,
if we want to update a shared variable based on the previous value of that
variable, we put the access to the previous value of the variable and the
assignment of the new value to the variable in the same procedure.  We then
ensure that no other procedure that assigns to the variable can run
concurrently with this procedure by serializing all of these procedures with
the same serializer.  This guarantees that the value of the variable cannot be
changed between an access and the corresponding assignment.

@subsubheading Serializers in Scheme

To make the above mechanism more concrete, suppose that we have extended Scheme
to include a procedure called @code{parallel-execute}:

@lisp
(parallel-execute <@var{p_1}> <@var{p_2}> @dots{} <@var{p_@i{k}}>)
@end lisp

Each <@var{p}> must be a procedure of no arguments.  @code{Parallel-execute}
creates a separate process for each <@var{p}>, which applies <@var{p}> (to no
arguments).  These processes all run
concurrently.@footnote{@code{Parallel-execute} is not part of standard Scheme,
but it can be implemented in @acronym{MIT} Scheme.  In our implementation, the
new concurrent processes also run concurrently with the original Scheme
process.  Also, in our implementation, the value returned by
@code{parallel-execute} is a special control object that can be used to halt
the newly created processes.}

As an example of how this is used, consider

@lisp
(define x 10)

(parallel-execute (lambda () (set! x (* x x)))
                  (lambda () (set! x (+ x 1))))
@end lisp

This creates two concurrent processes---@i{P}_1, which sets @code{x} to
@code{x} times @code{x}, and @i{P}_2, which increments @code{x}.  After
execution is complete, @code{x} will be left with one of five possible values,
depending on the interleaving of the events of @i{P}_1 and @i{P}_2:

@example
101: @i{P}_1 sets @code{x} to 100 and then @i{P}_2 increments
     @code{x} to 101.
121: @i{P}_2 increments @code{x} to 11 and then @i{P}_1 sets
     @code{x} to @code{x} times @code{x}.
110: @i{P}_2 changes @code{x} from 10 to 11 between the two
     times that @i{P}_1 accesses the value of @code{x} during
     the evaluation of @code{(* x x)}.
11:  @i{P}_2 accesses @code{x}, then @i{P}_1 sets @code{x} to
     100, then @i{P}_2 sets @code{x}.
100: @i{P}_1 accesses @code{x} (twice), then @i{P}_2 sets
     @code{x} to 11, then @i{P}_1 sets @code{x}.
@end example

We can constrain the concurrency by using serialized procedures, which are
created by @newterm{serializers}. Serializers are constructed by
@code{make-serializer}, whose implementation is given below.  A serializer
takes a procedure as argument and returns a serialized procedure that behaves
like the original procedure.  All calls to a given serializer return serialized
procedures in the same set.

Thus, in contrast to the example above, executing

@lisp
(define x 10)

(define s (make-serializer))

(parallel-execute (s (lambda () (set! x (* x x))))
                  (s (lambda () (set! x (+ x 1)))))
@end lisp

@noindent
can produce only two possible values for @code{x}, 101 or 121.  The other
possibilities are eliminated, because the execution of @i{P}_1 and @i{P}_2
cannot be interleaved.

Here is a version of the @code{make-account} procedure from section
@ref{3-1-1}, where the deposits and withdrawals have been serialized:

@lisp
(define (make-account balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  (let ((protected (make-serializer)))
    (define (dispatch m)
      (cond ((eq? m 'withdraw) (protected withdraw))
            ((eq? m 'deposit) (protected deposit))
            ((eq? m 'balance) balance)
            (else (error "Unknown request -- MAKE-ACCOUNT"
                         m))))
    dispatch))
@end lisp

With this implementation, two processes cannot be withdrawing from or
depositing into a single account concurrently.  This eliminates the source of
the error illustrated in @ref{Figure 3-29}, where Peter changes the account
balance between the times when Paul accesses the balance to compute the new
value and when Paul actually performs the assignment.  On the other hand, each
account has its own serializer, so that deposits and withdrawals for different
accounts can proceed concurrently.

@quotation
@strong{@anchor{Exercise 3-39}Exercise 3.39:} Which of the five possibilities
in the parallel execution shown above remain if we instead serialize execution
as follows:

@lisp
(define x 10)

(define s (make-serializer))

(parallel-execute (lambda () (set! x ((s (lambda () (* x x))))))
                  (s (lambda () (set! x (+ x 1)))))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-40}Exercise 3.40:} Give all possible values of
@code{x} that can result from executing

@lisp
(define x 10)

(parallel-execute (lambda () (set! x (* x x)))
                  (lambda () (set! x (* x x x))))
@end lisp

Which of these possibilities remain if we instead use serialized procedures:

@lisp
(define x 10)

(define s (make-serializer))

(parallel-execute (s (lambda () (set! x (* x x))))
                  (s (lambda () (set! x (* x x x)))))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-41}Exercise 3.41:} Ben Bitdiddle worries that it
would be better to implement the bank account as follows (where the commented
line has been changed):

@lisp
(define (make-account balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  @r{;; continued on next page}

  (let ((protected (make-serializer)))
    (define (dispatch m)
      (cond ((eq? m 'withdraw) (protected withdraw))
            ((eq? m 'deposit) (protected deposit))
            ((eq? m 'balance)
             ((protected (lambda () balance)))) @r{; serialized}
            (else (error "Unknown request -- MAKE-ACCOUNT"
                         m))))
    dispatch))
@end lisp

@noindent
because allowing unserialized access to the bank balance can result in
anomalous behavior.  Do you agree?  Is there any scenario that demonstrates
Ben's concern?
@end quotation

@quotation
@strong{@anchor{Exercise 3-42}Exercise 3.42:} Ben Bitdiddle suggests that it's
a waste of time to create a new serialized procedure in response to every
@code{withdraw} and @code{deposit} message.  He says that @code{make-account}
could be changed so that the calls to @code{protected} are done outside the
@code{dispatch} procedure.  That is, an account would return the same
serialized procedure (which was created at the same time as the account) each
time it is asked for a withdrawal procedure.

@lisp
(define (make-account balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  (let ((protected (make-serializer)))
    (let ((protected-withdraw (protected withdraw))
          (protected-deposit (protected deposit)))
      (define (dispatch m)
        (cond ((eq? m 'withdraw) protected-withdraw)
              ((eq? m 'deposit) protected-deposit)
              ((eq? m 'balance) balance)
              (else (error "Unknown request -- MAKE-ACCOUNT"
                           m))))
      dispatch)))
@end lisp

Is this a safe change to make?  In particular, is there any difference in what
concurrency is allowed by these two versions of @code{make-account} ?
@end quotation

@subsubheading Complexity of using multiple shared resources

Serializers provide a powerful abstraction that helps isolate the complexities
of concurrent programs so that they can be dealt with carefully and (hopefully)
correctly.  However, while using serializers is relatively straightforward when
there is only a single shared resource (such as a single bank account),
concurrent programming can be treacherously difficult when there are multiple
shared resources.

To illustrate one of the difficulties that can arise, suppose we wish to swap
the balances in two bank accounts.  We access each account to find the balance,
compute the difference between the balances, withdraw this difference from one
account, and deposit it in the other account.  We could implement this as
follows:@footnote{We have simplified @code{exchange} by exploiting the fact
that our @code{deposit} message accepts negative amounts.  (This is a serious
bug in our banking system!)}

@lisp
(define (exchange account1 account2)
  (let ((difference (- (account1 'balance)
                       (account2 'balance))))
    ((account1 'withdraw) difference)
    ((account2 'deposit) difference)))
@end lisp

This procedure works well when only a single process is trying to do the
exchange.  Suppose, however, that Peter and Paul both have access to accounts
@i{a}1, @i{a}2, and @i{a}3, and that Peter exchanges @i{a}1 and @i{a}2 while
Paul concurrently exchanges @i{a}1 and @i{a}3.  Even with account deposits and
withdrawals serialized for individual accounts (as in the @code{make-account}
procedure shown above in this section), @code{exchange} can still produce
incorrect results.  For example, Peter might compute the difference in the
balances for @i{a}1 and @i{a}2, but then Paul might change the balance in
@i{a}1 before Peter is able to complete the exchange.@footnote{If the account
balances start out as $10, $20, and $30, then after any number of concurrent
exchanges, the balances should still be $10, $20, and $30 in some order.
Serializing the deposits to individual accounts is not sufficient to guarantee
this.  See @ref{Exercise 3-43}.}  For correct behavior, we must arrange for the
@code{exchange} procedure to lock out any other concurrent accesses to the
accounts during the entire time of the exchange.

One way we can accomplish this is by using both accounts' serializers to
serialize the entire @code{exchange} procedure.  To do this, we will arrange
for access to an account's serializer.  Note that we are deliberately breaking
the modularity of the bank-account object by exposing the serializer.  The
following version of @code{make-account} is identical to the original version
given in section @ref{3-1-1}, except that a serializer is provided to protect
the balance variable, and the serializer is exported via message passing:

@lisp
(define (make-account-and-serializer balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  (let ((balance-serializer (make-serializer)))
    (define (dispatch m)
      (cond ((eq? m 'withdraw) withdraw)
            ((eq? m 'deposit) deposit)
            ((eq? m 'balance) balance)
            ((eq? m 'serializer) balance-serializer)
            (else (error "Unknown request -- MAKE-ACCOUNT"
                         m))))
    dispatch))
@end lisp

We can use this to do serialized deposits and withdrawals.  However, unlike our
earlier serialized account, it is now the responsibility of each user of
bank-account objects to explicitly manage the serialization, for example as
follows:@footnote{@ref{Exercise 3-45} investigates why deposits and withdrawals
are no longer automatically serialized by the account.}

@lisp
(define (deposit account amount)
  (let ((s (account 'serializer))
        (d (account 'deposit)))
    ((s d) amount)))
@end lisp

Exporting the serializer in this way gives us enough flexibility to implement a
serialized exchange program.  We simply serialize the original @code{exchange}
procedure with the serializers for both accounts:

@lisp
(define (serialized-exchange account1 account2)
  (let ((serializer1 (account1 'serializer))
        (serializer2 (account2 'serializer)))
    ((serializer1 (serializer2 exchange))
     account1
     account2)))
@end lisp

@quotation
@strong{@anchor{Exercise 3-43}Exercise 3.43:} Suppose that the balances in
three accounts start out as $10, $20, and $30, and that multiple processes run,
exchanging the balances in the accounts.  Argue that if the processes are run
sequentially, after any number of concurrent exchanges, the account balances
should be $10, $20, and $30 in some order.  Draw a timing diagram like the one
in @ref{Figure 3-29} to show how this condition can be violated if the
exchanges are implemented using the first version of the account-exchange
program in this section.  On the other hand, argue that even with this
@code{exchange} program, the sum of the balances in the accounts will be
preserved.  Draw a timing diagram to show how even this condition would be
violated if we did not serialize the transactions on individual accounts.
@end quotation

@quotation
@strong{@anchor{Exercise 3-44}Exercise 3.44:} Consider the problem of
transferring an amount from one account to another.  Ben Bitdiddle claims that
this can be accomplished with the following procedure, even if there are
multiple people concurrently transferring money among multiple accounts, using
any account mechanism that serializes deposit and withdrawal transactions, for
example, the version of @code{make-account} in the text above.

@lisp
(define (transfer from-account to-account amount)
  ((from-account 'withdraw) amount)
  ((to-account 'deposit) amount))
@end lisp

Louis Reasoner claims that there is a problem here, and that we need to use a
more sophisticated method, such as the one required for dealing with the
exchange problem.  Is Louis right?  If not, what is the essential difference
between the transfer problem and the exchange problem?  (You should assume that
the balance in @code{from-account} is at least @code{amount}.)
@end quotation

@quotation
@strong{@anchor{Exercise 3-45}Exercise 3.45:} Louis Reasoner thinks our
bank-account system is unnecessarily complex and error-prone now that deposits
and withdrawals aren't automatically serialized.  He suggests that
@code{make-account-and-serializer} should have exported the serializer (for use
by such procedures as @code{serialized-exchange}) in addition to (rather than
instead of) using it to serialize accounts and deposits as @code{make-account}
did.  He proposes to redefine accounts as follows:

@lisp
(define (make-account-and-serializer balance)
  (define (withdraw amount)
    (if (>= balance amount)
        (begin (set! balance (- balance amount))
               balance)
        "Insufficient funds"))
  (define (deposit amount)
    (set! balance (+ balance amount))
    balance)
  (let ((balance-serializer (make-serializer)))
    (define (dispatch m)
      (cond ((eq? m 'withdraw) (balance-serializer withdraw))
            ((eq? m 'deposit) (balance-serializer deposit))
            ((eq? m 'balance) balance)
            ((eq? m 'serializer) balance-serializer)
            (else (error "Unknown request -- MAKE-ACCOUNT"
                         m))))
    dispatch))
@end lisp

Then deposits are handled as with the original @code{make-account}:

@lisp
(define (deposit account amount)
 ((account 'deposit) amount))
@end lisp

Explain what is wrong with Louis's reasoning.  In particular, consider what
happens when @code{serialized-exchange} is called.
@end quotation

@subsubheading Implementing serializers

We implement serializers in terms of a more primitive synchronization mechanism
called a @newterm{mutex}.  A mutex is an object that supports two
operations---the mutex can be @newterm{acquired}, and the mutex can be
@newterm{released}.  Once a mutex has been acquired, no other acquire
operations on that mutex may proceed until the mutex is released.@footnote{The
term ``mutex'' is an abbreviation for @newterm{mutual exclusion}.  The general
problem of arranging a mechanism that permits concurrent processes to safely
share resources is called the mutual exclusion problem.  Our mutex is a simple
variant of the @newterm{semaphore} mechanism (see @ref{Exercise 3-47}), which
was introduced in the ``THE'' Multiprogramming System developed at the
Technological University of Eindhoven and named for the university's initials
in Dutch (Dijkstra 1968a).  The acquire and release operations were originally
called P and V, from the Dutch words @emph{passeren} (to pass) and
@emph{vrijgeven} (to release), in reference to the semaphores used on railroad
systems.  Dijkstra's classic exposition (1968b) was one of the first to clearly
present the issues of concurrency control, and showed how to use semaphores to
handle a variety of concurrency problems.} In our implementation, each
serializer has an associated mutex.  Given a procedure @code{p}, the serializer
returns a procedure that acquires the mutex, runs @code{p}, and then releases
the mutex.  This ensures that only one of the procedures produced by the
serializer can be running at once, which is precisely the serialization
property that we need to guarantee.

@lisp
(define (make-serializer)
  (let ((mutex (make-mutex)))
    (lambda (p)
      (define (serialized-p . args)
        (mutex 'acquire)
        (let ((val (apply p args)))
          (mutex 'release)
          val))
      serialized-p)))
@end lisp

The mutex is a mutable object (here we'll use a one-element list, which we'll
refer to as a @newterm{cell}) that can hold the value true or false.  When the
value is false, the mutex is available to be acquired.  When the value is true,
the mutex is unavailable, and any process that attempts to acquire the mutex
must wait.

Our mutex constructor @code{make-mutex} begins by initializing the cell
contents to false.  To acquire the mutex, we test the cell.  If the mutex is
available, we set the cell contents to true and proceed.  Otherwise, we wait in
a loop, attempting to acquire over and over again, until we find that the mutex
is available.@footnote{In most time-shared operating systems, processes that
are blocked by a mutex do not waste time ``busy-waiting'' as above.  Instead,
the system schedules another process to run while the first is waiting, and the
blocked process is awakened when the mutex becomes available.}  To release the
mutex, we set the cell contents to false.

@lisp
(define (make-mutex)
  (let ((cell (list false)))
    (define (the-mutex m)
      (cond ((eq? m 'acquire)
             (if (test-and-set! cell)
                 (the-mutex 'acquire))) @r{; retry}
            ((eq? m 'release) (clear! cell))))
    the-mutex))

(define (clear! cell)
  (set-car! cell false))
@end lisp

@code{Test-and-set!} tests the cell and returns the result of the test.  In
addition, if the test was false, @code{test-and-set!} sets the cell contents to
true before returning false.  We can express this behavior as the following
procedure:

@lisp
(define (test-and-set! cell)
  (if (car cell)
      true
      (begin (set-car! cell true)
             false)))
@end lisp

However, this implementation of @code{test-and-set!} does not suffice as it
stands.  There is a crucial subtlety here, which is the essential place where
concurrency control enters the system: The @code{test-and-set!} operation must
be performed @newterm{atomically}.  That is, we must guarantee that, once a
process has tested the cell and found it to be false, the cell contents will
actually be set to true before any other process can test the cell.  If we do
not make this guarantee, then the mutex can fail in a way similar to the
bank-account failure in @ref{Figure 3-29}.  (See @ref{Exercise 3-46}.)

The actual implementation of @code{test-and-set!} depends on the details of how
our system runs concurrent processes.  For example, we might be executing
concurrent processes on a sequential processor using a time-slicing mechanism
that cycles through the processes, permitting each process to run for a short
time before interrupting it and moving on to the next process.  In that case,
@code{test-and-set!}  can work by disabling time slicing during the testing and
setting.@footnote{In @acronym{MIT} Scheme for a single processor, which uses a
time-slicing model, @code{test-and-set!} can be implemented as follows:

@lisp
(define (test-and-set! cell)
  (without-interrupts
   (lambda ()
     (if (car cell)
         true
         (begin (set-car! cell true)
                false)))))
@end lisp

@code{Without-interrupts} disables time-slicing interrupts while its procedure
argument is being executed.}  Alternatively, multiprocessing computers provide
instructions that support atomic operations directly in
hardware.@footnote{There are many variants of such instructions---including
test-and-set, test-and-clear, swap, compare-and-exchange, load-reserve, and
store-conditional---whose design must be carefully matched to the machine's
processor-memory interface.  One issue that arises here is to determine what
happens if two processes attempt to acquire the same resource at exactly the
same time by using such an instruction.  This requires some mechanism for
making a decision about which process gets control.  Such a mechanism is called
an @newterm{arbiter}.  Arbiters usually boil down to some sort of hardware
device.  Unfortunately, it is possible to prove that one cannot physically
construct a fair arbiter that works 100% of the time unless one allows the
arbiter an arbitrarily long time to make its decision.  The fundamental
phenomenon here was originally observed by the fourteenth-century French
philosopher Jean Buridan in his commentary on Aristotle's @i{De caelo}.
Buridan argued that a perfectly rational dog placed between two equally
attractive sources of food will starve to death, because it is incapable of
deciding which to go to first.}

@quotation
@strong{@anchor{Exercise 3-46}Exercise 3.46:} Suppose that we implement
@code{test-and-set!}  using an ordinary procedure as shown in the text, without
attempting to make the operation atomic.  Draw a timing diagram like the one in
@ref{Figure 3-29} to demonstrate how the mutex implementation can fail by
allowing two processes to acquire the mutex at the same time.
@end quotation

@quotation
@strong{@anchor{Exercise 3-47}Exercise 3.47:} A semaphore (of size @i{n}) is a
generalization of a mutex.  Like a mutex, a semaphore supports acquire and
release operations, but it is more general in that up to @i{n} processes can
acquire it concurrently.  Additional processes that attempt to acquire the
semaphore must wait for release operations.  Give implementations of semaphores

@enumerate a

@item
in terms of mutexes

@item
in terms of atomic @code{test-and-set!} operations.

@end enumerate
@end quotation

@subsubheading Deadlock

Now that we have seen how to implement serializers, we can see that account
exchanging still has a problem, even with the @code{serialized-exchange}
procedure above.  Imagine that Peter attempts to exchange @i{a}1 with @i{a}2
while Paul concurrently attempts to exchange @i{a}2 with @i{a}1.  Suppose that
Peter's process reaches the point where it has entered a serialized procedure
protecting @i{a}1 and, just after that, Paul's process enters a serialized
procedure protecting @i{a}2.  Now Peter cannot proceed (to enter a serialized
procedure protecting @i{a}2) until Paul exits the serialized procedure
protecting @i{a}2.  Similarly, Paul cannot proceed until Peter exits the
serialized procedure protecting @i{a}1.  Each process is stalled forever,
waiting for the other.  This situation is called a @newterm{deadlock}.
Deadlock is always a danger in systems that provide concurrent access to
multiple shared resources.

One way to avoid the deadlock in this situation is to give each account a
unique identification number and rewrite @code{serialized-exchange} so that a
process will always attempt to enter a procedure protecting the lowest-numbered
account first.  Although this method works well for the exchange problem, there
are other situations that require more sophisticated deadlock-avoidance
techniques, or where deadlock cannot be avoided at all.  (See @ref{Exercise
3-48} and @ref{Exercise 3-49}.)@footnote{The general technique for avoiding
deadlock by numbering the shared resources and acquiring them in order is due
to Havender (1968).  Situations where deadlock cannot be avoided require
@newterm{deadlock-recovery} methods, which entail having processes ``back out''
of the deadlocked state and try again.  Deadlock-recovery mechanisms are widely
used in database management systems, a topic that is treated in detail in Gray
and Reuter 1993.}

@quotation
@strong{@anchor{Exercise 3-48}Exercise 3.48:} Explain in detail why the
deadlock-avoidance method described above, (i.e., the accounts are numbered,
and each process attempts to acquire the smaller-numbered account first) avoids
deadlock in the exchange problem.  Rewrite @code{serialized-exchange} to
incorporate this idea.  (You will also need to modify @code{make-account} so
that each account is created with a number, which can be accessed by sending an
appropriate message.)
@end quotation

@quotation
@strong{@anchor{Exercise 3-49}Exercise 3.49:} Give a scenario where the
deadlock-avoidance mechanism described above does not work.  (Hint: In the
exchange problem, each process knows in advance which accounts it will need to
get access to.  Consider a situation where a process must get access to some
shared resources before it can know which additional shared resources it will
require.)
@end quotation

@subsubheading Concurrency, time, and communication

We've seen how programming concurrent systems requires controlling the ordering
of events when different processes access shared state, and we've seen how to
achieve this control through judicious use of serializers.  But the problems of
concurrency lie deeper than this, because, from a fundamental point of view,
it's not always clear what is meant by ``shared state.''

Mechanisms such as @code{test-and-set!} require processes to examine a global
shared flag at arbitrary times.  This is problematic and inefficient to
implement in modern high-speed processors, where due to optimization techniques
such as pipelining and cached memory, the contents of memory may not be in a
consistent state at every instant.  In contemporary multiprocessing systems,
therefore, the serializer paradigm is being supplanted by new approaches to
concurrency control.@footnote{One such alternative to serialization is called
@newterm{barrier synchronization}.  The programmer permits concurrent processes
to execute as they please, but establishes certain synchronization points
(``barriers'') through which no process can proceed until all the processes
have reached the barrier.  Modern processors provide machine instructions that
permit programmers to establish synchronization points at places where
consistency is required.  The PowerPC^( @i{T}@i{M}), for example, includes for
this purpose two instructions called SYNC and EIEIO (Enforced In-order
Execution of Input/Output).}

The problematic aspects of shared state also arise in large, distributed
systems.  For instance, imagine a distributed banking system where individual
branch banks maintain local values for bank balances and periodically compare
these with values maintained by other branches.  In such a system the value of
``the account balance'' would be undetermined, except right after
synchronization.  If Peter deposits money in an account he holds jointly with
Paul, when should we say that the account balance has changed---when the
balance in the local branch changes, or not until after the synchronization?
And if Paul accesses the account from a different branch, what are the
reasonable constraints to place on the banking system such that the behavior is
``correct''?  The only thing that might matter for correctness is the behavior
observed by Peter and Paul individually and the ``state'' of the account
immediately after synchronization.  Questions about the ``real'' account
balance or the order of events between synchronizations may be irrelevant or
meaningless.@footnote{This may seem like a strange point of view, but there are
systems that work this way.  International charges to credit-card accounts, for
example, are normally cleared on a per-country basis, and the charges made in
different countries are periodically reconciled.  Thus the account balance may
be different in different countries.}

The basic phenomenon here is that synchronizing different processes,
establishing shared state, or imposing an order on events requires
communication among the processes.  In essence, any notion of time in
concurrency control must be intimately tied to communication.@footnote{For
distributed systems, this perspective was pursued by Lamport (1978), who showed
how to use communication to establish ``global clocks'' that can be used to
establish orderings on events in distributed systems.}  It is intriguing that a
similar connection between time and communication also arises in the Theory of
Relativity, where the speed of light (the fastest signal that can be used to
synchronize events) is a fundamental constant relating time and space.  The
complexities we encounter in dealing with time and state in our computational
models may in fact mirror a fundamental complexity of the physical universe.

@node 3-5,  , 3-4, Chapter 3
@section Streams

We've gained a good understanding of assignment as a tool in modeling, as well
as an appreciation of the complex problems that assignment raises. It is time
to ask whether we could have gone about things in a different way, so as to
avoid some of these problems.  In this section, we explore an alternative
approach to modeling state, based on data structures called @newterm{streams}.
As we shall see, streams can mitigate some of the complexity of modeling state.

Let's step back and review where this complexity comes from.  In an attempt to
model real-world phenomena, we made some apparently reasonable decisions: We
modeled real-world objects with local state by computational objects with local
variables.  We identified time variation in the real world with time variation
in the computer.  We implemented the time variation of the states of the model
objects in the computer with assignments to the local variables of the model
objects.

Is there another approach?  Can we avoid identifying time in the computer with
time in the modeled world?  Must we make the model change with time in order to
model phenomena in a changing world?  Think about the issue in terms of
mathematical functions.  We can describe the time-varying behavior of a
quantity @i{x} as a function of time @i{x}(@i{t}).  If we concentrate on @i{x}
instant by instant, we think of it as a changing quantity.  Yet if we
concentrate on the entire time history of values, we do not emphasize
change---the function itself does not change.@footnote{Physicists sometimes
adopt this view by introducing the ``world lines'' of particles as a device for
reasoning about motion.  We've also already mentioned (section @ref{2-2-3})
that this is the natural way to think about signal-processing systems.  We will
explore applications of streams to signal processing in section @ref{3-5-3}.}

If time is measured in discrete steps, then we can model a time function as a
(possibly infinite) sequence.  In this section, we will see how to model change
in terms of sequences that represent the time histories of the systems being
modeled.  To accomplish this, we introduce new data structures called
@newterm{streams}.  From an abstract point of view, a stream is simply a
sequence.  However, we will find that the straightforward implementation of
streams as lists (as in section @ref{2-2-1}) doesn't fully reveal the power of
stream processing.  As an alternative, we introduce the technique of
@newterm{delayed evaluation}, which enables us to represent very large (even
infinite) sequences as streams.

Stream processing lets us model systems that have state without ever using
assignment or mutable data.  This has important implications, both theoretical
and practical, because we can build models that avoid the drawbacks inherent in
introducing assignment.  On the other hand, the stream framework raises
difficulties of its own, and the question of which modeling technique leads to
more modular and more easily maintained systems remains open.

@menu
* 3-5-1::            Streams Are Delayed Lists
* 3-5-2::            Infinite Streams
* 3-5-3::            Exploiting the Stream Paradigm
* 3-5-4::            Streams and Delayed Evaluation
* 3-5-5::            Modularity of Functional Programs and Modularity of
                     Objects
@end menu

@node	3-5-1, 3-5-2, 3-5, 3-5
@subsection Streams Are Delayed Lists

As we saw in section @ref{2-2-3}, sequences can serve as standard interfaces
for combining program modules.  We formulated powerful abstractions for
manipulating sequences, such as @code{map}, @code{filter}, and
@code{accumulate}, that capture a wide variety of operations in a manner that
is both succinct and elegant.

Unfortunately, if we represent sequences as lists, this elegance is bought at
the price of severe inefficiency with respect to both the time and space
required by our computations.  When we represent manipulations on sequences as
transformations of lists, our programs must construct and copy data structures
(which may be huge) at every step of a process.

To see why this is true, let us compare two programs for computing the sum of
all the prime numbers in an interval.  The first program is written in standard
iterative style:@footnote{Assume that we have a predicate @code{prime?} (e.g.,
as in section @ref{1-2-6}) that tests for primality.}

@lisp
(define (sum-primes a b)
  (define (iter count accum)
    (cond ((> count b) accum)
          ((prime? count) (iter (+ count 1) (+ count accum)))
          (else (iter (+ count 1) accum))))
  (iter a 0))
@end lisp

The second program performs the same computation using the sequence operations
of section @ref{2-2-3}:

@lisp
(define (sum-primes a b)
  (accumulate +
              0
              (filter prime? (enumerate-interval a b))))
@end lisp

In carrying out the computation, the first program needs to store only the sum
being accumulated.  In contrast, the filter in the second program cannot do any
testing until @code{enumerate-interval} has constructed a complete list of the
numbers in the interval.  The filter generates another list, which in turn is
passed to @code{accumulate} before being collapsed to form a sum.  Such large
intermediate storage is not needed by the first program, which we can think of
as enumerating the interval incrementally, adding each prime to the sum as it
is generated.

The inefficiency in using lists becomes painfully apparent if we use the
sequence paradigm to compute the second prime in the interval from 10,000 to
1,000,000 by evaluating the expression

@lisp
(car (cdr (filter prime?
                  (enumerate-interval 10000 1000000))))
@end lisp

This expression does find the second prime, but the computational overhead is
outrageous.  We construct a list of almost a million integers, filter this list
by testing each element for primality, and then ignore almost all of the
result.  In a more traditional programming style, we would interleave the
enumeration and the filtering, and stop when we reached the second prime.

Streams are a clever idea that allows one to use sequence manipulations without
incurring the costs of manipulating sequences as lists.  With streams we can
achieve the best of both worlds: We can formulate programs elegantly as
sequence manipulations, while attaining the efficiency of incremental
computation.  The basic idea is to arrange to construct a stream only
partially, and to pass the partial construction to the program that consumes
the stream.  If the consumer attempts to access a part of the stream that has
not yet been constructed, the stream will automatically construct just enough
more of itself to produce the required part, thus preserving the illusion that
the entire stream exists.  In other words, although we will write programs as
if we were processing complete sequences, we design our stream implementation
to automatically and transparently interleave the construction of the stream
with its use.

On the surface, streams are just lists with different names for the procedures
that manipulate them.  There is a constructor, @code{cons-stream}, and two
selectors, @code{stream-car} and @code{stream-cdr}, which satisfy the
constraints

@example
(stream-car (cons-stream x y)) = x
(stream-cdr (cons-stream x y)) = y
@end example

There is a distinguishable object, @code{the-empty-stream}, which cannot be the
result of any @code{cons-stream} operation, and which can be identified with
the predicate @code{stream-null?}.@footnote{In the @acronym{MIT}
implementation, @code{the-empty-stream} is the same as the empty list
@code{'()}, and @code{stream-null?} is the same as @code{null?}.}  Thus we can
make and use streams, in just the same way as we can make and use lists, to
represent aggregate data arranged in a sequence.  In particular, we can build
stream analogs of the list operations from @ref{Chapter 2}, such as
@code{list-ref}, @code{map}, and @code{for-each}:@footnote{This should bother
you.  The fact that we are defining such similar procedures for streams and
lists indicates that we are missing some underlying abstraction.
Unfortunately, in order to exploit this abstraction, we will need to exert
finer control over the process of evaluation than we can at present.  We will
discuss this point further at the end of section @ref{3-5-4}.  In section
@ref{4-2}, we'll develop a framework that unifies lists and streams.}

@lisp
(define (stream-ref s n)
  (if (= n 0)
      (stream-car s)
      (stream-ref (stream-cdr s) (- n 1))))

(define (stream-map proc s)
  (if (stream-null? s)
      the-empty-stream
      (cons-stream (proc (stream-car s))
                   (stream-map proc (stream-cdr s)))))

(define (stream-for-each proc s)
  (if (stream-null? s)
      'done
      (begin (proc (stream-car s))
             (stream-for-each proc (stream-cdr s)))))
@end lisp

@code{Stream-for-each} is useful for viewing streams:

@lisp
(define (display-stream s)
  (stream-for-each display-line s))

(define (display-line x)
  (newline)
  (display x))
@end lisp

To make the stream implementation automatically and transparently interleave
the construction of a stream with its use, we will arrange for the @code{cdr}
of a stream to be evaluated when it is accessed by the @code{stream-cdr}
procedure rather than when the stream is constructed by @code{cons-stream}.
This implementation choice is reminiscent of our discussion of rational numbers
in section @ref{2-1-2}, where we saw that we can choose to implement rational
numbers so that the reduction of numerator and denominator to lowest terms is
performed either at construction time or at selection time.  The two
rational-number implementations produce the same data abstraction, but the
choice has an effect on efficiency.  There is a similar relationship between
streams and ordinary lists.  As a data abstraction, streams are the same as
lists.  The difference is the time at which the elements are evaluated.  With
ordinary lists, both the @code{car} and the @code{cdr} are evaluated at
construction time.  With streams, the @code{cdr} is evaluated at selection
time.

Our implementation of streams will be based on a special form called
@code{delay}.  Evaluating @code{(delay <@var{exp}>)} does not evaluate the
expression <@var{exp}>, but rather returns a so-called @newterm{delayed
object}, which we can think of as a ``promise'' to evaluate <@var{exp}> at some
future time.  As a companion to @code{delay}, there is a procedure called
@code{force} that takes a delayed object as argument and performs the
evaluation---in effect, forcing the @code{delay} to fulfill its promise.  We
will see below how @code{delay} and @code{force} can be implemented, but first
let us use these to construct streams.

@code{Cons-stream} is a special form defined so that

@lisp
(cons-stream <@var{a}> <@var{b}>)
@end lisp

@noindent
is equivalent to

@lisp
(cons <@var{a}> (delay <@var{b}>))
@end lisp

What this means is that we will construct streams using pairs.  However, rather
than placing the value of the rest of the stream into the @code{cdr} of the
pair we will put there a promise to compute the rest if it is ever requested.
@code{Stream-car} and @code{stream-cdr} can now be defined as procedures:

@lisp
(define (stream-car stream) (car stream))

(define (stream-cdr stream) (force (cdr stream)))
@end lisp

@code{Stream-car} selects the @code{car} of the pair; @code{stream-cdr} selects
the @code{cdr} of the pair and evaluates the delayed expression found there to
obtain the rest of the stream.@footnote{Although @code{stream-car} and
@code{stream-cdr} can be defined as procedures, @code{cons-stream} must be a
special form.  If @code{cons-stream} were a procedure, then, according to our
model of evaluation, evaluating @code{(cons-stream <@var{a}> <@var{b}>)} would
automatically cause <@var{b}> to be evaluated, which is precisely what we do
not want to happen.  For the same reason, @code{delay} must be a special form,
though @code{force} can be an ordinary procedure.}

@subsubheading The stream implementation in action

To see how this implementation behaves, let us analyze the ``outrageous'' prime
computation we saw above, reformulated in terms of streams:

@lisp
(stream-car
 (stream-cdr
  (stream-filter prime?
                 (stream-enumerate-interval 10000 1000000))))
@end lisp

We will see that it does indeed work efficiently.

We begin by calling @code{stream-enumerate-interval} with the arguments 10,000
and 1,000,000.  @code{Stream-enumerate-interval} is the stream analog of
@code{enumerate-interval} (section @ref{2-2-3}):

@lisp
(define (stream-enumerate-interval low high)
  (if (> low high)
      the-empty-stream
      (cons-stream
       low
       (stream-enumerate-interval (+ low 1) high))))
@end lisp

@noindent
and thus the result returned by @code{stream-enumerate-interval}, formed by the
@code{cons-stream}, is@footnote{The numbers shown here do not really appear in
the delayed expression.  What actually appears is the original expression, in
an environment in which the variables are bound to the appropriate numbers.
For example, @code{(+ low 1)} with @code{low} bound to 10,000 actually appears
where @code{10001} is shown.}

@lisp
(cons 10000
      (delay (stream-enumerate-interval 10001 1000000)))
@end lisp

That is, @code{stream-enumerate-interval} returns a stream represented as a
pair whose @code{car} is 10,000 and whose @code{cdr} is a promise to enumerate
more of the interval if so requested.  This stream is now filtered for primes,
using the stream analog of the @code{filter} procedure (section @ref{2-2-3}):

@lisp
(define (stream-filter pred stream)
  (cond ((stream-null? stream) the-empty-stream)
        ((pred (stream-car stream))
         (cons-stream (stream-car stream)
                      (stream-filter pred
                                     (stream-cdr stream))))
        (else (stream-filter pred (stream-cdr stream)))))
@end lisp

@code{Stream-filter} tests the @code{stream-car} of the stream (the @code{car}
of the pair, which is 10,000).  Since this is not prime, @code{stream-filter}
examines the @code{stream-cdr} of its input stream.  The call to
@code{stream-cdr} forces evaluation of the delayed
@code{stream-enumerate-interval}, which now returns

@lisp
(cons 10001
      (delay (stream-enumerate-interval 10002 1000000)))
@end lisp

@code{Stream-filter} now looks at the @code{stream-car} of this stream, 10,001,
sees that this is not prime either, forces another @code{stream-cdr}, and so
on, until @code{stream-enumerate-interval} yields the prime 10,007, whereupon
@code{stream-filter}, according to its definition, returns

@lisp
(cons-stream (stream-car stream)
             (stream-filter pred (stream-cdr stream)))
@end lisp

@noindent
which in this case is

@lisp
(cons 10007
      (delay
        (stream-filter
         prime?
         (cons 10008
               (delay
                 (stream-enumerate-interval 10009
                                            1000000))))))
@end lisp

This result is now passed to @code{stream-cdr} in our original expression.
This forces the delayed @code{stream-filter}, which in turn keeps forcing the
delayed @code{stream-enumerate-interval} until it finds the next prime, which
is 10,009.  Finally, the result passed to @code{stream-car} in our original
expression is

@lisp
(cons 10009
      (delay
        (stream-filter
         prime?
         (cons 10010
               (delay
                 (stream-enumerate-interval 10011
                                            1000000))))))
@end lisp

@code{Stream-car} returns 10,009, and the computation is complete.  Only as
many integers were tested for primality as were necessary to find the second
prime, and the interval was enumerated only as far as was necessary to feed the
prime filter.

In general, we can think of delayed evaluation as ``demand-driven''
programming, whereby each stage in the stream process is activated only enough
to satisfy the next stage.  What we have done is to decouple the actual order
of events in the computation from the apparent structure of our procedures.  We
write procedures as if the streams existed ``all at once'' when, in reality,
the computation is performed incrementally, as in traditional programming
styles.

@subsubheading Implementing @code{delay} and @code{force}

Although @code{delay} and @code{force} may seem like mysterious operations,
their implementation is really quite straightforward.  @code{Delay} must
package an expression so that it can be evaluated later on demand, and we can
accomplish this simply by treating the expression as the body of a procedure.
@code{Delay} can be a special form such that

@lisp
(delay <@var{exp}>)
@end lisp

@noindent
is syntactic sugar for

@lisp
(lambda () <@var{exp}>)
@end lisp

@code{Force} simply calls the procedure (of no arguments) produced by
@code{delay}, so we can implement @code{force} as a procedure:

@lisp
(define (force delayed-object)
  (delayed-object))
@end lisp

This implementation suffices for @code{delay} and @code{force} to work as
advertised, but there is an important optimization that we can include.  In
many applications, we end up forcing the same delayed object many times.  This
can lead to serious inefficiency in recursive programs involving streams.  (See
@ref{Exercise 3-57}.)  The solution is to build delayed objects so that the
first time they are forced, they store the value that is computed.  Subsequent
forcings will simply return the stored value without repeating the computation.
In other words, we implement @code{delay} as a special-purpose memoized
procedure similar to the one described in @ref{Exercise 3-27}.  One way to
accomplish this is to use the following procedure, which takes as argument a
procedure (of no arguments) and returns a memoized version of the procedure.
The first time the memoized procedure is run, it saves the computed result.  On
subsequent evaluations, it simply returns the result.

@lisp
(define (memo-proc proc)
  (let ((already-run? false) (result false))
    (lambda ()
      (if (not already-run?)
          (begin (set! result (proc))
                 (set! already-run? true)
                 result)
          result))))
@end lisp

@code{Delay} is then defined so that @code{(delay <@var{exp}>)} is equivalent
to

@lisp
(memo-proc (lambda () <@var{exp}>))
@end lisp

@noindent
and @code{force} is as defined previously.@footnote{There are many possible
implementations of streams other than the one described in this section.
Delayed evaluation, which is the key to making streams practical, was inherent
in Algol 60's @newterm{call-by-name} parameter-passing method.  The use of this
mechanism to implement streams was first described by Landin (1965).  Delayed
evaluation for streams was introduced into Lisp by Friedman and Wise (1976). In
their implementation, @code{cons} always delays evaluating its arguments, so
that lists automatically behave as streams.  The memoizing optimization is also
known as @newterm{call-by-need}.  The Algol community would refer to our
original delayed objects as @newterm{call-by-name thunks} and to the optimized
versions as @newterm{call-by-need thunks}.}

@quotation
@strong{@anchor{Exercise 3-50}Exercise 3.50:} Complete the following
definition, which generalizes @code{stream-map} to allow procedures that take
multiple arguments, analogous to @code{map} in section @ref{2-2-3}, footnote
@ref{Footnote 12}.

@lisp
(define (stream-map proc . argstreams)
  (if (<@var{??}> (car argstreams))
      the-empty-stream
      (<@var{??}>
       (apply proc (map <@var{??}> argstreams))
       (apply stream-map
              (cons proc (map <@var{??}> argstreams))))))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-51}Exercise 3.51:} In order to take a closer look at
delayed evaluation, we will use the following procedure, which simply returns
its argument after printing it:

@lisp
(define (show x)
  (display-line x)
  x)
@end lisp

What does the interpreter print in response to evaluating each expression in
the following sequence?@footnote{Exercises such as @ref{Exercise 3-51} and
@ref{Exercise 3-52} are valuable for testing our understanding of how
@code{delay} works.  On the other hand, intermixing delayed evaluation with
printing---and, even worse, with assignment---is extremely confusing, and
instructors of courses on computer languages have traditionally tormented their
students with examination questions such as the ones in this section.  Needless
to say, writing programs that depend on such subtleties is odious programming
style.  Part of the power of stream processing is that it lets us ignore the
order in which events actually happen in our programs.  Unfortunately, this is
precisely what we cannot afford to do in the presence of assignment, which
forces us to be concerned with time and change.}

@lisp
(define x (stream-map show (stream-enumerate-interval 0 10)))

(stream-ref x 5)

(stream-ref x 7)
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-52}Exercise 3.52:} Consider the sequence of
expressions

@lisp
(define sum 0)

(define (accum x)
  (set! sum (+ x sum))
  sum)

(define seq (stream-map accum (stream-enumerate-interval 1 20)))
(define y (stream-filter even? seq))
(define z (stream-filter (lambda (x) (= (remainder x 5) 0))
                         seq))

(stream-ref y 7)

(display-stream z)
@end lisp

What is the value of @code{sum} after each of the above expressions is
evaluated?  What is the printed response to evaluating the @code{stream-ref}
and @code{display-stream} expressions?  Would these responses differ if we had
implemented @code{(delay <@var{exp}>)} simply as @code{(lambda () <@var{exp}>)}
without using the optimization provided by @code{memo-proc}?  Explain
@end quotation

@node	3-5-2, 3-5-3, 3-5-1, 3-5
@subsection Infinite Streams

We have seen how to support the illusion of manipulating streams as complete
entities even though, in actuality, we compute only as much of the stream as we
need to access.  We can exploit this technique to represent sequences
efficiently as streams, even if the sequences are very long.  What is more
striking, we can use streams to represent sequences that are infinitely long.
For instance, consider the following definition of the stream of positive
integers:

@lisp
(define (integers-starting-from n)
  (cons-stream n (integers-starting-from (+ n 1))))

(define integers (integers-starting-from 1))
@end lisp

This makes sense because @code{integers} will be a pair whose @code{car} is 1
and whose @code{cdr} is a promise to produce the integers beginning with 2.
This is an infinitely long stream, but in any given time we can examine only a
finite portion of it.  Thus, our programs will never know that the entire
infinite stream is not there.

Using @code{integers} we can define other infinite streams, such as the stream
of integers that are not divisible by 7:

@lisp
(define (divisible? x y) (= (remainder x y) 0))

(define no-sevens
  (stream-filter (lambda (x) (not (divisible? x 7)))
                 integers))
@end lisp

Then we can find integers not divisible by 7 simply by accessing elements of
this stream:

@lisp
(stream-ref no-sevens 100)
@i{117}
@end lisp

In analogy with @code{integers}, we can define the infinite stream of Fibonacci
numbers:

@lisp
(define (fibgen a b)
  (cons-stream a (fibgen b (+ a b))))

(define fibs (fibgen 0 1))
@end lisp

@code{Fibs} is a pair whose @code{car} is 0 and whose @code{cdr} is a promise
to evaluate @code{(fibgen 1 1)}.  When we evaluate this delayed @code{(fibgen 1
1)}, it will produce a pair whose @code{car} is 1 and whose @code{cdr} is a
promise to evaluate @code{(fibgen 1 2)}, and so on.

For a look at a more exciting infinite stream, we can generalize the
@code{no-sevens} example to construct the infinite stream of prime numbers,
using a method known as the @newterm{sieve of
Eratosthenes}.@footnote{Eratosthenes, a third-century @acronym{B.C.}
Alexandrian Greek philosopher, is famous for giving the first accurate estimate
of the circumference of the Earth, which he computed by observing shadows cast
at noon on the day of the summer solstice.  Eratosthenes's sieve method,
although ancient, has formed the basis for special-purpose hardware ``sieves''
that, until recently, were the most powerful tools in existence for locating
large primes.  Since the 70s, however, these methods have been superseded by
outgrowths of the probabilistic techniques discussed in section @ref{1-2-6}.}
We start with the integers beginning with 2, which is the first prime.  To get
the rest of the primes, we start by filtering the multiples of 2 from the rest
of the integers.  This leaves a stream beginning with 3, which is the next
prime.  Now we filter the multiples of 3 from the rest of this stream.  This
leaves a stream beginning with 5, which is the next prime, and so on.  In other
words, we construct the primes by a sieving process, described as follows: To
sieve a stream @code{S}, form a stream whose first element is the first element
of @code{S} and the rest of which is obtained by filtering all multiples of the
first element of @code{S} out of the rest of @code{S} and sieving the
result. This process is readily described in terms of stream operations:

@lisp
(define (sieve stream)
  (cons-stream
   (stream-car stream)
   (sieve (stream-filter
           (lambda (x)
             (not (divisible? x (stream-car stream))))
           (stream-cdr stream)))))

(define primes (sieve (integers-starting-from 2)))
@end lisp

Now to find a particular prime we need only ask for it:

@lisp
(stream-ref primes 50)
@i{233}
@end lisp

It is interesting to contemplate the signal-processing system set up by
@code{sieve}, shown in the ``Henderson diagram'' in @ref{Figure
3-31}.@footnote{We have named these figures after Peter Henderson, who was the
first person to show us diagrams of this sort as a way of thinking about stream
processing.  Each solid line represents a stream of values being transmitted.
The dashed line from the @code{car} to the @code{cons} and the @code{filter}
indicates that this is a single value rather than a stream.}  The input stream
feeds into an ``un@code{cons}er'' that separates the first element of the
stream from the rest of the stream.  The first element is used to construct a
divisibility filter, through which the rest is passed, and the output of the
filter is fed to another sieve box.  Then the original first element is
@code{cons}ed onto the output of the internal sieve to form the output stream.
Thus, not only is the stream infinite, but the signal processor is also
infinite, because the sieve contains a sieve within it.

@quotation
@strong{@anchor{Figure 3-31}Figure 3.31:} The prime sieve viewed as a
signal-processing system.

@example
  +---------------------------------------------------------------+
  | sieve                                                         |
  |                                                               |
  |        __/|                                        |\__       |
  |     __/car|........................................|   \__    |
  |   _/      |           :                            |      \_  |
----><_       |           V                            |  cons _>---->
  |    \__    |    +------------+    +------------+    |    __/   |
  |       \cdr|--->| filter:    |    | sieve      |--->| __/      |
  |          \|    |            |--->|            |    |/         |
  |                | not        |    |            |               |
  |                | divisible? |    |            |               |
  |                +------------+    +------------+               |
  +---------------------------------------------------------------+
@end example
@end quotation

@subsubheading Defining streams implicitly

The @code{integers} and @code{fibs} streams above were defined by specifying
``generating'' procedures that explicitly compute the stream elements one by
one. An alternative way to specify streams is to take advantage of delayed
evaluation to define streams implicitly.  For example, the following expression
defines the stream @code{ones} to be an infinite stream of ones:

@lisp
(define ones (cons-stream 1 ones))
@end lisp

This works much like the definition of a recursive procedure: @code{ones} is a
pair whose @code{car} is 1 and whose @code{cdr} is a promise to evaluate
@code{ones}.  Evaluating the @code{cdr} gives us again a 1 and a promise to
evaluate @code{ones}, and so on.

We can do more interesting things by manipulating streams with operations such
as @code{add-streams}, which produces the elementwise sum of two given
streams:@footnote{This uses the generalized version of @code{stream-map} from
@ref{Exercise 3-50}.}

@lisp
(define (add-streams s1 s2)
  (stream-map + s1 s2))
@end lisp

Now we can define the integers as follows:

@lisp
(define integers (cons-stream 1 (add-streams ones integers)))
@end lisp

This defines @code{integers} to be a stream whose first element is 1 and the
rest of which is the sum of @code{ones} and @code{integers}.  Thus, the second
element of @code{integers} is 1 plus the first element of @code{integers}, or
2; the third element of @code{integers} is 1 plus the second element of
@code{integers}, or 3; and so on.  This definition works because, at any point,
enough of the @code{integers} stream has been generated so that we can feed it
back into the definition to produce the next integer.

We can define the Fibonacci numbers in the same style:

@lisp
(define fibs
  (cons-stream 0
               (cons-stream 1
                            (add-streams (stream-cdr fibs)
                                         fibs))))
@end lisp

This definition says that @code{fibs} is a stream beginning with 0 and 1, such
that the rest of the stream can be generated by adding @code{fibs} to itself
shifted by one place:

@example
      1  1  2  3  5  8   13  21  @dots{} = @code{(stream-cdr fibs)}
      0  1  1  2  3  5   8   13  @dots{} = @code{fibs}
0  1  1  2  3  5  8  13  21  34  @dots{} = @code{fibs}
@end example

@code{Scale-stream} is another useful procedure in formulating such stream
definitions.  This multiplies each item in a stream by a given constant:

@lisp
(define (scale-stream stream factor)
  (stream-map (lambda (x) (* x factor)) stream))
@end lisp

For example,

@lisp
(define double (cons-stream 1 (scale-stream double 2)))
@end lisp

@noindent
produces the stream of powers of 2: 1, 2, 4, 8, 16, 32, @dots{}.

An alternate definition of the stream of primes can be given by starting with
the integers and filtering them by testing for primality.  We will need the
first prime, 2, to get started:

@lisp
(define primes
  (cons-stream
   2
   (stream-filter prime? (integers-starting-from 3))))
@end lisp

This definition is not so straightforward as it appears, because we will test
whether a number @i{n} is prime by checking whether @i{n} is divisible by a
prime (not by just any integer) less than or equal to @emph{[sqrt]}(@i{n}):

@lisp
(define (prime? n)
  (define (iter ps)
    (cond ((> (square (stream-car ps)) n) true)
          ((divisible? n (stream-car ps)) false)
          (else (iter (stream-cdr ps)))))
  (iter primes))
@end lisp

This is a recursive definition, since @code{primes} is defined in terms of the
@code{prime?} predicate, which itself uses the @code{primes} stream.  The
reason this procedure works is that, at any point, enough of the @code{primes}
stream has been generated to test the primality of the numbers we need to check
next.  That is, for every @i{n} we test for primality, either @i{n} is not
prime (in which case there is a prime already generated that divides it) or
@i{n} is prime (in which case there is a prime already generated---i.e., a
prime less than @i{n}---that is greater than
@emph{[sqrt]}(@i{n})).@footnote{This last point is very subtle and relies on
the fact that @i{p}_(@i{n}+1) <= @i{p}_@i{n}^2.  (Here, @i{p}_@i{k} denotes the
@i{k}th prime.)  Estimates such as these are very difficult to establish.  The
ancient proof by Euclid that there are an infinite number of primes shows that
@i{p}_(@i{n}+1)<= @i{p}_1 @i{p}_2@dots{}@i{p}_@i{n} + 1, and no substantially
better result was proved until 1851, when the Russian mathematician
P. L. Chebyshev established that @i{p}_(@i{n}+1)<= 2@i{p}_@i{n} for all @i{n}.
This result, originally conjectured in 1845, is known as @newterm{Bertrand's
hypothesis}.  A proof can be found in section 22.3 of Hardy and Wright 1960.}

@quotation
@strong{@anchor{Exercise 3-53}Exercise 3.53:} Without running the program,
describe the elements of the stream defined by

@lisp
(define s (cons-stream 1 (add-streams s s)))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-54}Exercise 3.54:} Define a procedure
@code{mul-streams}, analogous to @code{add-streams}, that produces the
elementwise product of its two input streams.  Use this together with the
stream of @code{integers} to complete the following definition of the stream
whose @i{n}th element (counting from 0) is @i{n} + 1 factorial:

@lisp
(define factorials (cons-stream 1 (mul-streams <@var{??}> <@var{??}>)))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-55}Exercise 3.55:} Define a procedure
@code{partial-sums} that takes as argument a stream @i{S} and returns the
stream whose elements are @i{S}_0, @i{S}_0 + @i{S}_1, @i{S}_0 + @i{S}_1 +
@i{S}_2, @dots{}.  For example, @code{(partial-sums integers)} should be the
stream 1, 3, 6, 10, 15, @dots{}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-56}Exercise 3.56:} A famous problem, first raised by
R. Hamming, is to enumerate, in ascending order with no repetitions, all
positive integers with no prime factors other than 2, 3, or 5.  One obvious way
to do this is to simply test each integer in turn to see whether it has any
factors other than 2, 3, and 5.  But this is very inefficient, since, as the
integers get larger, fewer and fewer of them fit the requirement.  As an
alternative, let us call the required stream of numbers @code{S} and notice the
following facts about it.

@itemize @bullet

@item
@code{S} begins with 1.

@item
The elements of @code{(scale-stream S 2)} are also
elements of @code{S}.

@item
The same is true for @code{(scale-stream S 3)}
and @code{(scale-stream 5 S)}.

@item
These are all the elements of @code{S}.

@end itemize

Now all we have to do is combine elements from these sources.  For this we
define a procedure @code{merge} that combines two ordered streams into one
ordered result stream, eliminating repetitions:

@lisp
(define (merge s1 s2)
  (cond ((stream-null? s1) s2)
        ((stream-null? s2) s1)
        (else
         (let ((s1car (stream-car s1))
               (s2car (stream-car s2)))
           (cond ((< s1car s2car)
                  (cons-stream s1car (merge (stream-cdr s1) s2)))
                 ((> s1car s2car)
                  (cons-stream s2car (merge s1 (stream-cdr s2))))
                 (else
                  (cons-stream s1car
                               (merge (stream-cdr s1)
                                      (stream-cdr s2)))))))))
@end lisp

Then the required stream may be constructed with @code{merge}, as follows:

@lisp
(define S (cons-stream 1 (merge <@var{??}> <@var{??}>)))
@end lisp

Fill in the missing expressions in the places marked <@var{??}> above.
@end quotation

@quotation
@strong{@anchor{Exercise 3-57}Exercise 3.57:} How many additions are performed
when we compute the @i{n}th Fibonacci number using the definition of
@code{fibs} based on the @code{add-streams} procedure?  Show that the number of
additions would be exponentially greater if we had implemented @code{(delay
<@var{exp}>)} simply as @code{(lambda () <@var{exp}>)}, without using the
optimization provided by the @code{memo-proc} procedure described in section
@ref{3-5-1}.@footnote{This exercise shows how call-by-need is closely related
to ordinary memoization as described in @ref{Exercise 3-27}.  In that exercise,
we used assignment to explicitly construct a local table.  Our call-by-need
stream optimization effectively constructs such a table automatically, storing
values in the previously forced parts of the stream.}
@end quotation

@quotation
@strong{@anchor{Exercise 3-58}Exercise 3.58:} Give an interpretation of the
stream computed by the following procedure:

@lisp
(define (expand num den radix)
  (cons-stream
   (quotient (* num radix) den)
   (expand (remainder (* num radix) den) den radix)))
@end lisp

(@code{Quotient} is a primitive that returns the integer quotient of two
integers.)  What are the successive elements produced by @code{(expand 1 7
10)}?  What is produced by @code{(expand 3 8 10)}?
@end quotation

@quotation
@strong{@anchor{Exercise 3-59}Exercise 3.59:} In section @ref{2-5-3} we saw how
to implement a polynomial arithmetic system representing polynomials as lists
of terms.  In a similar way, we can work with @newterm{power series}, such as

@example
               x^2     x^3       x^4
e^x = 1 + x + ----- + ----- + --------- + ...
                2     3 * 2   4 * 3 * 2

             x^2       x^4
cos x = 1 - ----- + --------- - ...
              2     4 * 3 * 2

             x^3         x^5
sin x = x - ----- + ------------- - ...
            3 * 2   5 * 4 * 3 * 2
@end example

@noindent
represented as infinite streams.  We will represent the series @i{a}_0 +
@i{a}_1 @i{x} + @i{a}_2 @i{x}^2 + @i{a}_3 @i{x}^3 + @dots{} as the stream whose
elements are the coefficients @i{a}_0, @i{a}_1, @i{a}_2, @i{a}_3, @dots{}.

@enumerate a

@item
The integral of the series @i{a}_0 + @i{a}_1 @i{x} + @i{a}_2 @i{x}^2 + @i{a}_3
@i{x}^3 + @dots{} is the series

@example
             1             1             1
c + a_0 x + --- x_1 r^2 + --- a_2 r^3 + --- a_3 r^4 + ...
             2             3             4
@end example

@noindent
where @i{c} is any constant.  Define a procedure @code{integrate-series} that
takes as input a stream @i{a}_0, @i{a}_1, @i{a}_2, @dots{} representing a power
series and returns the stream @i{a}_0, (1/2)@i{a}_1, (1/3)@i{a}_2, @dots{} of
coefficients of the non-constant terms of the integral of the series.  (Since
the result has no constant term, it doesn't represent a power series; when we
use @code{integrate-series}, we will @code{cons} on the appropriate constant.)

@item
The function @i{x} |-> @i{e}^@i{x} is its own derivative.  This implies that
@i{e}^@i{x} and the integral of @i{e}^@i{x} are the same series, except for the
constant term, which is @i{e}^0 = 1.  Accordingly, we can generate the series
for @i{e}^@i{x} as

@lisp
(define exp-series
  (cons-stream 1 (integrate-series exp-series)))
@end lisp

Show how to generate the series for sine and cosine, starting from the facts
that the derivative of sine is cosine and the derivative of cosine is the
negative of sine:

@lisp
(define cosine-series
  (cons-stream 1 <@var{??}>))

(define sine-series
  (cons-stream 0 <@var{??}>))
@end lisp
@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 3-60}Exercise 3.60:} With power series represented as
streams of coefficients as in @ref{Exercise 3-59}, adding series is implemented
by @code{add-streams}.  Complete the definition of the following procedure for
multiplying series:

@lisp
(define (mul-series s1 s2)
  (cons-stream <@var{??}> (add-streams <@var{??}> <@var{??}>)))
@end lisp

You can test your procedure by verifying that @i{s}@i{i}@i{n}^2 @i{x} +
@i{cos}^2 @i{x} = 1, using the series from @ref{Exercise 3-59}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-61}Exercise 3.61:} Let @i{S} be a power series
(@ref{Exercise 3-59}) whose constant term is 1.  Suppose we want to find the
power series 1/@i{S}, that is, the series @i{X} such that @i{S} * @i{X} = 1.
Write @i{S} = 1 + @i{S}_@i{R} where @i{S}_@i{R} is the part of @i{S} after the
constant term.  Then we can solve for @i{X} as follows:

@example
        S * X = 1
(1 + S_R) * X = 1
  X + S_R * X = 1
            X = 1 - S_R * X
@end example

In other words, @i{X} is the power series whose constant term is 1 and whose
higher-order terms are given by the negative of @i{S}_@i{R} times @i{X}.  Use
this idea to write a procedure @code{invert-unit-series} that computes 1/@i{S}
for a power series @i{S} with constant term 1.  You will need to use
@code{mul-series} from @ref{Exercise 3-60}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-62}Exercise 3.62:} Use the results of @ref{Exercise
3-60} and @ref{Exercise 3-61} to define a procedure @code{div-series} that
divides two power series.  @code{Div-series} should work for any two series,
provided that the denominator series begins with a nonzero constant term.  (If
the denominator has a zero constant term, then @code{div-series} should signal
an error.)  Show how to use @code{div-series} together with the result of
@ref{Exercise 3-59} to generate the power series for tangent.
@end quotation

@node	3-5-3, 3-5-4, 3-5-2, 3-5
@subsection Exploiting the Stream Paradigm

Streams with delayed evaluation can be a powerful modeling tool, providing many
of the benefits of local state and assignment.  Moreover, they avoid some of
the theoretical tangles that accompany the introduction of assignment into a
programming language.

The stream approach can be illuminating because it allows us to build systems
with different module boundaries than systems organized around assignment to
state variables.  For example, we can think of an entire time series (or
signal) as a focus of interest, rather than the values of the state variables
at individual moments.  This makes it convenient to combine and compare
components of state from different moments.

@subsubheading Formulating iterations as stream processes

In section @ref{1-2-1}, we introduced iterative processes, which proceed by
updating state variables.  We know now that we can represent state as a
``timeless'' stream of values rather than as a set of variables to be updated.
Let's adopt this perspective in revisiting the square-root procedure from
section @ref{1-1-7}.  Recall that the idea is to generate a sequence of better
and better guesses for the square root of @i{x} by applying over and over again
the procedure that improves guesses:

@lisp
(define (sqrt-improve guess x)
  (average guess (/ x guess)))
@end lisp

In our original @code{sqrt} procedure, we made these guesses be the successive
values of a state variable. Instead we can generate the infinite stream of
guesses, starting with an initial guess of 1:@footnote{We can't use @code{let}
to bind the local variable @code{guesses}, because the value of @code{guesses}
depends on @code{guesses} itself.  @ref{Exercise 3-63} addresses why we want a
local variable here.}

@lisp
(define (sqrt-stream x)
  (define guesses
    (cons-stream 1.0
                 (stream-map (lambda (guess)
                               (sqrt-improve guess x))
                             guesses)))
  guesses)

(display-stream (sqrt-stream 2))
@i{1.}
@i{1.5}
@i{1.4166666666666665}
@i{1.4142156862745097}
@i{1.4142135623746899}
@dots{}
@end lisp

We can generate more and more terms of the stream to get better and better
guesses.  If we like, we can write a procedure that keeps generating terms
until the answer is good enough.  (See @ref{Exercise 3-64}.)

Another iteration that we can treat in the same way is to generate an
approximation to @i{[pi]}, based upon the alternating series that we saw in
section @ref{1-3-1}:

@example
[pi]        1     1     1
---- = 1 - --- + --- - --- + ...
  4         3     5     7
@end example

We first generate the stream of summands of the series (the reciprocals of the
odd integers, with alternating signs).  Then we take the stream of sums of more
and more terms (using the @code{partial-sums} procedure of @ref{Exercise 3-55})
and scale the result by 4:

@lisp
(define (pi-summands n)
  (cons-stream (/ 1.0 n)
               (stream-map - (pi-summands (+ n 2)))))

(define pi-stream
  (scale-stream (partial-sums (pi-summands 1)) 4))

(display-stream pi-stream)
@i{4.}
@i{2.666666666666667}
@i{3.466666666666667}
@i{2.8952380952380956}
@i{3.3396825396825403}
@i{2.9760461760461765}
@i{3.2837384837384844}
@i{3.017071817071818}
@dots{}
@end lisp

This gives us a stream of better and better approximations to @i{[pi]},
although the approximations converge rather slowly.  Eight terms of the
sequence bound the value of @i{[pi]} between 3.284 and 3.017.

So far, our use of the stream of states approach is not much different from
updating state variables.  But streams give us an opportunity to do some
interesting tricks.  For example, we can transform a stream with a
@newterm{sequence accelerator} that converts a sequence of approximations to a
new sequence that converges to the same value as the original, only faster.

One such accelerator, due to the eighteenth-century Swiss mathematician
Leonhard Euler, works well with sequences that are partial sums of alternating
series (series of terms with alternating signs).  In Euler's technique, if
@i{S}_@i{n} is the @i{n}th term of the original sum sequence, then the
accelerated sequence has terms

@example
             (S_(n+1) - S_n)^2
S_(n+1) - ------------------------
          S_(n-1) - 2S_n + S_(n+1)
@end example

Thus, if the original sequence is represented as a stream of values, the
transformed sequence is given by

@lisp
(define (euler-transform s)
  (let ((s0 (stream-ref s 0))           @r{; @i{S}_(@i{n}-1)}
        (s1 (stream-ref s 1))           @r{; @i{S}_@i{n}}
        (s2 (stream-ref s 2)))          @r{; @i{S}_(@i{n}+1)}
    (cons-stream (- s2 (/ (square (- s2 s1))
                          (+ s0 (* -2 s1) s2)))
                 (euler-transform (stream-cdr s)))))
@end lisp

We can demonstrate Euler acceleration with our sequence of approximations to
@i{[pi]}:

@lisp
(display-stream (euler-transform pi-stream))
@i{3.166666666666667}
@i{3.1333333333333337}
@i{3.1452380952380956}
@i{3.13968253968254}
@i{3.1427128427128435}
@i{3.1408813408813416}
@i{3.142071817071818}
@i{3.1412548236077655}
@dots{}
@end lisp

Even better, we can accelerate the accelerated sequence, and recursively
accelerate that, and so on.  Namely, we create a stream of streams (a structure
we'll call a @newterm{tableau}) in which each stream is the transform of the
preceding one:

@lisp
(define (make-tableau transform s)
  (cons-stream s
               (make-tableau transform
                             (transform s))))
@end lisp

The tableau has the form

@example
s_00   s_01   s_02   s_03   s_04   ...
       s_10   s_11   s_12   s_13   ...
              s_20   s_21   s_22   ...
                            ...
@end example

Finally, we form a sequence by taking the first term in each row of the
tableau:

@lisp
(define (accelerated-sequence transform s)
  (stream-map stream-car
              (make-tableau transform s)))
@end lisp

We can demonstrate this kind of ``super-acceleration'' of the @i{[pi]}
sequence:

@lisp
(display-stream (accelerated-sequence euler-transform
                                      pi-stream))
@i{4.}
@i{3.166666666666667}
@i{3.142105263157895}
@i{3.141599357319005}
@i{3.1415927140337785}
@i{3.1415926539752927}
@i{3.1415926535911765}
@i{3.141592653589778}
@dots{}
@end lisp

The result is impressive.  Taking eight terms of the sequence yields the
correct value of @i{[pi]} to 14 decimal places.  If we had used only the
original @i{[pi]} sequence, we would need to compute on the order of 10^13
terms (i.e., expanding the series far enough so that the individual terms are
less then 10^(-13)) to get that much accuracy!

We could have implemented these acceleration techniques without using streams.
But the stream formulation is particularly elegant and convenient because the
entire sequence of states is available to us as a data structure that can be
manipulated with a uniform set of operations.

@quotation
@strong{@anchor{Exercise 3-63}Exercise 3.63:} Louis Reasoner asks why the
@code{sqrt-stream} procedure was not written in the following more
straightforward way, without the local variable @code{guesses}:

@lisp
(define (sqrt-stream x)
  (cons-stream 1.0
               (stream-map (lambda (guess)
                             (sqrt-improve guess x))
                           (sqrt-stream x))))
@end lisp

Alyssa P. Hacker replies that this version of the procedure is considerably
less efficient because it performs redundant computation.  Explain Alyssa's
answer.  Would the two versions still differ in efficiency if our
implementation of @code{delay} used only @code{(lambda () <@var{exp}>)} without
using the optimization provided by @code{memo-proc} (section @ref{3-5-1})?
@end quotation

@quotation
@strong{@anchor{Exercise 3-64}Exercise 3.64:} Write a procedure
@code{stream-limit} that takes as arguments a stream and a number (the
tolerance).  It should examine the stream until it finds two successive
elements that differ in absolute value by less than the tolerance, and return
the second of the two elements.  Using this, we could compute square roots up
to a given tolerance by

@lisp
(define (sqrt x tolerance)
  (stream-limit (sqrt-stream x) tolerance))
@end lisp
@end quotation

@quotation
@strong{@anchor{Exercise 3-65}Exercise 3.65:} Use the series

@example
            1     1     1
ln 2 = 1 - --- + --- - --- + ...
            2     3     4
@end example

@noindent
to compute three sequences of approximations to the natural logarithm of 2, in
the same way we did above for @i{[pi]}.  How rapidly do these sequences
converge?
@end quotation

@subsubheading Infinite streams of pairs

In section @ref{2-2-3}, we saw how the sequence paradigm handles traditional
nested loops as processes defined on sequences of pairs.  If we generalize this
technique to infinite streams, then we can write programs that are not easily
represented as loops, because the ``looping'' must range over an infinite set.

For example, suppose we want to generalize the @code{prime-sum-pairs} procedure
of section @ref{2-2-3} to produce the stream of pairs of @emph{all} integers
(@i{i},@i{j}) with @i{i} <= @i{j} such that @i{i} + @i{j} is prime.  If
@code{int-pairs} is the sequence of all pairs of integers (@i{i},@i{j}) with
@i{i} <= @i{j}, then our required stream is simply@footnote{As in section
@ref{2-2-3}, we represent a pair of integers as a list rather than a Lisp
pair.}

@lisp
(stream-filter (lambda (pair)
                 (prime? (+ (car pair) (cadr pair))))
               int-pairs)
@end lisp

Our problem, then, is to produce the stream @code{int-pairs}.  More generally,
suppose we have two streams @i{S} = (@i{S}_@i{i}) and @i{T} = (@i{T}_@i{j}),
and imagine the infinite rectangular array

@example
(S_0, T_0)  (S_0, T_1)  (S_0, T_2)  ...
(S_1, T_0)  (S_1, T_1)  (S_1, T_2)  ...
(S_2, T_0)  (S_2, T_1)  (S_2, T_2)  ...
   ...
@end example

We wish to generate a stream that contains all the pairs in the array that lie
on or above the diagonal, i.e., the pairs

@example
(S_0, T_0)  (S_0, T_1)  (S_0, T_2)  ...
            (S_1, T_1)  (S_1, T_2)  ...
                        (S_2, T_2)  ...
                                    ...
@end example

@noindent
(If we take both @i{S} and @i{T} to be the stream of integers, then this will
be our desired stream @code{int-pairs}.)

Call the general stream of pairs @code{(pairs S T)}, and consider it to be
composed of three parts: the pair (@i{S}_0,@i{T}_0), the rest of the pairs in
the first row, and the remaining pairs:@footnote{See @ref{Exercise 3-68} for
some insight into why we chose this decomposition.}

@example
(S_0, T_0) | (S_0, T_1)  (S_0, T_2)  ...
-----------+-----------------------------
           | (S_1, T_1)  (S_1, T_2)  ...
           |             (S_2, T_2)  ...
           |                         ...
@end example

Observe that the third piece in this decomposition (pairs that are not in the
first row) is (recursively) the pairs formed from @code{(stream-cdr S)} and
@code{(stream-cdr T)}.  Also note that the second piece (the rest of the first
row) is

@lisp
(stream-map (lambda (x) (list (stream-car s) x))
            (stream-cdr t))
@end lisp

Thus we can form our stream of pairs as follows:

@lisp
(define (pairs s t)
  (cons-stream
   (list (stream-car s) (stream-car t))
   (<@var{combine-in-some-way}>
       (stream-map (lambda (x) (list (stream-car s) x))
                   (stream-cdr t))
       (pairs (stream-cdr s) (stream-cdr t)))))
@end lisp

In order to complete the procedure, we must choose some way to combine the two
inner streams.  One idea is to use the stream analog of the @code{append}
procedure from section @ref{2-2-1}:

@lisp
(define (stream-append s1 s2)
  (if (stream-null? s1)
      s2
      (cons-stream (stream-car s1)
                   (stream-append (stream-cdr s1) s2))))
@end lisp

This is unsuitable for infinite streams, however, because it takes all the
elements from the first stream before incorporating the second stream.  In
particular, if we try to generate all pairs of positive integers using

@lisp
(pairs integers integers)
@end lisp

@noindent
our stream of results will first try to run through all pairs with the first
integer equal to 1, and hence will never produce pairs with any other value of
the first integer.

To handle infinite streams, we need to devise an order of combination that
ensures that every element will eventually be reached if we let our program run
long enough.  An elegant way to accomplish this is with the following
@code{interleave} procedure:@footnote{The precise statement of the required
property on the order of combination is as follows: There should be a function
@i{f} of two arguments such that the pair corresponding to element @i{i} of the
first stream and element @i{j} of the second stream will appear as element
number @i{f}(@i{i},@i{j}) of the output stream.  The trick of using
@code{interleave} to accomplish this was shown to us by David Turner, who
employed it in the language KRC (Turner 1981).}

@lisp
(define (interleave s1 s2)
  (if (stream-null? s1)
      s2
      (cons-stream (stream-car s1)
                   (interleave s2 (stream-cdr s1)))))
@end lisp

Since @code{interleave} takes elements alternately from the two streams, every
element of the second stream will eventually find its way into the interleaved
stream, even if the first stream is infinite.

We can thus generate the required stream of pairs as

@lisp
(define (pairs s t)
  (cons-stream
   (list (stream-car s) (stream-car t))
   (interleave
    (stream-map (lambda (x) (list (stream-car s) x))
                (stream-cdr t))
    (pairs (stream-cdr s) (stream-cdr t)))))
@end lisp

@quotation
@strong{@anchor{Exercise 3-66}Exercise 3.66:} Examine the stream @code{(pairs
integers integers)}. Can you make any general comments about the order in which
the pairs are placed into the stream? For example, about how many pairs precede
the pair (1,100)?  the pair (99,100)? the pair (100,100)? (If you can make
precise mathematical statements here, all the better. But feel free to give
more qualitative answers if you find yourself getting bogged down.)
@end quotation

@quotation
@strong{@anchor{Exercise 3-67}Exercise 3.67:} Modify the @code{pairs} procedure
so that @code{(pairs integers integers)} will produce the stream of @emph{all}
pairs of integers (@i{i},@i{j}) (without the condition @i{i} <= @i{j}).  Hint:
You will need to mix in an additional stream.
@end quotation

@quotation
@strong{@anchor{Exercise 3-68}Exercise 3.68:} Louis Reasoner thinks that
building a stream of pairs from three parts is unnecessarily complicated.
Instead of separating the pair (@i{S}_0,@i{T}_0) from the rest of the pairs in
the first row, he proposes to work with the whole first row, as follows:

@lisp
(define (pairs s t)
  (interleave
   (stream-map (lambda (x) (list (stream-car s) x))
               t)
   (pairs (stream-cdr s) (stream-cdr t))))
@end lisp

Does this work?  Consider what happens if we evaluate @code{(pairs integers
integers)} using Louis's definition of @code{pairs}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-69}Exercise 3.69:} Write a procedure @code{triples}
that takes three infinite streams, @i{S}, @i{T}, and @i{U}, and produces the
stream of triples (@i{S}_@i{i},@i{T}_@i{j},@i{U}_@i{k}) such that @i{i} <=
@i{j} <= @i{k}.  Use @code{triples} to generate the stream of all Pythagorean
triples of positive integers, i.e., the triples (@i{i},@i{j},@i{k}) such that
@i{i} <= @i{j} and @i{i}^2 + @i{j}^2 = @i{k}^2.
@end quotation

@quotation
@strong{@anchor{Exercise 3-70}Exercise 3.70:} It would be nice to be able to
generate streams in which the pairs appear in some useful order, rather than in
the order that results from an @emph{ad hoc} interleaving process.  We can use
a technique similar to the @code{merge} procedure of @ref{Exercise 3-56}, if we
define a way to say that one pair of integers is ``less than'' another.  One
way to do this is to define a ``weighting function'' @i{W}(@i{i},@i{j}) and
stipulate that (@i{i}_1,@i{j}_1) is less than (@i{i}_2,@i{j}_2) if
@i{W}(@i{i}_1,@i{j}_1) < @i{W}(@i{i}_2,@i{j}_2).  Write a procedure
@code{merge-weighted} that is like @code{merge}, except that
@code{merge-weighted} takes an additional argument @code{weight}, which is a
procedure that computes the weight of a pair, and is used to determine the
order in which elements should appear in the resulting merged
stream.@footnote{We will require that the weighting function be such that the
weight of a pair increases as we move out along a row or down along a column of
the array of pairs.}  Using this, generalize @code{pairs} to a procedure
@code{weighted-pairs} that takes two streams, together with a procedure that
computes a weighting function, and generates the stream of pairs, ordered
according to weight.  Use your procedure to generate

@enumerate a

@item
the stream of all pairs of positive integers (@i{i},@i{j}) with @i{i} <= @i{j}
ordered according to the sum @i{i} + @i{j}

@item
the stream of all pairs of positive integers (@i{i},@i{j}) with @i{i} <= @i{j},
where neither @i{i} nor @i{j} is divisible by 2, 3, or 5, and the pairs are
ordered according to the sum 2 @i{i} + 3 @i{j} + 5 @i{i} @i{j}.

@end enumerate
@end quotation

@quotation
@strong{@anchor{Exercise 3-71}Exercise 3.71:} Numbers that can be expressed as
the sum of two cubes in more than one way are sometimes called
@newterm{Ramanujan numbers}, in honor of the mathematician Srinivasa
Ramanujan.@footnote{To quote from G. H. Hardy's obituary of Ramanujan (Hardy
1921): ``It was Mr. Littlewood (I believe) who remarked that `every positive
integer was one of his friends.'  I remember once going to see him when he was
lying ill at Putney.  I had ridden in taxi-cab No. 1729, and remarked that the
number seemed to me a rather dull one, and that I hoped it was not an
unfavorable omen.  `No,' he replied, `it is a very interesting number; it is
the smallest number expressible as the sum of two cubes in two different ways.'
'' The trick of using weighted pairs to generate the Ramanujan numbers was
shown to us by Charles Leiserson.} Ordered streams of pairs provide an elegant
solution to the problem of computing these numbers.  To find a number that can
be written as the sum of two cubes in two different ways, we need only generate
the stream of pairs of integers (@i{i},@i{j}) weighted according to the sum
@i{i}^3 + @i{j}^3 (see @ref{Exercise 3-70}), then search the stream for two
consecutive pairs with the same weight.  Write a procedure to generate the
Ramanujan numbers.  The first such number is 1,729.  What are the next five?
@end quotation

@quotation
@strong{@anchor{Exercise 3-72}Exercise 3.72:} In a similar way to @ref{Exercise
3-71} generate a stream of all numbers that can be written as the sum of two
squares in three different ways (showing how they can be so written).
@end quotation

@subsubheading Streams as signals

We began our discussion of streams by describing them as computational analogs
of the ``signals'' in signal-processing systems.  In fact, we can use streams
to model signal-processing systems in a very direct way, representing the
values of a signal at successive time intervals as consecutive elements of a
stream.  For instance, we can implement an @newterm{integrator} or
@newterm{summer} that, for an input stream @i{x} = (@i{x}_@i{i}), an initial
value @i{C}, and a small increment @i{d}@i{t}, accumulates the sum

@example
           i
          ---
S_i = C + >   x_j dt
          ---
          j=1
@end example

@noindent
and returns the stream of values @i{S} = (@i{S}_@i{i}).  The following
@code{integral} procedure is reminiscent of the ``implicit style'' definition
of the stream of integers (section @ref{3-5-2}):

@lisp
(define (integral integrand initial-value dt)
  (define int
    (cons-stream initial-value
                 (add-streams (scale-stream integrand dt)
                              int)))
  int)
@end lisp

@ref{Figure 3-32} is a picture of a signal-processing system that corresponds
to the @code{integral} procedure.  The input stream is scaled by @i{d}@i{t} and
passed through an adder, whose output is passed back through the same adder.
The self-reference in the definition of @code{int} is reflected in the figure
by the feedback loop that connects the output of the adder to one of the
inputs.

@quotation
@strong{@anchor{Figure 3-32}Figure 3.32:} The @code{integral} procedure viewed
as a signal-processing system.

@example
                             initial-value
                                  |
       +-----------+              |   |\__
input  |           |      |\__    +-->|   \_  integral
------>| scale: dt +----->|   \_      |cons_>--*------->
       |           |      | add_>---->| __/    |
       +-----------+  +-->| __/       |/       |
                      |   |/                   |
                      |                        |
                      +------------------------+
@end example
@end quotation

@quotation
@strong{@anchor{Exercise 3-73}Exercise 3.73:} We can model electrical circuits
using streams to represent the values of currents or voltages at a sequence of
times.  For instance, suppose we have an @newterm{RC circuit} consisting of a
resistor of resistance @i{R} and a capacitor of capacitance @i{C} in series.
The voltage response @i{v} of the circuit to an injected current @i{i} is
determined by the formula in @ref{Figure 3-33}, whose structure is shown by the
accompanying signal-flow diagram.

Write a procedure @code{RC} that models this circuit.  @code{RC} should take as
inputs the values of @i{R}, @i{C}, and @i{d}@i{t} and should return a procedure
that takes as inputs a stream representing the current @i{i} and an initial
value for the capacitor voltage @i{v}_0 and produces as output the stream of
voltages @i{v}.  For example, you should be able to use @code{RC} to model an
RC circuit with @i{R} = 5 ohms, @i{C} = 1 farad, and a 0.5-second time step by
evaluating @code{(define RC1 (RC 5 1 0.5))}.  This defines @code{RC1} as a
procedure that takes a stream representing the time sequence of currents and an
initial capacitor voltage and produces the output stream of voltages.
@end quotation

@quotation
@strong{@anchor{Figure 3-33}Figure 3.33:} An RC circuit and the associated
signal-flow diagram.

@example
  +                 -
 ->----'\/\/\,---| |---
  i                 C

              / t
              |  i
 v  =  v   +  |      dt + R i
        0     |
              / 0

         +--------------+
     +-->|   scale: R   |---------------------+   |\_
     |   +--------------+                     |   |  \_
     |                                        +-->|    \   v
  i  |   +--------------+     +------------+      | add >--->
 ----+-->|  scale: 1/C  |---->|  integral  |----->|   _/
         +--------------+     +------------+      | _/
                                                  |/
@end example
@end quotation

@quotation
@strong{@anchor{Exercise 3-74}Exercise 3.74:} Alyssa P. Hacker is designing a
system to process signals coming from physical sensors.  One important feature
she wishes to produce is a signal that describes the @newterm{zero crossings}
of the input signal.  That is, the resulting signal should be + 1 whenever the
input signal changes from negative to positive, - 1 whenever the input signal
changes from positive to negative, and 0 otherwise.  (Assume that the sign of a
0 input is positive.)  For example, a typical input signal with its associated
zero-crossing signal would be

@lisp
@dots{} 1  2  1.5  1  0.5  -0.1  -2  -3  -2  -0.5  0.2  3  4 @dots{}
@dots{}  0  0    0  0    0     -1  0   0   0     0    1  0  0 @dots{}
@end lisp

In Alyssa's system, the signal from the sensor is represented as a stream
@code{sense-data} and the stream @code{zero-crossings} is the corresponding
stream of zero crossings.  Alyssa first writes a procedure
@code{sign-change-detector} that takes two values as arguments and compares the
signs of the values to produce an appropriate 0, 1, or - 1.  She then
constructs her zero-crossing stream as follows:

@lisp
(define (make-zero-crossings input-stream last-value)
  (cons-stream
   (sign-change-detector (stream-car input-stream) last-value)
   (make-zero-crossings (stream-cdr input-stream)
                        (stream-car input-stream))))

(define zero-crossings (make-zero-crossings sense-data 0))
@end lisp

Alyssa's boss, Eva Lu Ator, walks by and suggests that this program is
approximately equivalent to the following one, which uses the generalized
version of @code{stream-map} from @ref{Exercise 3-50}:

@lisp
(define zero-crossings
  (stream-map sign-change-detector sense-data <@var{expression}>))
@end lisp

Complete the program by supplying the indicated <@var{expression}>.
@end quotation

@quotation
@strong{@anchor{Exercise 3-75}Exercise 3.75:} Unfortunately, Alyssa's
zero-crossing detector in @ref{Exercise 3-74} proves to be insufficient,
because the noisy signal from the sensor leads to spurious zero crossings.  Lem
E.  Tweakit, a hardware specialist, suggests that Alyssa smooth the signal to
filter out the noise before extracting the zero crossings.  Alyssa takes his
advice and decides to extract the zero crossings from the signal constructed by
averaging each value of the sense data with the previous value.  She explains
the problem to her assistant, Louis Reasoner, who attempts to implement the
idea, altering Alyssa's program as follows:

@lisp
(define (make-zero-crossings input-stream last-value)
  (let ((avpt (/ (+ (stream-car input-stream) last-value) 2)))
    (cons-stream (sign-change-detector avpt last-value)
                 (make-zero-crossings (stream-cdr input-stream)
                                      avpt))))
@end lisp

This does not correctly implement Alyssa's plan.  Find the bug that Louis has
installed and fix it without changing the structure of the program.  (Hint: You
will need to increase the number of arguments to @code{make-zero-crossings}.)
@end quotation

@quotation
@strong{@anchor{Exercise 3-76}Exercise 3.76:} Eva Lu Ator has a criticism of
Louis's approach in @ref{Exercise 3-75}.  The program he wrote is not modular,
because it intermixes the operation of smoothing with the zero-crossing
extraction.  For example, the extractor should not have to be changed if Alyssa
finds a better way to condition her input signal.  Help Louis by writing a
procedure @code{smooth} that takes a stream as input and produces a stream in
which each element is the average of two successive input stream elements.
Then use @code{smooth} as a component to implement the zero-crossing detector
in a more modular style.
@end quotation

@node	3-5-4, 3-5-5, 3-5-3, 3-5
@subsection Streams and Delayed Evaluation

The @code{integral} procedure at the end of the preceding section shows how we
can use streams to model signal-processing systems that contain feedback loops.
The feedback loop for the adder shown in @ref{Figure 3-32} is modeled by the
fact that @code{integral}'s internal stream @code{int} is defined in terms of
itself:

@lisp
(define int
  (cons-stream initial-value
               (add-streams (scale-stream integrand dt)
                            int)))
@end lisp

The interpreter's ability to deal with such an implicit definition depends on
the @code{delay} that is incorporated into @code{cons-stream}.  Without this
@code{delay}, the interpreter could not construct @code{int} before evaluating
both arguments to @code{cons-stream}, which would require that @code{int}
already be defined.  In general, @code{delay} is crucial for using streams to
model signal-processing systems that contain loops.  Without @code{delay}, our
models would have to be formulated so that the inputs to any signal-processing
component would be fully evaluated before the output could be produced.  This
would outlaw loops.

Unfortunately, stream models of systems with loops may require uses of
@code{delay} beyond the ``hidden'' @code{delay} supplied by @code{cons-stream}.
For instance, @ref{Figure 3-34} shows a signal-processing system for solving
the differential equation @i{dy}/@i{dt} = @i{f}(@i{y}) where @i{f} is a given
function.  The figure shows a mapping component, which applies @i{f} to its
input signal, linked in a feedback loop to an integrator in a manner very
similar to that of the analog computer circuits that are actually used to solve
such equations.

@quotation
@strong{@anchor{Figure 3-34}Figure 3.34:} An ``analog computer circuit'' that
solves the equation @i{dy}/@i{dt} = @i{f}(@i{y}).

@example
                            y_0
                             |
                             V
    +----------+  dy   +----------+     y
+-->|  map: f  +------>| integral +--*----->
|   +----------+       +----------+  |
|                                    |
+------------------------------------+
@end example
@end quotation

Assuming we are given an initial value @i{y}_0 for @i{y}, we could try to model
this system using the procedure

@lisp
(define (solve f y0 dt)
  (define y (integral dy y0 dt))
  (define dy (stream-map f y))
  y)
@end lisp

This procedure does not work, because in the first line of @code{solve} the
call to @code{integral} requires that the input @code{dy} be defined, which
does not happen until the second line of @code{solve}.

On the other hand, the intent of our definition does make sense, because we
can, in principle, begin to generate the @code{y} stream without knowing
@code{dy}.  Indeed, @code{integral} and many other stream operations have
properties similar to those of @code{cons-stream}, in that we can generate part
of the answer given only partial information about the arguments.  For
@code{integral}, the first element of the output stream is the specified
@code{initial-value}.  Thus, we can generate the first element of the output
stream without evaluating the integrand @code{dy}.  Once we know the first
element of @code{y}, the @code{stream-map} in the second line of @code{solve}
can begin working to generate the first element of @code{dy}, which will
produce the next element of @code{y}, and so on.

To take advantage of this idea, we will redefine @code{integral} to expect the
integrand stream to be a @newterm{delayed argument}.  @code{Integral} will
@code{force} the integrand to be evaluated only when it is required to generate
more than the first element of the output stream:

@lisp
(define (integral delayed-integrand initial-value dt)
  (define int
    (cons-stream initial-value
                 (let ((integrand (force delayed-integrand)))
                   (add-streams (scale-stream integrand dt)
                                int))))
  int)
@end lisp

Now we can implement our @code{solve} procedure by delaying the evaluation of
@code{dy} in the definition of @code{y}:@footnote{This procedure is not
guaranteed to work in all Scheme implementations, although for any
implementation there is a simple variation that will work.  The problem has to
do with subtle differences in the ways that Scheme implementations handle
internal definitions.  (See section @ref{4-1-6}.)}

@lisp
(define (solve f y0 dt)
  (define y (integral (delay dy) y0 dt))
  (define dy (stream-map f y))
  y)
@end lisp

In general, every caller of @code{integral} must now @code{delay} the integrand
argument.  We can demonstrate that the @code{solve} procedure works by
approximating @i{e}approx 2.718 by computing the value at @i{y} = 1 of the
solution to the differential equation @i{dy}/@i{dt} = @i{y} with initial
condition @i{y}(0) = 1:

@lisp
(stream-ref (solve (lambda (y) y) 1 0.001) 1000)
@i{2.716924}
@end lisp

@quotation
@strong{@anchor{Exercise 3-77}Exercise 3.77:} The @code{integral} procedure
used above was analogous to the ``implicit'' definition of the infinite stream
of integers in section @ref{3-5-2}.  Alternatively, we can give a definition of
@code{integral} that is more like @code{integers-starting-from} (also in
section @ref{3-5-2}):

@lisp
(define (integral integrand initial-value dt)
  (cons-stream initial-value
               (if (stream-null? integrand)
                   the-empty-stream
                   (integral (stream-cdr integrand)
                             (+ (* dt (stream-car integrand))
                                initial-value)
                             dt))))
@end lisp

When used in systems with loops, this procedure has the same problem as does
our original version of @code{integral}.  Modify the procedure so that it
expects the @code{integrand} as a delayed argument and hence can be used in the
@code{solve} procedure shown above.
@end quotation

@quotation
@strong{@anchor{Figure 3-35}Figure 3.35:} Signal-flow diagram for the solution
to a second-order linear differential equation.

@example
               dy_0                y_0
                |                   |
                V                   V
   ddy     +----------+    dy  +----------+    y
+--------->| integral +-----*--+ integral +--*--->
|          +----------+     |  +----------+  |
|                           |                |
|            +----------+   |                |
|     __/|<--+ scale: a |<--+                |
|   _/   |   +----------+                    |
+--<_add |                                   |
     \__ |   +----------+                    |
        \|<--+ scale: b |<-------------------+
             +----------+
@end example
@end quotation

@quotation
@strong{@anchor{Exercise 3-78}Exercise 3.78:} Consider the problem of designing
a signal-processing system to study the homogeneous second-order linear
differential equation

@example
d^2 y        d y
-----  -  a -----  -  by  =  0
d t^2        d t
@end example

The output stream, modeling @i{y}, is generated by a network that contains a
loop. This is because the value of @i{d}^2@i{y}/@i{d}@i{t}^2 depends upon the
values of @i{y} and @i{d}@i{y}/@i{d}@i{t} and both of these are determined by
integrating @i{d}^2@i{y}/@i{d}@i{t}^2.  The diagram we would like to encode is
shown in @ref{Figure 3-35}.  Write a procedure @code{solve-2nd} that takes as
arguments the constants @i{a}, @i{b}, and @i{d}@i{t} and the initial values
@i{y}_0 and @i{d}@i{y}_0 for @i{y} and @i{d}@i{y}/@i{d}@i{t} and generates the
stream of successive values of @i{y}.
@end quotation

@quotation
@strong{@anchor{Exercise 3-79}Exercise 3.79:} Generalize the @code{solve-2nd}
procedure of @ref{Exercise 3-78} so that it can be used to solve general
second-order differential equations @i{d}^2 @i{y}/@i{d}@i{t}^2 =
@i{f}(@i{d}@i{y}/@i{d}@i{t}, @i{y}).
@end quotation

@quotation
@strong{@anchor{Exercise 3-80}Exercise 3.80:} A @newterm{series RLC circuit}
consists of a resistor, a capacitor, and an inductor connected in series, as
shown in @ref{Figure 3-36}.  If @i{R}, @i{L}, and @i{C} are the resistance,
inductance, and capacitance, then the relations between voltage (@i{v}) and
current (@i{i}) for the three components are described by the equations

@example
v_R = i_R R

         d_(i L)
v_L = L ---------
           d t

         d v_C
i_C = C -------
          d t
@end example

@noindent
and the circuit connections dictate the relations

@example
i_R = i_L = -i_C

v_C = v_L + v_R
@end example

@noindent
Combining these equations shows that the state of the circuit (summarized by
@i{v}_@i{C}, the voltage across the capacitor, and @i{i}_@i{L}, the current in
the inductor) is described by the pair of differential equations

@example
d v_C        i_L
-----  =  -  ---
 d t          C

d i_L      1           R
-----  =  --- v_C  -  --- i_L
 d t       L           L
@end example

@noindent
The signal-flow diagram representing this system of differential equations is
shown in @ref{Figure 3-37}.
@end quotation

@example
@strong{@anchor{Figure 3-36}Figure 3.36:} A series RLC circuit.
              + v_R -
        i_R
     +--->----'\/\/\,--------+
     |                       |  i_L
    \|/          R          \|/
  +  |  i_C                  |_   +
    -+-                      __)
v_C -+- C                   (_)   v_L
     |                       __)
  -  |                       |    -
     +-----------------------+
@end example

@quotation
@strong{@anchor{Figure 3-37}Figure 3.37:} A signal-flow diagram for the
solution to a series RLC circuit.

@example
                 +-------------+
+----------------+  scale: l/L |<--+
|                +-------------+   |
|                                  |
|                +-------------+   |  v_C
|       dv_C +-->|   integral  +---*------>
|            |   +-------------+
|            |        ^
|            |        | v_(C_0)
|            |
|            |   +-------------+
|            +---+ scale: -l/C |<--+
|                +-------------+   |
|  |\__                            |
+->|   \_  di_L  +-------------+   |  i_L
   | add_>------>|   integral  +---*------>
+->| __/         +-------------+   |
|  |/                 ^            |
|                     | i_(L_0)    |
|                                  |
|                +-------------+   |
+----------------+ scale: -R/L |<--+
                 +-------------+
@end example
@end quotation

Write a procedure @code{RLC} that takes as arguments the parameters @i{R},
@i{L}, and @i{C} of the circuit and the time increment @i{d}@i{t}.  In a manner
similar to that of the @code{RC} procedure of @ref{Exercise 3-73}, @code{RLC}
should produce a procedure that takes the initial values of the state
variables, @i{v}_(@i{C}_0) and @i{i}_(@i{L}_0), and produces a pair (using
@code{cons}) of the streams of states @i{v}_@i{C} and @i{i}_@i{L}.  Using
@code{RLC}, generate the pair of streams that models the behavior of a series
RLC circuit with @i{R} = 1 ohm, @i{C} = 0.2 farad, @i{L} = 1 henry, @i{d}@i{t}
= 0.1 second, and initial values @i{i}_(@i{L}_0) = 0 amps and @i{v}_(@i{C}_0) =
10 volts.

@subsubheading Normal-order evaluation

The examples in this section illustrate how the explicit use of @code{delay}
and @code{force} provides great programming flexibility, but the same examples
also show how this can make our programs more complex.  Our new @code{integral}
procedure, for instance, gives us the power to model systems with loops, but we
must now remember that @code{integral} should be called with a delayed
integrand, and every procedure that uses @code{integral} must be aware of this.
In effect, we have created two classes of procedures: ordinary procedures and
procedures that take delayed arguments.  In general, creating separate classes
of procedures forces us to create separate classes of higher-order procedures
as well.@footnote{This is a small reflection, in Lisp, of the difficulties that
conventional strongly typed languages such as Pascal have in coping with
higher-order procedures.  In such languages, the programmer must specify the
data types of the arguments and the result of each procedure: number, logical
value, sequence, and so on.  Consequently, we could not express an abstraction
such as ``map a given procedure @code{proc} over all the elements in a
sequence'' by a single higher-order procedure such as @code{stream-map}.
Rather, we would need a different mapping procedure for each different
combination of argument and result data types that might be specified for a
@code{proc}.  Maintaining a practical notion of ``data type'' in the presence
of higher-order procedures raises many difficult issues.  One way of dealing
with this problem is illustrated by the language ML (Gordon, Milner, and
Wadsworth 1979), whose ``polymorphic data types'' include templates for
higher-order transformations between data types.  Moreover, data types for most
procedures in ML are never explicitly declared by the programmer.  Instead, ML
includes a @newterm{type-inferencing} mechanism that uses information in the
environment to deduce the data types for newly defined procedures.}

One way to avoid the need for two different classes of procedures is to make
all procedures take delayed arguments.  We could adopt a model of evaluation in
which all arguments to procedures are automatically delayed and arguments are
forced only when they are actually needed (for example, when they are required
by a primitive operation).  This would transform our language to use
normal-order evaluation, which we first described when we introduced the
substitution model for evaluation in section @ref{1-1-5}.  Converting to
normal-order evaluation provides a uniform and elegant way to simplify the use
of delayed evaluation, and this would be a natural strategy to adopt if we were
concerned only with stream processing.  In section @ref{4-2}, after we have
studied the evaluator, we will see how to transform our language in just this
way.  Unfortunately, including delays in procedure calls wreaks havoc with our
ability to design programs that depend on the order of events, such as programs
that use assignment, mutate data, or perform input or output.  Even the single
@code{delay} in @code{cons-stream} can cause great confusion, as illustrated by
@ref{Exercise 3-51} and @ref{Exercise 3-52}.  As far as anyone knows,
mutability and delayed evaluation do not mix well in programming languages, and
devising ways to deal with both of these at once is an active area of research.

@node	3-5-5,  , 3-5-4, 3-5
@subsection Modularity of Functional Programs and Modularity of Objects

As we saw in section @ref{3-1-2}, one of the major benefits of introducing
assignment is that we can increase the modularity of our systems by
encapsulating, or ``hiding,'' parts of the state of a large system within local
variables.  Stream models can provide an equivalent modularity without the use
of assignment.  As an illustration, we can reimplement the Monte Carlo
estimation of @i{[pi]}, which we examined in section @ref{3-1-2}, from a
stream-processing point of view.

The key modularity issue was that we wished to hide the internal state of a
random-number generator from programs that used random numbers.  We began with
a procedure @code{rand-update}, whose successive values furnished our supply of
random numbers, and used this to produce a random-number generator:

@lisp
(define rand
  (let ((x random-init))
    (lambda ()
      (set! x (rand-update x))
      x)))
@end lisp

In the stream formulation there is no random-number generator @emph{per se},
just a stream of random numbers produced by successive calls to
@code{rand-update}:

@lisp
(define random-numbers
  (cons-stream random-init
               (stream-map rand-update random-numbers)))
@end lisp

We use this to construct the stream of outcomes of the Ces@`aro experiment
performed on consecutive pairs in the @code{random-numbers} stream:

@lisp
(define cesaro-stream
  (map-successive-pairs (lambda (r1 r2) (= (gcd r1 r2) 1))
                        random-numbers))

(define (map-successive-pairs f s)
  (cons-stream
   (f (stream-car s) (stream-car (stream-cdr s)))
   (map-successive-pairs f (stream-cdr (stream-cdr s)))))
@end lisp

The @code{cesaro-stream} is now fed to a @code{monte-carlo} procedure, which
produces a stream of estimates of probabilities.  The results are then
converted into a stream of estimates of @i{[pi]}.  This version of the program
doesn't need a parameter telling how many trials to perform.  Better estimates
of @i{[pi]} (from performing more experiments) are obtained by looking farther
into the @code{pi} stream:

@lisp
(define (monte-carlo experiment-stream passed failed)
  (define (next passed failed)
    (cons-stream
     (/ passed (+ passed failed))
     (monte-carlo
      (stream-cdr experiment-stream) passed failed)))
  (if (stream-car experiment-stream)
      (next (+ passed 1) failed)
      (next passed (+ failed 1))))

(define pi
  (stream-map (lambda (p) (sqrt (/ 6 p)))
              (monte-carlo cesaro-stream 0 0)))
@end lisp

There is considerable modularity in this approach, because we still can
formulate a general @code{monte-carlo} procedure that can deal with arbitrary
experiments.  Yet there is no assignment or local state.

@quotation
@strong{@anchor{Exercise 3-81}Exercise 3.81:} @ref{Exercise 3-6} discussed
generalizing the random-number generator to allow one to reset the
random-number sequence so as to produce repeatable sequences of ``random''
numbers.  Produce a stream formulation of this same generator that operates on
an input stream of requests to @code{generate} a new random number or to
@code{reset} the sequence to a specified value and that produces the desired
stream of random numbers.  Don't use assignment in your solution.
@end quotation

@quotation
@strong{@anchor{Exercise 3-82}Exercise 3.82:} Redo @ref{Exercise 3-5} on Monte
Carlo integration in terms of streams.  The stream version of
@code{estimate-integral} will not have an argument telling how many trials to
perform.  Instead, it will produce a stream of estimates based on successively
more trials.
@end quotation

@subsubheading A functional-programming view of time

Let us now return to the issues of objects and state that were raised at the
beginning of this chapter and examine them in a new light.  We introduced
assignment and mutable objects to provide a mechanism for modular construction
of programs that model systems with state.  We constructed computational
objects with local state variables and used assignment to modify these
variables.  We modeled the temporal behavior of the objects in the world by the
temporal behavior of the corresponding computational objects.

Now we have seen that streams provide an alternative way to model objects with
local state.  We can model a changing quantity, such as the local state of some
object, using a stream that represents the time history of successive states.
In essence, we represent time explicitly, using streams, so that we decouple
time in our simulated world from the sequence of events that take place during
evaluation.  Indeed, because of the presence of @code{delay} there may be
little relation between simulated time in the model and the order of events
during the evaluation.

In order to contrast these two approaches to modeling, let us reconsider the
implementation of a ``withdrawal processor'' that monitors the balance in a
bank account.  In section @ref{3-1-3} we implemented a simplified version of
such a processor:

@lisp
(define (make-simplified-withdraw balance)
  (lambda (amount)
    (set! balance (- balance amount))
    balance))
@end lisp

Calls to @code{make-simplified-withdraw} produce computational objects, each
with a local state variable @code{balance} that is decremented by successive
calls to the object.  The object takes an @code{amount} as an argument and
returns the new balance.  We can imagine the user of a bank account typing a
sequence of inputs to such an object and observing the sequence of returned
values shown on a display screen.

Alternatively, we can model a withdrawal processor as a procedure that takes as
input a balance and a stream of amounts to withdraw and produces the stream of
successive balances in the account:

@lisp
(define (stream-withdraw balance amount-stream)
  (cons-stream
   balance
   (stream-withdraw (- balance (stream-car amount-stream))
                    (stream-cdr amount-stream))))
@end lisp

@code{Stream-withdraw} implements a well-defined mathematical function whose
output is fully determined by its input.  Suppose, however, that the input
@code{amount-stream} is the stream of successive values typed by the user and
that the resulting stream of balances is displayed.  Then, from the perspective
of the user who is typing values and watching results, the stream process has
the same behavior as the object created by @code{make-simplified-withdraw}.
However, with the stream version, there is no assignment, no local state
variable, and consequently none of the theoretical difficulties that we
encountered in section @ref{3-1-3}.  Yet the system has state!

This is really remarkable.  Even though @code{stream-withdraw} implements a
well-defined mathematical function whose behavior does not change, the user's
perception here is one of interacting with a system that has a changing state.
One way to resolve this paradox is to realize that it is the user's temporal
existence that imposes state on the system.  If the user could step back from
the interaction and think in terms of streams of balances rather than
individual transactions, the system would appear stateless.@footnote{Similarly
in physics, when we observe a moving particle, we say that the position (state)
of the particle is changing.  However, from the perspective of the particle's
world line in space-time there is no change involved.}

From the point of view of one part of a complex process, the other parts appear
to change with time.  They have hidden time-varying local state.  If we wish to
write programs that model this kind of natural decomposition in our world (as
we see it from our viewpoint as a part of that world) with structures in our
computer, we make computational objects that are not functional---they must
change with time.  We model state with local state variables, and we model the
changes of state with assignments to those variables.  By doing this we make
the time of execution of a computation model time in the world that we are part
of, and thus we get ``objects'' in our computer.

Modeling with objects is powerful and intuitive, largely because this matches
the perception of interacting with a world of which we are part.  However, as
we've seen repeatedly throughout this chapter, these models raise thorny
problems of constraining the order of events and of synchronizing multiple
processes.  The possibility of avoiding these problems has stimulated the
development of @newterm{functional programming languages}, which do not include
any provision for assignment or mutable data.  In such a language, all
procedures implement well-defined mathematical functions of their arguments,
whose behavior does not change.  The functional approach is extremely
attractive for dealing with concurrent systems.@footnote{John Backus, the
inventor of Fortran, gave high visibility to functional programming when he was
awarded the @acronym{ACM} Turing award in 1978.  His acceptance speech (Backus
1978) strongly advocated the functional approach.  A good overview of
functional programming is given in Henderson 1980 and in Darlington, Henderson,
and Turner 1982.}

On the other hand, if we look closely, we can see time-related problems
creeping into functional models as well.  One particularly troublesome area
arises when we wish to design interactive systems, especially ones that model
interactions between independent entities.  For instance, consider once more
the implementation a banking system that permits joint bank accounts.  In a
conventional system using assignment and objects, we would model the fact that
Peter and Paul share an account by having both Peter and Paul send their
transaction requests to the same bank-account object, as we saw in section
@ref{3-1-3}.  From the stream point of view, where there are no ``objects''
@emph{per se}, we have already indicated that a bank account can be modeled as
a process that operates on a stream of transaction requests to produce a stream
of responses.  Accordingly, we could model the fact that Peter and Paul have a
joint bank account by merging Peter's stream of transaction requests with
Paul's stream of requests and feeding the result to the bank-account stream
process, as shown in @ref{Figure 3-38}.

@quotation
@strong{@anchor{Figure 3-38}Figure 3.38:} A joint bank account, modeled by
merging two streams of transaction requests.

@example
Peter's requests   +---------+     +---------+
------------------>|         |     |         |
Paul's requests    |  merge  |---->| bank    |---->
------------------>|         |     | account |
                   +---------+     +---------+
@end example
@end quotation

The trouble with this formulation is in the notion of @newterm{merge}.  It will
not do to merge the two streams by simply taking alternately one request from
Peter and one request from Paul. Suppose Paul accesses the account only very
rarely.  We could hardly force Peter to wait for Paul to access the account
before he could issue a second transaction.  However such a merge is
implemented, it must interleave the two transaction streams in some way that is
constrained by ``real time'' as perceived by Peter and Paul, in the sense that,
if Peter and Paul meet, they can agree that certain transactions were processed
before the meeting, and other transactions were processed after the
meeting.@footnote{Observe that, for any two streams, there is in general more
than one acceptable order of interleaving.  Thus, technically, ``merge'' is a
relation rather than a function---the answer is not a deterministic function of
the inputs.  We already mentioned (@ref{Footnote 39}) that nondeterminism is
essential when dealing with concurrency.  The merge relation illustrates the
same essential nondeterminism, from the functional perspective.  In section
@ref{4-3}, we will look at nondeterminism from yet another point of view.} This
is precisely the same constraint that we had to deal with in section
@ref{3-4-1}, where we found the need to introduce explicit synchronization to
ensure a ``correct'' order of events in concurrent processing of objects with
state.  Thus, in an attempt to support the functional style, the need to merge
inputs from different agents reintroduces the same problems that the functional
style was meant to eliminate.

We began this chapter with the goal of building computational models whose
structure matches our perception of the real world we are trying to model.  We
can model the world as a collection of separate, time-bound, interacting
objects with state, or we can model the world as a single, timeless, stateless
unity.  Each view has powerful advantages, but neither view alone is completely
satisfactory.  A grand unification has yet to emerge.@footnote{The object model
approximates the world by dividing it into separate pieces.  The functional
model does not modularize along object boundaries.  The object model is useful
when the unshared state of the ``objects'' is much larger than the state that
they share.  An example of a place where the object viewpoint fails is quantum
mechanics, where thinking of things as individual particles leads to paradoxes
and confusions.  Unifying the object view with the functional view may have
little to do with programming, but rather with fundamental epistemological
issues.}

@node    Chapter 4, Chapter 5, Chapter 3, Top
